{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LinearRegression from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 구현할 것\n",
    "- 공부시간과 성적간의 관계를 모델링한다.\n",
    "    - **머신러닝 모델(모형)이란** 수집한 데이터를 기반으로 입력값(Feature)와 출력값(Target)간의 관계를 하나의 공식으로 정의한 함수이다. 그 공식을 찾는 과정을 **모델링**이라고 한다.\n",
    "    - 이 예제에서는 공부한 시험시간으로 점수를 예측하는 모델을 정의한다.\n",
    "    - 입력값과 출력값 간의 관계를 정의할 수있는 다양한 함수(공식)이 있다. 여기에서는 딥러닝과 관계가 있는 **Linear Regression** 을 사용해본다.\n",
    "\n",
    "# 데이터 확인\n",
    "- 입력데이터: 공부시간\n",
    "- 출력데이터: 성적\n",
    "\n",
    "|공부시간|점수|\n",
    "|-|-|\n",
    "|1|20|\n",
    "|2|40|\n",
    "|3|60|\n",
    "\n",
    "우리가 수집한 공부시간과 점수 데이터를 바탕으로 둘 간의 관계를 식으로 정의 할 수 있으면 **내가 몇시간 공부하면 점수를 얼마 받을 수 있는지 예측할 수 있게 된다.**   \n",
    "수집한 데이터를 기반으로 앞으로 예측할 수있는 모형을 만드는 것이 머신러닝 모델링이다.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습(훈련) 데이터셋 만들기\n",
    "- 모델을 학습시키기 위한 데이터셋을 구성한다.\n",
    "- 입력데이터와 출력데이터을 각각 다른 행렬로 구성한다.\n",
    "- 하나의 데이터 포인트의 입력/출력 값은 같은 index에 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 (Linear Regression)\n",
    "- Feature들의 가중합을 이용해 Target을 추정한다.\n",
    "- Feature에 곱해지는 가중치(weight)들은 각 Feature가 Target 얼마나 영향을 주는지 영향도가 된다.\n",
    "    - 음수일 경우는 target값을 줄이고 양수일 경우는 target값을 늘린다.\n",
    "    - 가중치가 0에 가까울 수록 target에 영향을 주지 않는 feature이고 0에서 멀수록 target에 많은 영향을 준다.\n",
    "- 모델 학습과정에서 가장 적절한 Feature의 가중치를 찾아야 한다.\n",
    "      \n",
    "\n",
    "\\begin{align}\n",
    "&\\large \\hat{y} = W\\cdot X + b\\\\\n",
    "&\\small \\hat{y}: \\text{모델추정값}\\\\\n",
    "&\\small W: \\text{가중치}\\\\\n",
    "&\\small X: \\text{Feature(입력값)}\\\\\n",
    "&\\small b: \\text{bias(편향)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset 구성\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `공부시간` 1개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다.   \n",
    "  이 문제에서 예측할 항목은 `시험점수` 한개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_hours = [[1], [2], [3]]\n",
    "scores = [[20],[40],[60]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/8y/jsfncy6n39v2jylvdpm61nmh0000gn/T/ipykernel_1554/4133644965.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'study_hours' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mstudy_hours\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m y_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scores, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study_hours' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X_train = torch.tensor(study_hours, dtype=torch.float32)\n",
    "y_train = torch.tensor(scores, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 (weight, bias) 정의\n",
    "- 학습대상 / 최적화대상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.0236]], requires_grad=True), tensor([0.1116], requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred = X_train * weight + bias\n",
    "# 랜덤한 값 생성\n",
    "weight = torch.randn(1, 1, requires_grad=True) # shape (1:X의 feature갯수, 1: 출력값(예측)의 갯수)\n",
    "bias = torch.randn(1, requires_grad=True)\n",
    "\n",
    "weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9120],\n",
       "        [-3.9357],\n",
       "        [-5.9593]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추론 (W * x + b)\n",
    "pred = X_train @ weight + bias\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1480.9337, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 오차 - MSE\n",
    "loss = torch.mean((y_train - pred) ** 2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight, bias gradient 계산\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-166.2587]]), tensor([-70.9980]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad, bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight 값 변경\n",
    "# new_weight = weight - weight.grad * learning_rate\n",
    "lr = 0.01\n",
    "new_weight = weight.data - weight.grad * lr\n",
    "new_bias = bias.data - bias.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.4656]]) tensor([[1.8030]])\n",
      "tensor([1.6050]) tensor([0.8950])\n"
     ]
    }
   ],
   "source": [
    "print(new_weight.data, weight.data)\n",
    "print(new_bias.data, bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5361) tensor(1256.6406)\n"
     ]
    }
   ],
   "source": [
    "pred2 = torch.mean(X_train @ new_weight + new_bias)\n",
    "loss2 = torch.mean((y_train - pred2) ** 2)\n",
    "\n",
    "print(pred2, loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight =  torch.randn(1, 1, requires_grad=True)\n",
    "bias = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 함수 (예측 모델)\n",
    "def linear_model(X) :\n",
    "    return X @ weight + bias\n",
    "\n",
    "# 오차 계산 함수 (MSE)\n",
    "def loss_fn(pred, y) :\n",
    "    return torch.mean((pred-y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n",
    "1. 모델을 이용해 추정한다.\n",
    "   - pred = model(input)\n",
    "1. loss를 계산한다.\n",
    "   - loss = loss_fn(pred, target)\n",
    "1. 계산된 loss를 파라미터에 대해 미분하여 계산한 gradient 값을 각 파라미터에 저장한다.\n",
    "   - loss.backward()\n",
    "1. optimizer를 이용해 파라미터를 update한다.\n",
    "   - optimizer.step()  \n",
    "1. 파라미터의 gradient(미분값)을 0으로 초기화한다.\n",
    "   - optimizer.zero_grad()\n",
    "- 위의 단계를 반복한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/range(0, 1000) loss = 0.00046\n",
      "101/range(0, 1000) loss = 0.00028\n",
      "201/range(0, 1000) loss = 0.00017\n",
      "301/range(0, 1000) loss = 0.00011\n",
      "401/range(0, 1000) loss = 0.00007\n",
      "501/range(0, 1000) loss = 0.00004\n",
      "601/range(0, 1000) loss = 0.00003\n",
      "701/range(0, 1000) loss = 0.00002\n",
      "801/range(0, 1000) loss = 0.00001\n",
      "901/range(0, 1000) loss = 0.00001\n",
      "1000/range(0, 1000) loss = 0.00000\n"
     ]
    }
   ],
   "source": [
    "epochs = range(1000) # 반복 횟수\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in epochs :\n",
    "    # 1. 추정\n",
    "    pred = linear_model(X_train)\n",
    "    # 2. loss 계산\n",
    "    loss = loss_fn(pred, y_train)\n",
    "    # 3. parameter들의 gradient를 계산\n",
    "    loss.backward()\n",
    "    # 4. parameter update\n",
    "    weight.data = weight.data - weight.grad * lr\n",
    "    bias.data = bias.data -  bias.grad * lr\n",
    "    # 5. gradient 초기화\n",
    "    weight.grad = None\n",
    "    bias.grad = None\n",
    "    # 로그 출력\n",
    "    if epoch % 100 == 0 or epoch == 1000-1 :\n",
    "        print(f\"{epoch+1}/{epochs} loss = {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[19.7538]]), tensor([0.5597]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.data, bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[99.9938],\n",
       "        [79.9961],\n",
       "        [40.0006]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X = torch.tensor([[5], [4], [2]], dtype=torch.float32)\n",
    "linear_model(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 입력, 다중 출력\n",
    "- 다중입력: Feature가 여러개인 경우\n",
    "- 다중출력: Output 결과가 여러개인 경우\n",
    "\n",
    "다음 가상 데이터를 이용해 사과와 오렌지 수확량을 예측하는 선형회귀 모델을 정의한다.  \n",
    "[참조](https://www.kaggle.com/code/aakashns/pytorch-basics-linear-regression-from-scratch)\n",
    "\n",
    "\n",
    "|온도(F)|강수량(mm)|습도(%)|사과생산량(ton)|오렌지생산량|\n",
    "|-|-|-|-:|-:|\n",
    "|73|67|43|56|70|\n",
    "|91|88|64|81|101|\n",
    "|87|134|58|119|133|\n",
    "|102|43|37|22|37|\n",
    "|69|96|70|103|119|\n",
    "\n",
    "```\n",
    "사과수확량  = w11 * 온도 + w12 * 강수량 + w13 * 습도 + b1\n",
    "오렌지수확량 = w21 * 온도 + w22 * 강수량 + w23 *습도 + b2\n",
    "```\n",
    "\n",
    "- `온도`, `강수량`, `습도` 값이 **사과**와, **오렌지 수확량**에 어느정도 영향을 주는지 가중치를 찾는다.\n",
    "    - 모델은 사과의 수확량, 오렌지의 수확량 **두개의 예측결과를 출력**해야 한다.\n",
    "    - 사과에 대해 예측하기 위한 weight 3개와 오렌지에 대해 예측하기 위한 weight 3개 이렇게 두 묶음, 총 6개의 weight를 정의하고 학습을 통해 가장 적당한 값을 찾는다.\n",
    "        - `개별 과일를 예측하기 위한 weight들 @ feature들` 의 계산 결과를  **Node, Unit, Neuron** 이라고 한다.\n",
    "        - 두 과일에 대한 Unit들을 묶어서 **Layer** 라고 한다.\n",
    "- 목적은 우리가 수집한 train 데이터셋을 이용해 **정확한 예측을 위한 weight와 bias 들**을 찾는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Dataset\n",
    "- Train data는 feature(input)와 target(output) 각각 2개의 행렬로 구성한다.\n",
    "- Feature의 행은 관측치(개별 데이터)를 열을 Feature(특성, 변수)를 표현한다. 이 문제에서는 `온도, 강수량, 습도` 세개의 변수를 가진다.\n",
    "- Target은 모델이 예측할 대상으로 행은 개별 관측치, 열은 각 항목에 대한 정답으로 구성한다. 이 문제에서 예측할 항목은 `사과수확량, 오렌지 수확량` 2개의 값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  input: 생산환경 (temp, rainfall, humidity) : (5, 3)\n",
    "environs = [\n",
    "    [73, 67, 43], \n",
    "    [91, 88, 64], \n",
    "    [87, 134, 58], \n",
    "    [102, 43, 37], \n",
    "    [69, 96, 70]\n",
    "]\n",
    "\n",
    "# Targets: 생산량 - (apples, oranges) - (5, 2)\n",
    "apple_orange_output = [\n",
    "    [56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set tenor 처리\n",
    "X = torch.tensor(environs, dtype=torch.float32)\n",
    "y = torch.tensor(apple_orange_output, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight와 bias\n",
    "- weight: 각 feature들이 생산량에 영향을 주었는지의 가중치로 feature에 곱해줄 값.\n",
    "    - 사과, 오렌지의 생산량을 구해야 하므로 가중치가 두개가 된다.\n",
    "    - weight의 shape: `(3, 2)`\n",
    "- bias는 모든 feature들이 0일때 생산량이 얼마일지를 나타내는 값으로 feature와 weight간의 가중합 결과에 더해줄 값이다.\n",
    "    - 사과, 오렌지의 생산량을 구하므로 bias가 두개가 된다.\n",
    "    - bias의 shape: `(2, )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model\n",
    "모델은 weights `w`와 inputs `x`의 내적(dot product)한 값에 bias `b`를 더하는 함수.\n",
    "\n",
    "$$\n",
    "\\hspace{2.5cm} X \\hspace{1.1cm} \\cdot \\hspace{1.2cm} W \\hspace{1.2cm}  + \\hspace{1cm} b \\hspace{2cm}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[ \\begin{array}{cc}\n",
    "73 & 67 & 43 \\\\\n",
    "91 & 88 & 64 \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "69 & 96 & 70\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\cdot\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "w_{11} & w_{21} \\\\\n",
    "w_{12} & w_{22} \\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "+\n",
    "%\n",
    "\\left[ \\begin{array}{cc}\n",
    "b_{1} & b_{2} \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "b_{1} & b_{2} \\\\\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "<center style=\"font-size:0.9em\">\n",
    "$w_{11},\\,w_{12},\\,w_{13}$: 사과 생산량 계산시 각 feature들(생산환경)에 곱할 가중치   <br>\n",
    "$w_{21},\\,w_{22},\\,w_{23}$: 오렌지 생산량 계산시 각 feature들(생산환경)에 곱할 가중치    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"figures/3_unit_layer.png\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight / bias 정의 -> 초기값은 random\n",
    "\n",
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "\n",
    "# weight(3: input feature 갯수 , 2 : output 갯수)\n",
    "# bias (2 : output 갯수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  98.5771,  -22.2787],\n",
       "        [ 126.6030,  -30.1395],\n",
       "        [ 167.5319, -116.6961],\n",
       "        [  92.1661,   43.9837],\n",
       "        [ 121.9021,  -61.9948]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = X @ weight + bias\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13239.7314, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss 계산 (MSE)\n",
    "loss = torch.mean((pred - y)**2) # 전체 추론한 결과의 평균 오차를 계산\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss를 가지고 파라미터들 (weight, bias)의 gradient 계산\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5367,  0.9635],\n",
       "        [ 0.9293, -1.7289],\n",
       "        [-0.0547,  0.5288]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3988.2917, -10433.9824],\n",
       "        [  3640.1494, -13651.4873],\n",
       "        [  2296.7097,  -7850.9058]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.5091,  0.4881]), tensor([  45.1560, -129.4251]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.data,  bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13239.7314453125, 6804.6806640625)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터 업데이트\n",
    "lr = 0.00001\n",
    "\n",
    "weight.data = weight.data - lr * weight.grad\n",
    "bias.data = bias.data - lr * bias.grad\n",
    "\n",
    "pred2 = X @ weight + bias\n",
    "loss2 = torch.mean((pred2 - y) ** 2)\n",
    "\n",
    "loss.item(), loss2.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(3, 2, requires_grad=True)\n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "\n",
    "# 모델 정의\n",
    "def model(X) :\n",
    "    return X @ weight + bias\n",
    "\n",
    "# loss 함수 (MSE)\n",
    "def loss_fn(pred, y) :\n",
    "    return torch.mean((pred - y) ** 2) # 전체 오차의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001/5000 loss = 0.53224\n",
      "0101/5000 loss = 0.53225\n",
      "0201/5000 loss = 0.53225\n",
      "0301/5000 loss = 0.53224\n",
      "0401/5000 loss = 0.53224\n",
      "0501/5000 loss = 0.53224\n",
      "0601/5000 loss = 0.53224\n",
      "0701/5000 loss = 0.53224\n",
      "0801/5000 loss = 0.53224\n",
      "0901/5000 loss = 0.53224\n",
      "1001/5000 loss = 0.53224\n",
      "1101/5000 loss = 0.53224\n",
      "1201/5000 loss = 0.53224\n",
      "1301/5000 loss = 0.53224\n",
      "1401/5000 loss = 0.53224\n",
      "1501/5000 loss = 0.53224\n",
      "1601/5000 loss = 0.53224\n",
      "1701/5000 loss = 0.53223\n",
      "1801/5000 loss = 0.53223\n",
      "1901/5000 loss = 0.53223\n",
      "2001/5000 loss = 0.53223\n",
      "2101/5000 loss = 0.53223\n",
      "2201/5000 loss = 0.53223\n",
      "2301/5000 loss = 0.53223\n",
      "2401/5000 loss = 0.53223\n",
      "2501/5000 loss = 0.53223\n",
      "2601/5000 loss = 0.53223\n",
      "2701/5000 loss = 0.53223\n",
      "2801/5000 loss = 0.53222\n",
      "2901/5000 loss = 0.53222\n",
      "3001/5000 loss = 0.53223\n",
      "3101/5000 loss = 0.53222\n",
      "3201/5000 loss = 0.53222\n",
      "3301/5000 loss = 0.53222\n",
      "3401/5000 loss = 0.53222\n",
      "3501/5000 loss = 0.53222\n",
      "3601/5000 loss = 0.53222\n",
      "3701/5000 loss = 0.53222\n",
      "3801/5000 loss = 0.53222\n",
      "3901/5000 loss = 0.53222\n",
      "4001/5000 loss = 0.53222\n",
      "4101/5000 loss = 0.53221\n",
      "4201/5000 loss = 0.53221\n",
      "4301/5000 loss = 0.53221\n",
      "4401/5000 loss = 0.53221\n",
      "4501/5000 loss = 0.53221\n",
      "4601/5000 loss = 0.53221\n",
      "4701/5000 loss = 0.53221\n",
      "4801/5000 loss = 0.53221\n",
      "4901/5000 loss = 0.53221\n",
      "5000/5000 loss = 0.53221\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "lr = 0.00001\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    # 1. 추론\n",
    "    pred = model(X)\n",
    "    # 2. loss 계산\n",
    "    loss = loss_fn(pred, y)\n",
    "    # 3. parameter들 gradient 계산\n",
    "    loss.backward()\n",
    "    # 4. parameter update\n",
    "    weight.data = weight.data - lr * weight.grad\n",
    "    bias.data = bias.data - lr * bias.grad\n",
    "    # 5. gradient 초기화\n",
    "    weight.grad =  None\n",
    "    bias.grad = None\n",
    "    # 6. log 출력\n",
    "    if epoch % 100 == 0 or epoch == epochs-1 :\n",
    "        print(f\"{epoch+1:04d}/{epochs} loss = {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57.4188,  70.1766],\n",
      "        [ 82.0480, 100.7532],\n",
      "        [118.6512, 132.9801],\n",
      "        [ 21.0494,  37.0318],\n",
      "        [101.9616, 119.1167]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# 새로운 데이터로 추론\n",
    "\n",
    "p = model(X)\n",
    "print(p)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[81.0113, 95.4744]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X = torch.tensor([68, 82, 56],  dtype=torch.float32)\n",
    "new_X = new_X.unsqueeze(dim=0)\n",
    "\n",
    "model(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch built-in 모델을 사용해 Linear Regression 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[73, 67, 43], \n",
    "     [91, 88, 64], \n",
    "     [87, 134, 58], \n",
    "     [102, 43, 37], \n",
    "     [69, 96, 70]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(\n",
    "    [[56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "Pytorch는 nn.Linear 클래스를 통해 Linear Regression 모델을 제공한다.  \n",
    "nn.Linear에 입력 feature의 개수와 출력 값의 개수를 지정하면 random 값으로 초기화한 weight와 bias들을 생성해 모델을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer와 Loss 함수 정의\n",
    "- torch.optim 모듈에 다양한 Optimizer 클래스가 구현되있다.\n",
    "- torch.nn 또는 torch.nn.functional 모듈에 다양한 Loss 함수가 제공된다. 이중 mse_loss() 를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형 회귀 모델 정리\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(3, 2) # 3 : input feature 갯수 , 2 : output 갯수 # weight, bias, X @ weight + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.functional.mse_loss # 함수\n",
    "# loss_fn = torch.nn.MSELoss() 클래스 -> 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3517,  0.1757,  0.2677],\n",
       "         [-0.0240, -0.1319, -0.1833]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0005, -0.3246], requires_grad=True)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer (torch.optim 모듈에 정의) weight.data = weight.data - lr * weight.grad\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), # 최적화 대상 파라미터들을 모델에서 조회해 전달\n",
    "    lr = 0.00001,\n",
    ")\n",
    "\n",
    "list(model.parameters()) # 1번 텐서 weight / 2번 텐서 bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 로직 함수 구현\n",
    "def train (inputs, targets, epochs, model, loss_fn, optimizer)  :\n",
    "    lr = 0.00001\n",
    "    \n",
    "    for epoch in range(epochs) :\n",
    "        # 추론\n",
    "        pred = model(inputs)\n",
    "        # loss 계산\n",
    "        loss = loss_fn(pred, targets) # torch.nn.functionall.mse_losss(pred, targets)\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # 파라미터 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 로그\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1 :\n",
    "            print(f\"{epoch}/{epochs} - {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5000 - 12772.0078125\n",
      "100/5000 - 4.411526679992676\n",
      "200/5000 - 1.1059350967407227\n",
      "300/5000 - 0.5996366143226624\n",
      "400/5000 - 0.522042453289032\n",
      "500/5000 - 0.5101489424705505\n",
      "600/5000 - 0.5083187818527222\n",
      "700/5000 - 0.5080389976501465\n",
      "800/5000 - 0.5079915523529053\n",
      "900/5000 - 0.507982075214386\n",
      "1000/5000 - 0.5079742670059204\n",
      "1100/5000 - 0.5079706907272339\n",
      "1200/5000 - 0.5079675912857056\n",
      "1300/5000 - 0.5079671740531921\n",
      "1400/5000 - 0.5079569816589355\n",
      "1500/5000 - 0.5079562067985535\n",
      "1600/5000 - 0.5079514384269714\n",
      "1700/5000 - 0.507946252822876\n",
      "1800/5000 - 0.5079442262649536\n",
      "1900/5000 - 0.5079383850097656\n",
      "2000/5000 - 0.5079374313354492\n",
      "2100/5000 - 0.5079326033592224\n",
      "2200/5000 - 0.5079256296157837\n",
      "2300/5000 - 0.5079246163368225\n",
      "2400/5000 - 0.5079189538955688\n",
      "2500/5000 - 0.5079146027565002\n",
      "2600/5000 - 0.5079125761985779\n",
      "2700/5000 - 0.5079047679901123\n",
      "2800/5000 - 0.5079020857810974\n",
      "2900/5000 - 0.5078997611999512\n",
      "3000/5000 - 0.5078930854797363\n",
      "3100/5000 - 0.5078886151313782\n",
      "3200/5000 - 0.5078860521316528\n",
      "3300/5000 - 0.5078809261322021\n",
      "3400/5000 - 0.507877767086029\n",
      "3500/5000 - 0.5078725218772888\n",
      "3600/5000 - 0.5078690648078918\n",
      "3700/5000 - 0.507865846157074\n",
      "3800/5000 - 0.50786292552948\n",
      "3900/5000 - 0.507858157157898\n",
      "4000/5000 - 0.5078538656234741\n",
      "4100/5000 - 0.5078513026237488\n",
      "4200/5000 - 0.5078433752059937\n",
      "4300/5000 - 0.5078421831130981\n",
      "4400/5000 - 0.5078379511833191\n",
      "4500/5000 - 0.5078341364860535\n",
      "4600/5000 - 0.5078306794166565\n",
      "4700/5000 - 0.5078235864639282\n",
      "4800/5000 - 0.507820725440979\n",
      "4900/5000 - 0.5078178644180298\n",
      "4999/5000 - 0.5078122019767761\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(3, 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "train(inputs, targets, 5000, model, nn.functional.mse_loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
