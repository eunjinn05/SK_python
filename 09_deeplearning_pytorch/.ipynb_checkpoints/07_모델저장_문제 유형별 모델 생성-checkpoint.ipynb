{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장\n",
    "\n",
    "-   학습한 모델을 저장장치에 파일로 저장하고 나중에 불러와 사용(추가 학습, 예측 서비스) 할 수 있도록 한다.\n",
    "-   파이토치는 **모델의 파라미터만 저장**하는 방법과 **모델 구조와 파라미터 모두를 저장**하는 두가지 방식을 제공한다.\n",
    "-   저장 함수\n",
    "    -   `torch.save(저장할 객체, 저장경로)`\n",
    "-   보통 저장파일의 확장자는 `pt`나 `pth` 를 지정한다.\n",
    "\n",
    "## 모델 전체 저장하기 및 불러오기\n",
    "\n",
    "-   저장하기\n",
    "    -   `torch.save(model, 저장경로)`\n",
    "-   불러오기\n",
    "    -   `load_model = torch.load(저장경로)`\n",
    "-   저장시 **pickle**을 이용해 직렬화하기 때문에 불어오는 실행환경에도 모델을 저장할 때 사용한 클래스가 있어야 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델의 파라미터만 저장\n",
    "\n",
    "-   모델을 구성하는 파라미터만 저장한다.\n",
    "-   모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "-   모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "\n",
    "-   모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "-   `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "-   모델의 state_dict을 조회 후 저장한다.\n",
    "    -   `torch.save(model.state_dict(), \"저장경로\")`\n",
    "-   생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    -   `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint를 저장 및 불러오기\n",
    "\n",
    "-   학습이 끝나지 않은 모델을 저장 후 나중에 이어서 학습시킬 경우에는 모델의 구조, 파라미터 뿐만 아니라 optimizer, loss 함수등 학습에 필요한 객체들을 저장해야 한다.\n",
    "-   Dictionary에 저장하려는 값들을 key-value 쌍으로 저장후 `torch.save()`를 이용해 저장한다.\n",
    "\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch':epoch,\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'loss':train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameter())\n",
    "\n",
    "# loading된 checkpoint 값 이용해 이전 학습상태 복원\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel (nn.Module) :\n",
    "    def __init__(self) :\n",
    "        # Linear의 parameter개수 : input feature X output feature + output feature\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(3,4) # 3 X 4 + 4 \n",
    "        self.lr2 = nn.Linear(4,2)\n",
    "        self.relu = nn.ReLU() # activation func parameter 없음 relu(X) => max(X, 0)\n",
    "\n",
    "    def forward(self, X) :\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0082, -0.7021],\n",
       "        [-0.1756, -0.3419],\n",
       "        [-0.0499, -0.3615],\n",
       "        [ 0.0662, -0.2060],\n",
       "        [ 0.0372, -0.2418]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('saved_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 저장\n",
    "torch.save(model, 'saved_models/my_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장된 모델 load\n",
    "load_model = torch.load('saved_models/my_model.pt')\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=4, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_layer = model.lr1\n",
    "lr_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.1756, -0.1666,  0.1756],\n",
       "         [ 0.3354, -0.4799, -0.0765],\n",
       "         [ 0.5011, -0.4655, -0.4762],\n",
       "         [ 0.1042, -0.2033, -0.5108]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.3932, -0.2339,  0.2176,  0.2400], requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#layer에서 weight bias 조회\n",
    "lr_weight = lr_layer.weight\n",
    "lr_bias = lr_layer.bias\n",
    "\n",
    "lr_weight, lr_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.1756, -0.1666,  0.1756],\n",
       "                      [ 0.3354, -0.4799, -0.0765],\n",
       "                      [ 0.5011, -0.4655, -0.4762],\n",
       "                      [ 0.1042, -0.2033, -0.5108]])),\n",
       "             ('lr1.bias', tensor([ 0.3932, -0.2339,  0.2176,  0.2400])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[ 0.3716, -0.1085,  0.1544, -0.3164],\n",
       "                      [ 0.4602,  0.1761, -0.4670,  0.3406]])),\n",
       "             ('lr2.bias', tensor([-0.1050, -0.4179]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr1.weight', 'lr1.bias', 'lr2.weight', 'lr2.bias'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장\n",
    "torch.save(state_dict, 'saved_models/my_model_parameter.pt')\n",
    "sd = torch.load('saved_models/my_model_parameter.pt')\n",
    "\n",
    "# 모델 객체를 생성 -> load한 state_dict 모델 파라미터에 덮어쓴다\n",
    "new_model = MyModel()\n",
    "new_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/miniconda3/envs/ml/lib/python3.12/site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Linear: 1-1                            16\n",
       "├─Linear: 1-2                            10\n",
       "├─ReLU: 1-3                              --\n",
       "=================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 2]                  --\n",
       "├─Linear: 1-1                            [100, 4]                  16\n",
       "├─ReLU: 1-2                              [100, 4]                  --\n",
       "├─Linear: 1-3                            [100, 2]                  10\n",
       "==========================================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,(100, 3),dtypes=[torch.float32])  #(model, (input data의 shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- 해결하려는 문제 유형에 따라 출력 Layer의 구조가 바뀐다.\n",
    "- 딥러닝 구조에서 **Feature를 추출하는 Layer 들을 Backbone** 이라고 하고 **추론하는 Layer들을 Head** 라고 한다. \n",
    "\n",
    "\n",
    "> - MLP(Multi Layer Perceptron), DNN(Deep Neural Network), ANN(Artificial Neural Network)\n",
    ">     -   Fully Connected Layer(nn.Linear)로 구성된 딥러닝 모델\n",
    ">     -   input feature들 모두에 대응하는 weight들(가중치)을 사용한다.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Regression(회귀)\n",
    "\n",
    "### Boston Housing Dataset\n",
    "\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "\n",
    "-   CRIM: 범죄율\n",
    "-   ZN: 25,000 평방피트당 주거지역 비율\n",
    "-   INDUS: 비소매 상업지구 비율\n",
    "-   CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "-   NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "-   RM: 주택당 방의 수\n",
    "-   AGE: 1940년 이전에 건설된 주택의 비율\n",
    "-   DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "-   RAD: 고속도로 접근성\n",
    "-   TAX: 재산세율\n",
    "-   PTRATIO: 학생/교사 비율\n",
    "-   B: 흑인 비율\n",
    "-   LSTAT: 하위 계층 비율\n",
    "    <br><br>\n",
    "-   **Target**\n",
    "    -   MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset 준비\n",
    "## 데이터 불러오기 -> 전처리 -> Dataset -> DataLoader\n",
    "df = pd.read_csv('data/boston_hosing.csv')\n",
    "\n",
    "X_boston = df.drop(columns='MEDV').values\n",
    "y_boston = df['MEDV'].values\n",
    "y_boston = y_boston.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_boston, y_boston, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모델 -> 선형회귀 기반 : 전처리 - 연속형 : Feature scaling, 범주형 One Hot Encoding\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "X_train_scaled = Scaler.fit_transform(X_train)\n",
    "X_test_scaled = Scaler.transform(X_test)\n",
    "\n",
    "# dataset\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=404, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=102)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모델 정의\n",
    "# nn.Model 상속\n",
    "# __init__() : layer 객체들 초기화, forward() : 추론 계산과정 정의\n",
    "import torch.nn as nn\n",
    "\n",
    "class BostonHousingModeling(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(13, 16)\n",
    "        self.lr2 = nn.Linear(16, 8)\n",
    "        self.lr3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU() # activation 함수\n",
    "        \n",
    "    def forward (self, X) :\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "        output = self.lr3(X) # 출력 layer(output)은 activation 함수 통과 X / 값의 범위가 정해져있고 그 범위의 값을 반환하는 함수가 있을 경우에는 사용할 수 있다 / 0 ~ 1 함수 - logstic (sigmoid)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape, y_train.shape # 13개 input 1개 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonHousingModeling                    [404, 1]                  --\n",
       "├─Linear: 1-1                            [404, 16]                 224\n",
       "├─ReLU: 1-2                              [404, 16]                 --\n",
       "├─Linear: 1-3                            [404, 8]                  136\n",
       "├─ReLU: 1-4                              [404, 8]                  --\n",
       "├─Linear: 1-5                            [404, 1]                  9\n",
       "==========================================================================================\n",
       "Total params: 369\n",
       "Trainable params: 369\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.10\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_model = BostonHousingModeling().to(device)\n",
    "\n",
    "summary(boston_model, (404, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001 / 1000] - train loss : 602.0856323242188, valid_loss : 580.1764526367188\n",
      "[0002 / 1000] - train loss : 601.6622314453125, valid_loss : 579.8179931640625\n",
      "[0003 / 1000] - train loss : 601.2432861328125, valid_loss : 579.4624633789062\n",
      "[0004 / 1000] - train loss : 600.8281860351562, valid_loss : 579.113037109375\n",
      "[0005 / 1000] - train loss : 600.4169921875, valid_loss : 578.7623901367188\n",
      "[0006 / 1000] - train loss : 600.006591796875, valid_loss : 578.4111328125\n",
      "[0007 / 1000] - train loss : 599.5994873046875, valid_loss : 578.0565795898438\n",
      "[0008 / 1000] - train loss : 599.1917724609375, valid_loss : 577.7015380859375\n",
      "[0009 / 1000] - train loss : 598.7833251953125, valid_loss : 577.3465576171875\n",
      "[0010 / 1000] - train loss : 598.3739624023438, valid_loss : 576.9861450195312\n",
      "[0011 / 1000] - train loss : 597.9638061523438, valid_loss : 576.6162109375\n",
      "[0012 / 1000] - train loss : 597.554443359375, valid_loss : 576.240478515625\n",
      "[0013 / 1000] - train loss : 597.1470336914062, valid_loss : 575.8668212890625\n",
      "[0014 / 1000] - train loss : 596.7392578125, valid_loss : 575.4910888671875\n",
      "[0015 / 1000] - train loss : 596.3292846679688, valid_loss : 575.111083984375\n",
      "[0016 / 1000] - train loss : 595.917724609375, valid_loss : 574.731201171875\n",
      "[0017 / 1000] - train loss : 595.5052490234375, valid_loss : 574.3515014648438\n",
      "[0018 / 1000] - train loss : 595.0912475585938, valid_loss : 573.970947265625\n",
      "[0019 / 1000] - train loss : 594.6753540039062, valid_loss : 573.5913696289062\n",
      "[0020 / 1000] - train loss : 594.2577514648438, valid_loss : 573.2089233398438\n",
      "[0021 / 1000] - train loss : 593.8380737304688, valid_loss : 572.8240356445312\n",
      "[0022 / 1000] - train loss : 593.4166259765625, valid_loss : 572.4376831054688\n",
      "[0023 / 1000] - train loss : 592.99169921875, valid_loss : 572.0476684570312\n",
      "[0024 / 1000] - train loss : 592.5640258789062, valid_loss : 571.6541137695312\n",
      "[0025 / 1000] - train loss : 592.1324462890625, valid_loss : 571.2586059570312\n",
      "[0026 / 1000] - train loss : 591.6948852539062, valid_loss : 570.8593139648438\n",
      "[0027 / 1000] - train loss : 591.251220703125, valid_loss : 570.4558715820312\n",
      "[0028 / 1000] - train loss : 590.802001953125, valid_loss : 570.0490112304688\n",
      "[0029 / 1000] - train loss : 590.3475952148438, valid_loss : 569.6357421875\n",
      "[0030 / 1000] - train loss : 589.8888549804688, valid_loss : 569.2161254882812\n",
      "[0031 / 1000] - train loss : 589.4246826171875, valid_loss : 568.7890014648438\n",
      "[0032 / 1000] - train loss : 588.9545288085938, valid_loss : 568.3546142578125\n",
      "[0033 / 1000] - train loss : 588.4788208007812, valid_loss : 567.9125366210938\n",
      "[0034 / 1000] - train loss : 587.997314453125, valid_loss : 567.4645385742188\n",
      "[0035 / 1000] - train loss : 587.509521484375, valid_loss : 567.0119018554688\n",
      "[0036 / 1000] - train loss : 587.0150146484375, valid_loss : 566.5526123046875\n",
      "[0037 / 1000] - train loss : 586.5132446289062, valid_loss : 566.0856323242188\n",
      "[0038 / 1000] - train loss : 586.0042724609375, valid_loss : 565.6061401367188\n",
      "[0039 / 1000] - train loss : 585.4882202148438, valid_loss : 565.113037109375\n",
      "[0040 / 1000] - train loss : 584.962890625, valid_loss : 564.6124877929688\n",
      "[0041 / 1000] - train loss : 584.4278564453125, valid_loss : 564.1006469726562\n",
      "[0042 / 1000] - train loss : 583.8842163085938, valid_loss : 563.5816040039062\n",
      "[0043 / 1000] - train loss : 583.3312377929688, valid_loss : 563.0557861328125\n",
      "[0044 / 1000] - train loss : 582.7705688476562, valid_loss : 562.5217895507812\n",
      "[0045 / 1000] - train loss : 582.2022705078125, valid_loss : 561.9793090820312\n",
      "[0046 / 1000] - train loss : 581.6249389648438, valid_loss : 561.4257202148438\n",
      "[0047 / 1000] - train loss : 581.0382080078125, valid_loss : 560.8624267578125\n",
      "[0048 / 1000] - train loss : 580.4404907226562, valid_loss : 560.2886962890625\n",
      "[0049 / 1000] - train loss : 579.8327026367188, valid_loss : 559.7054443359375\n",
      "[0050 / 1000] - train loss : 579.2154541015625, valid_loss : 559.1121826171875\n",
      "[0051 / 1000] - train loss : 578.5875244140625, valid_loss : 558.5047607421875\n",
      "[0052 / 1000] - train loss : 577.9491577148438, valid_loss : 557.8848876953125\n",
      "[0053 / 1000] - train loss : 577.298583984375, valid_loss : 557.254638671875\n",
      "[0054 / 1000] - train loss : 576.6366577148438, valid_loss : 556.611572265625\n",
      "[0055 / 1000] - train loss : 575.9640502929688, valid_loss : 555.95556640625\n",
      "[0056 / 1000] - train loss : 575.2803344726562, valid_loss : 555.2843627929688\n",
      "[0057 / 1000] - train loss : 574.5836181640625, valid_loss : 554.5994873046875\n",
      "[0058 / 1000] - train loss : 573.8742065429688, valid_loss : 553.900146484375\n",
      "[0059 / 1000] - train loss : 573.1516723632812, valid_loss : 553.1889038085938\n",
      "[0060 / 1000] - train loss : 572.4135131835938, valid_loss : 552.4627685546875\n",
      "[0061 / 1000] - train loss : 571.6603393554688, valid_loss : 551.7219848632812\n",
      "[0062 / 1000] - train loss : 570.8924560546875, valid_loss : 550.9642944335938\n",
      "[0063 / 1000] - train loss : 570.1094360351562, valid_loss : 550.18798828125\n",
      "[0064 / 1000] - train loss : 569.3114013671875, valid_loss : 549.3977661132812\n",
      "[0065 / 1000] - train loss : 568.49658203125, valid_loss : 548.5930786132812\n",
      "[0066 / 1000] - train loss : 567.6648559570312, valid_loss : 547.76904296875\n",
      "[0067 / 1000] - train loss : 566.817138671875, valid_loss : 546.9271850585938\n",
      "[0068 / 1000] - train loss : 565.950439453125, valid_loss : 546.06884765625\n",
      "[0069 / 1000] - train loss : 565.06201171875, valid_loss : 545.1947021484375\n",
      "[0070 / 1000] - train loss : 564.1509399414062, valid_loss : 544.3040161132812\n",
      "[0071 / 1000] - train loss : 563.2193603515625, valid_loss : 543.39599609375\n",
      "[0072 / 1000] - train loss : 562.2647705078125, valid_loss : 542.4696044921875\n",
      "[0073 / 1000] - train loss : 561.281982421875, valid_loss : 541.5189819335938\n",
      "[0074 / 1000] - train loss : 560.2755126953125, valid_loss : 540.5399169921875\n",
      "[0075 / 1000] - train loss : 559.2457275390625, valid_loss : 539.5294799804688\n",
      "[0076 / 1000] - train loss : 558.1897583007812, valid_loss : 538.4931640625\n",
      "[0077 / 1000] - train loss : 557.10986328125, valid_loss : 537.4326171875\n",
      "[0078 / 1000] - train loss : 556.001220703125, valid_loss : 536.3457641601562\n",
      "[0079 / 1000] - train loss : 554.8658447265625, valid_loss : 535.2345581054688\n",
      "[0080 / 1000] - train loss : 553.7024536132812, valid_loss : 534.0913696289062\n",
      "[0081 / 1000] - train loss : 552.5119018554688, valid_loss : 532.9249267578125\n",
      "[0082 / 1000] - train loss : 551.2897338867188, valid_loss : 531.7353515625\n",
      "[0083 / 1000] - train loss : 550.0374755859375, valid_loss : 530.5181274414062\n",
      "[0084 / 1000] - train loss : 548.74853515625, valid_loss : 529.2659301757812\n",
      "[0085 / 1000] - train loss : 547.4214477539062, valid_loss : 527.9842529296875\n",
      "[0086 / 1000] - train loss : 546.0569458007812, valid_loss : 526.673583984375\n",
      "[0087 / 1000] - train loss : 544.6585083007812, valid_loss : 525.3267211914062\n",
      "[0088 / 1000] - train loss : 543.2293701171875, valid_loss : 523.952392578125\n",
      "[0089 / 1000] - train loss : 541.76611328125, valid_loss : 522.546875\n",
      "[0090 / 1000] - train loss : 540.2639770507812, valid_loss : 521.1035766601562\n",
      "[0091 / 1000] - train loss : 538.7241821289062, valid_loss : 519.632568359375\n",
      "[0092 / 1000] - train loss : 537.1513671875, valid_loss : 518.1334838867188\n",
      "[0093 / 1000] - train loss : 535.5447998046875, valid_loss : 516.5986938476562\n",
      "[0094 / 1000] - train loss : 533.9037475585938, valid_loss : 515.031982421875\n",
      "[0095 / 1000] - train loss : 532.2229614257812, valid_loss : 513.4308471679688\n",
      "[0096 / 1000] - train loss : 530.5001831054688, valid_loss : 511.7969970703125\n",
      "[0097 / 1000] - train loss : 528.7424926757812, valid_loss : 510.1282043457031\n",
      "[0098 / 1000] - train loss : 526.9459228515625, valid_loss : 508.4220275878906\n",
      "[0099 / 1000] - train loss : 525.1094360351562, valid_loss : 506.67340087890625\n",
      "[0100 / 1000] - train loss : 523.2295532226562, valid_loss : 504.884033203125\n",
      "[0101 / 1000] - train loss : 521.3076782226562, valid_loss : 503.06231689453125\n",
      "[0102 / 1000] - train loss : 519.3385009765625, valid_loss : 501.2071228027344\n",
      "[0103 / 1000] - train loss : 517.326904296875, valid_loss : 499.31292724609375\n",
      "[0104 / 1000] - train loss : 515.2758178710938, valid_loss : 497.3800964355469\n",
      "[0105 / 1000] - train loss : 513.1843872070312, valid_loss : 495.403076171875\n",
      "[0106 / 1000] - train loss : 511.05169677734375, valid_loss : 493.3863220214844\n",
      "[0107 / 1000] - train loss : 508.8783264160156, valid_loss : 491.33367919921875\n",
      "[0108 / 1000] - train loss : 506.66461181640625, valid_loss : 489.24127197265625\n",
      "[0109 / 1000] - train loss : 504.4090270996094, valid_loss : 487.1077575683594\n",
      "[0110 / 1000] - train loss : 502.1092834472656, valid_loss : 484.93310546875\n",
      "[0111 / 1000] - train loss : 499.7652893066406, valid_loss : 482.7225341796875\n",
      "[0112 / 1000] - train loss : 497.37823486328125, valid_loss : 480.4580383300781\n",
      "[0113 / 1000] - train loss : 494.9500427246094, valid_loss : 478.15582275390625\n",
      "[0114 / 1000] - train loss : 492.4812316894531, valid_loss : 475.8105773925781\n",
      "[0115 / 1000] - train loss : 489.9696044921875, valid_loss : 473.4251708984375\n",
      "[0116 / 1000] - train loss : 487.4124755859375, valid_loss : 471.0028076171875\n",
      "[0117 / 1000] - train loss : 484.813232421875, valid_loss : 468.5422668457031\n",
      "[0118 / 1000] - train loss : 482.1742248535156, valid_loss : 466.0426940917969\n",
      "[0119 / 1000] - train loss : 479.4921875, valid_loss : 463.5034484863281\n",
      "[0120 / 1000] - train loss : 476.7685546875, valid_loss : 460.92669677734375\n",
      "[0121 / 1000] - train loss : 474.004638671875, valid_loss : 458.31488037109375\n",
      "[0122 / 1000] - train loss : 471.2008361816406, valid_loss : 455.66851806640625\n",
      "[0123 / 1000] - train loss : 468.3574523925781, valid_loss : 452.9881896972656\n",
      "[0124 / 1000] - train loss : 465.4757995605469, valid_loss : 450.27490234375\n",
      "[0125 / 1000] - train loss : 462.5559387207031, valid_loss : 447.5205383300781\n",
      "[0126 / 1000] - train loss : 459.595947265625, valid_loss : 444.72821044921875\n",
      "[0127 / 1000] - train loss : 456.59649658203125, valid_loss : 441.9000549316406\n",
      "[0128 / 1000] - train loss : 453.55902099609375, valid_loss : 439.0322570800781\n",
      "[0129 / 1000] - train loss : 450.4836120605469, valid_loss : 436.13177490234375\n",
      "[0130 / 1000] - train loss : 447.368896484375, valid_loss : 433.19976806640625\n",
      "[0131 / 1000] - train loss : 444.2149963378906, valid_loss : 430.2354431152344\n",
      "[0132 / 1000] - train loss : 441.02490234375, valid_loss : 427.23687744140625\n",
      "[0133 / 1000] - train loss : 437.79620361328125, valid_loss : 424.20733642578125\n",
      "[0134 / 1000] - train loss : 434.5257873535156, valid_loss : 421.1464538574219\n",
      "[0135 / 1000] - train loss : 431.2200927734375, valid_loss : 418.0533142089844\n",
      "[0136 / 1000] - train loss : 427.8807373046875, valid_loss : 414.931396484375\n",
      "[0137 / 1000] - train loss : 424.5078430175781, valid_loss : 411.7819519042969\n",
      "[0138 / 1000] - train loss : 421.1019592285156, valid_loss : 408.602294921875\n",
      "[0139 / 1000] - train loss : 417.6614074707031, valid_loss : 405.3964538574219\n",
      "[0140 / 1000] - train loss : 414.18536376953125, valid_loss : 402.1654357910156\n",
      "[0141 / 1000] - train loss : 410.67864990234375, valid_loss : 398.9078674316406\n",
      "[0142 / 1000] - train loss : 407.1413269042969, valid_loss : 395.6234436035156\n",
      "[0143 / 1000] - train loss : 403.5749206542969, valid_loss : 392.3135070800781\n",
      "[0144 / 1000] - train loss : 399.9775390625, valid_loss : 388.9770202636719\n",
      "[0145 / 1000] - train loss : 396.3530578613281, valid_loss : 385.6141357421875\n",
      "[0146 / 1000] - train loss : 392.7022399902344, valid_loss : 382.2286376953125\n",
      "[0147 / 1000] - train loss : 389.0242919921875, valid_loss : 378.8192138671875\n",
      "[0148 / 1000] - train loss : 385.3210754394531, valid_loss : 375.38922119140625\n",
      "[0149 / 1000] - train loss : 381.5954895019531, valid_loss : 371.9399108886719\n",
      "[0150 / 1000] - train loss : 377.84857177734375, valid_loss : 368.473388671875\n",
      "[0151 / 1000] - train loss : 374.08221435546875, valid_loss : 364.9888610839844\n",
      "[0152 / 1000] - train loss : 370.2964782714844, valid_loss : 361.48577880859375\n",
      "[0153 / 1000] - train loss : 366.4930114746094, valid_loss : 357.966064453125\n",
      "[0154 / 1000] - train loss : 362.67218017578125, valid_loss : 354.4336242675781\n",
      "[0155 / 1000] - train loss : 358.8359375, valid_loss : 350.8893737792969\n",
      "[0156 / 1000] - train loss : 354.9836120605469, valid_loss : 347.3330993652344\n",
      "[0157 / 1000] - train loss : 351.1158752441406, valid_loss : 343.7646179199219\n",
      "[0158 / 1000] - train loss : 347.234375, valid_loss : 340.1864929199219\n",
      "[0159 / 1000] - train loss : 343.3421936035156, valid_loss : 336.6014709472656\n",
      "[0160 / 1000] - train loss : 339.44091796875, valid_loss : 333.0093078613281\n",
      "[0161 / 1000] - train loss : 335.5315856933594, valid_loss : 329.41131591796875\n",
      "[0162 / 1000] - train loss : 331.6147155761719, valid_loss : 325.8104248046875\n",
      "[0163 / 1000] - train loss : 327.6932678222656, valid_loss : 322.2080383300781\n",
      "[0164 / 1000] - train loss : 323.76812744140625, valid_loss : 318.6060485839844\n",
      "[0165 / 1000] - train loss : 319.8409118652344, valid_loss : 315.00604248046875\n",
      "[0166 / 1000] - train loss : 315.9122009277344, valid_loss : 311.4082946777344\n",
      "[0167 / 1000] - train loss : 311.98370361328125, valid_loss : 307.8141174316406\n",
      "[0168 / 1000] - train loss : 308.0566711425781, valid_loss : 304.223388671875\n",
      "[0169 / 1000] - train loss : 304.132568359375, valid_loss : 300.6388854980469\n",
      "[0170 / 1000] - train loss : 300.2137756347656, valid_loss : 297.0618591308594\n",
      "[0171 / 1000] - train loss : 296.301513671875, valid_loss : 293.4940185546875\n",
      "[0172 / 1000] - train loss : 292.3982849121094, valid_loss : 289.93731689453125\n",
      "[0173 / 1000] - train loss : 288.505859375, valid_loss : 286.39276123046875\n",
      "[0174 / 1000] - train loss : 284.6246032714844, valid_loss : 282.8620910644531\n",
      "[0175 / 1000] - train loss : 280.7550354003906, valid_loss : 279.3458251953125\n",
      "[0176 / 1000] - train loss : 276.8974304199219, valid_loss : 275.8436279296875\n",
      "[0177 / 1000] - train loss : 273.0522155761719, valid_loss : 272.3535461425781\n",
      "[0178 / 1000] - train loss : 269.22320556640625, valid_loss : 268.87994384765625\n",
      "[0179 / 1000] - train loss : 265.41241455078125, valid_loss : 265.4241027832031\n",
      "[0180 / 1000] - train loss : 261.6211853027344, valid_loss : 261.9888610839844\n",
      "[0181 / 1000] - train loss : 257.8499755859375, valid_loss : 258.5757751464844\n",
      "[0182 / 1000] - train loss : 254.1013641357422, valid_loss : 255.1857147216797\n",
      "[0183 / 1000] - train loss : 250.37596130371094, valid_loss : 251.8190460205078\n",
      "[0184 / 1000] - train loss : 246.67420959472656, valid_loss : 248.47628784179688\n",
      "[0185 / 1000] - train loss : 242.9991912841797, valid_loss : 245.1596221923828\n",
      "[0186 / 1000] - train loss : 239.3513641357422, valid_loss : 241.87049865722656\n",
      "[0187 / 1000] - train loss : 235.732421875, valid_loss : 238.608154296875\n",
      "[0188 / 1000] - train loss : 232.14402770996094, valid_loss : 235.37535095214844\n",
      "[0189 / 1000] - train loss : 228.58737182617188, valid_loss : 232.17245483398438\n",
      "[0190 / 1000] - train loss : 225.0636749267578, valid_loss : 228.99942016601562\n",
      "[0191 / 1000] - train loss : 221.57496643066406, valid_loss : 225.858642578125\n",
      "[0192 / 1000] - train loss : 218.1226043701172, valid_loss : 222.7507781982422\n",
      "[0193 / 1000] - train loss : 214.70767211914062, valid_loss : 219.67723083496094\n",
      "[0194 / 1000] - train loss : 211.33078002929688, valid_loss : 216.6398162841797\n",
      "[0195 / 1000] - train loss : 207.9922332763672, valid_loss : 213.63980102539062\n",
      "[0196 / 1000] - train loss : 204.69351196289062, valid_loss : 210.6780242919922\n",
      "[0197 / 1000] - train loss : 201.4356689453125, valid_loss : 207.7542266845703\n",
      "[0198 / 1000] - train loss : 198.22035217285156, valid_loss : 204.8700408935547\n",
      "[0199 / 1000] - train loss : 195.04812622070312, valid_loss : 202.02627563476562\n",
      "[0200 / 1000] - train loss : 191.9197540283203, valid_loss : 199.22357177734375\n",
      "[0201 / 1000] - train loss : 188.8363800048828, valid_loss : 196.46261596679688\n",
      "[0202 / 1000] - train loss : 185.79876708984375, valid_loss : 193.74363708496094\n",
      "[0203 / 1000] - train loss : 182.80763244628906, valid_loss : 191.0658721923828\n",
      "[0204 / 1000] - train loss : 179.86297607421875, valid_loss : 188.42933654785156\n",
      "[0205 / 1000] - train loss : 176.96441650390625, valid_loss : 185.83441162109375\n",
      "[0206 / 1000] - train loss : 174.1127471923828, valid_loss : 183.28341674804688\n",
      "[0207 / 1000] - train loss : 171.30963134765625, valid_loss : 180.7768096923828\n",
      "[0208 / 1000] - train loss : 168.5550537109375, valid_loss : 178.31484985351562\n",
      "[0209 / 1000] - train loss : 165.84878540039062, valid_loss : 175.8977813720703\n",
      "[0210 / 1000] - train loss : 163.19203186035156, valid_loss : 173.52577209472656\n",
      "[0211 / 1000] - train loss : 160.58473205566406, valid_loss : 171.1988983154297\n",
      "[0212 / 1000] - train loss : 158.02734375, valid_loss : 168.9168243408203\n",
      "[0213 / 1000] - train loss : 155.5193328857422, valid_loss : 166.679931640625\n",
      "[0214 / 1000] - train loss : 153.061279296875, valid_loss : 164.48822021484375\n",
      "[0215 / 1000] - train loss : 150.65330505371094, valid_loss : 162.34156799316406\n",
      "[0216 / 1000] - train loss : 148.2953643798828, valid_loss : 160.23959350585938\n",
      "[0217 / 1000] - train loss : 145.98736572265625, valid_loss : 158.18194580078125\n",
      "[0218 / 1000] - train loss : 143.72911071777344, valid_loss : 156.16790771484375\n",
      "[0219 / 1000] - train loss : 141.5200958251953, valid_loss : 154.1973419189453\n",
      "[0220 / 1000] - train loss : 139.35997009277344, valid_loss : 152.27044677734375\n",
      "[0221 / 1000] - train loss : 137.24867248535156, valid_loss : 150.38693237304688\n",
      "[0222 / 1000] - train loss : 135.1853790283203, valid_loss : 148.54641723632812\n",
      "[0223 / 1000] - train loss : 133.1697540283203, valid_loss : 146.74830627441406\n",
      "[0224 / 1000] - train loss : 131.20094299316406, valid_loss : 144.9914093017578\n",
      "[0225 / 1000] - train loss : 129.2786102294922, valid_loss : 143.2743377685547\n",
      "[0226 / 1000] - train loss : 127.40059661865234, valid_loss : 141.5971221923828\n",
      "[0227 / 1000] - train loss : 125.56741333007812, valid_loss : 139.95965576171875\n",
      "[0228 / 1000] - train loss : 123.77843475341797, valid_loss : 138.36093139648438\n",
      "[0229 / 1000] - train loss : 122.03303527832031, valid_loss : 136.8008270263672\n",
      "[0230 / 1000] - train loss : 120.32999420166016, valid_loss : 135.2787628173828\n",
      "[0231 / 1000] - train loss : 118.66868591308594, valid_loss : 133.7941436767578\n",
      "[0232 / 1000] - train loss : 117.04766082763672, valid_loss : 132.3461456298828\n",
      "[0233 / 1000] - train loss : 115.46607971191406, valid_loss : 130.93386840820312\n",
      "[0234 / 1000] - train loss : 113.92332458496094, valid_loss : 129.55580139160156\n",
      "[0235 / 1000] - train loss : 112.41812133789062, valid_loss : 128.2117462158203\n",
      "[0236 / 1000] - train loss : 110.95000457763672, valid_loss : 126.90042877197266\n",
      "[0237 / 1000] - train loss : 109.51792907714844, valid_loss : 125.62147521972656\n",
      "[0238 / 1000] - train loss : 108.12055969238281, valid_loss : 124.37320709228516\n",
      "[0239 / 1000] - train loss : 106.75737762451172, valid_loss : 123.15523529052734\n",
      "[0240 / 1000] - train loss : 105.4266357421875, valid_loss : 121.96697235107422\n",
      "[0241 / 1000] - train loss : 104.12735748291016, valid_loss : 120.80757904052734\n",
      "[0242 / 1000] - train loss : 102.85897827148438, valid_loss : 119.67605590820312\n",
      "[0243 / 1000] - train loss : 101.62045288085938, valid_loss : 118.5712661743164\n",
      "[0244 / 1000] - train loss : 100.4112548828125, valid_loss : 117.49211120605469\n",
      "[0245 / 1000] - train loss : 99.23062133789062, valid_loss : 116.43859100341797\n",
      "[0246 / 1000] - train loss : 98.07746124267578, valid_loss : 115.40990447998047\n",
      "[0247 / 1000] - train loss : 96.95082092285156, valid_loss : 114.40434265136719\n",
      "[0248 / 1000] - train loss : 95.8501968383789, valid_loss : 113.42150115966797\n",
      "[0249 / 1000] - train loss : 94.77448272705078, valid_loss : 112.46121215820312\n",
      "[0250 / 1000] - train loss : 93.7227554321289, valid_loss : 111.52283477783203\n",
      "[0251 / 1000] - train loss : 92.69432830810547, valid_loss : 110.6054916381836\n",
      "[0252 / 1000] - train loss : 91.68732452392578, valid_loss : 109.70732879638672\n",
      "[0253 / 1000] - train loss : 90.70149230957031, valid_loss : 108.8284912109375\n",
      "[0254 / 1000] - train loss : 89.73648071289062, valid_loss : 107.96768951416016\n",
      "[0255 / 1000] - train loss : 88.79032897949219, valid_loss : 107.12482452392578\n",
      "[0256 / 1000] - train loss : 87.86294555664062, valid_loss : 106.29959106445312\n",
      "[0257 / 1000] - train loss : 86.95335388183594, valid_loss : 105.49057006835938\n",
      "[0258 / 1000] - train loss : 86.06151580810547, valid_loss : 104.6977310180664\n",
      "[0259 / 1000] - train loss : 85.18695831298828, valid_loss : 103.92086791992188\n",
      "[0260 / 1000] - train loss : 84.32904052734375, valid_loss : 103.15959930419922\n",
      "[0261 / 1000] - train loss : 83.4871597290039, valid_loss : 102.41281127929688\n",
      "[0262 / 1000] - train loss : 82.66072845458984, valid_loss : 101.67997741699219\n",
      "[0263 / 1000] - train loss : 81.84809112548828, valid_loss : 100.96096801757812\n",
      "[0264 / 1000] - train loss : 81.04972076416016, valid_loss : 100.25537109375\n",
      "[0265 / 1000] - train loss : 80.2652587890625, valid_loss : 99.56273651123047\n",
      "[0266 / 1000] - train loss : 79.49363708496094, valid_loss : 98.8821792602539\n",
      "[0267 / 1000] - train loss : 78.7347412109375, valid_loss : 98.2137451171875\n",
      "[0268 / 1000] - train loss : 77.98820495605469, valid_loss : 97.5567855834961\n",
      "[0269 / 1000] - train loss : 77.25343322753906, valid_loss : 96.9098129272461\n",
      "[0270 / 1000] - train loss : 76.52935791015625, valid_loss : 96.2738037109375\n",
      "[0271 / 1000] - train loss : 75.81629180908203, valid_loss : 95.64850616455078\n",
      "[0272 / 1000] - train loss : 75.11399841308594, valid_loss : 95.03355407714844\n",
      "[0273 / 1000] - train loss : 74.42185974121094, valid_loss : 94.42903900146484\n",
      "[0274 / 1000] - train loss : 73.73974609375, valid_loss : 93.83514404296875\n",
      "[0275 / 1000] - train loss : 73.06745147705078, valid_loss : 93.2506332397461\n",
      "[0276 / 1000] - train loss : 72.40455627441406, valid_loss : 92.67449951171875\n",
      "[0277 / 1000] - train loss : 71.7505874633789, valid_loss : 92.1067123413086\n",
      "[0278 / 1000] - train loss : 71.10535430908203, valid_loss : 91.54721069335938\n",
      "[0279 / 1000] - train loss : 70.46839904785156, valid_loss : 90.99624633789062\n",
      "[0280 / 1000] - train loss : 69.83976745605469, valid_loss : 90.45367431640625\n",
      "[0281 / 1000] - train loss : 69.2195816040039, valid_loss : 89.91905212402344\n",
      "[0282 / 1000] - train loss : 68.60739135742188, valid_loss : 89.3911361694336\n",
      "[0283 / 1000] - train loss : 68.00304412841797, valid_loss : 88.87010955810547\n",
      "[0284 / 1000] - train loss : 67.40645599365234, valid_loss : 88.3563232421875\n",
      "[0285 / 1000] - train loss : 66.81645202636719, valid_loss : 87.8492660522461\n",
      "[0286 / 1000] - train loss : 66.2336654663086, valid_loss : 87.34803771972656\n",
      "[0287 / 1000] - train loss : 65.65773010253906, valid_loss : 86.85261535644531\n",
      "[0288 / 1000] - train loss : 65.08891296386719, valid_loss : 86.36396789550781\n",
      "[0289 / 1000] - train loss : 64.52684783935547, valid_loss : 85.88164520263672\n",
      "[0290 / 1000] - train loss : 63.97146224975586, valid_loss : 85.40571594238281\n",
      "[0291 / 1000] - train loss : 63.4227180480957, valid_loss : 84.9361572265625\n",
      "[0292 / 1000] - train loss : 62.88052749633789, valid_loss : 84.4698715209961\n",
      "[0293 / 1000] - train loss : 62.34453201293945, valid_loss : 84.0085220336914\n",
      "[0294 / 1000] - train loss : 61.815086364746094, valid_loss : 83.55306243896484\n",
      "[0295 / 1000] - train loss : 61.29201126098633, valid_loss : 83.1034927368164\n",
      "[0296 / 1000] - train loss : 60.775211334228516, valid_loss : 82.6598129272461\n",
      "[0297 / 1000] - train loss : 60.26457214355469, valid_loss : 82.22168731689453\n",
      "[0298 / 1000] - train loss : 59.759674072265625, valid_loss : 81.78935241699219\n",
      "[0299 / 1000] - train loss : 59.26063919067383, valid_loss : 81.36290740966797\n",
      "[0300 / 1000] - train loss : 58.767372131347656, valid_loss : 80.94376373291016\n",
      "[0301 / 1000] - train loss : 58.27909851074219, valid_loss : 80.53013610839844\n",
      "[0302 / 1000] - train loss : 57.7959098815918, valid_loss : 80.12136840820312\n",
      "[0303 / 1000] - train loss : 57.31834411621094, valid_loss : 79.71388244628906\n",
      "[0304 / 1000] - train loss : 56.84541320800781, valid_loss : 79.31141662597656\n",
      "[0305 / 1000] - train loss : 56.37773513793945, valid_loss : 78.91414642333984\n",
      "[0306 / 1000] - train loss : 55.91498565673828, valid_loss : 78.52227020263672\n",
      "[0307 / 1000] - train loss : 55.45735168457031, valid_loss : 78.1354751586914\n",
      "[0308 / 1000] - train loss : 55.004940032958984, valid_loss : 77.75370788574219\n",
      "[0309 / 1000] - train loss : 54.5578727722168, valid_loss : 77.3770980834961\n",
      "[0310 / 1000] - train loss : 54.11592102050781, valid_loss : 77.00536346435547\n",
      "[0311 / 1000] - train loss : 53.67887496948242, valid_loss : 76.63848876953125\n",
      "[0312 / 1000] - train loss : 53.24672317504883, valid_loss : 76.2762222290039\n",
      "[0313 / 1000] - train loss : 52.819698333740234, valid_loss : 75.91688537597656\n",
      "[0314 / 1000] - train loss : 52.39793395996094, valid_loss : 75.55865478515625\n",
      "[0315 / 1000] - train loss : 51.980953216552734, valid_loss : 75.2042007446289\n",
      "[0316 / 1000] - train loss : 51.56900405883789, valid_loss : 74.8543930053711\n",
      "[0317 / 1000] - train loss : 51.161800384521484, valid_loss : 74.50914001464844\n",
      "[0318 / 1000] - train loss : 50.75920486450195, valid_loss : 74.16838836669922\n",
      "[0319 / 1000] - train loss : 50.361595153808594, valid_loss : 73.8302001953125\n",
      "[0320 / 1000] - train loss : 49.96853256225586, valid_loss : 73.4961929321289\n",
      "[0321 / 1000] - train loss : 49.57998275756836, valid_loss : 73.1665267944336\n",
      "[0322 / 1000] - train loss : 49.1960334777832, valid_loss : 72.84117126464844\n",
      "[0323 / 1000] - train loss : 48.816932678222656, valid_loss : 72.52008056640625\n",
      "[0324 / 1000] - train loss : 48.44162368774414, valid_loss : 72.20294189453125\n",
      "[0325 / 1000] - train loss : 48.0703010559082, valid_loss : 71.88977813720703\n",
      "[0326 / 1000] - train loss : 47.70346450805664, valid_loss : 71.57806396484375\n",
      "[0327 / 1000] - train loss : 47.341346740722656, valid_loss : 71.26943969726562\n",
      "[0328 / 1000] - train loss : 46.98405456542969, valid_loss : 70.9644546508789\n",
      "[0329 / 1000] - train loss : 46.63150405883789, valid_loss : 70.66352844238281\n",
      "[0330 / 1000] - train loss : 46.283634185791016, valid_loss : 70.36666870117188\n",
      "[0331 / 1000] - train loss : 45.940433502197266, valid_loss : 70.07378387451172\n",
      "[0332 / 1000] - train loss : 45.60190963745117, valid_loss : 69.78488159179688\n",
      "[0333 / 1000] - train loss : 45.267906188964844, valid_loss : 69.49958801269531\n",
      "[0334 / 1000] - train loss : 44.93836212158203, valid_loss : 69.21765899658203\n",
      "[0335 / 1000] - train loss : 44.6132926940918, valid_loss : 68.939453125\n",
      "[0336 / 1000] - train loss : 44.292762756347656, valid_loss : 68.66539001464844\n",
      "[0337 / 1000] - train loss : 43.976436614990234, valid_loss : 68.39564514160156\n",
      "[0338 / 1000] - train loss : 43.66432189941406, valid_loss : 68.12970733642578\n",
      "[0339 / 1000] - train loss : 43.35646057128906, valid_loss : 67.86762237548828\n",
      "[0340 / 1000] - train loss : 43.05266189575195, valid_loss : 67.60882568359375\n",
      "[0341 / 1000] - train loss : 42.753116607666016, valid_loss : 67.3521728515625\n",
      "[0342 / 1000] - train loss : 42.4576530456543, valid_loss : 67.09892272949219\n",
      "[0343 / 1000] - train loss : 42.1658935546875, valid_loss : 66.84660339355469\n",
      "[0344 / 1000] - train loss : 41.876766204833984, valid_loss : 66.59577941894531\n",
      "[0345 / 1000] - train loss : 41.588809967041016, valid_loss : 66.34744262695312\n",
      "[0346 / 1000] - train loss : 41.301448822021484, valid_loss : 66.11310577392578\n",
      "[0347 / 1000] - train loss : 41.021026611328125, valid_loss : 65.88652038574219\n",
      "[0348 / 1000] - train loss : 40.74641799926758, valid_loss : 65.66516876220703\n",
      "[0349 / 1000] - train loss : 40.47719955444336, valid_loss : 65.44910430908203\n",
      "[0350 / 1000] - train loss : 40.213863372802734, valid_loss : 65.23611450195312\n",
      "[0351 / 1000] - train loss : 39.95465850830078, valid_loss : 65.02470397949219\n",
      "[0352 / 1000] - train loss : 39.69965744018555, valid_loss : 64.81431579589844\n",
      "[0353 / 1000] - train loss : 39.448219299316406, valid_loss : 64.6053466796875\n",
      "[0354 / 1000] - train loss : 39.20050048828125, valid_loss : 64.39834594726562\n",
      "[0355 / 1000] - train loss : 38.95618438720703, valid_loss : 64.19335174560547\n",
      "[0356 / 1000] - train loss : 38.715457916259766, valid_loss : 63.99098205566406\n",
      "[0357 / 1000] - train loss : 38.47832107543945, valid_loss : 63.79140853881836\n",
      "[0358 / 1000] - train loss : 38.24468994140625, valid_loss : 63.594268798828125\n",
      "[0359 / 1000] - train loss : 38.01462173461914, valid_loss : 63.399444580078125\n",
      "[0360 / 1000] - train loss : 37.78815841674805, valid_loss : 63.207252502441406\n",
      "[0361 / 1000] - train loss : 37.565181732177734, valid_loss : 63.017459869384766\n",
      "[0362 / 1000] - train loss : 37.345401763916016, valid_loss : 62.829803466796875\n",
      "[0363 / 1000] - train loss : 37.12843322753906, valid_loss : 62.644649505615234\n",
      "[0364 / 1000] - train loss : 36.91477966308594, valid_loss : 62.4619255065918\n",
      "[0365 / 1000] - train loss : 36.70446014404297, valid_loss : 62.28152847290039\n",
      "[0366 / 1000] - train loss : 36.497406005859375, valid_loss : 62.10342788696289\n",
      "[0367 / 1000] - train loss : 36.29357147216797, valid_loss : 61.92747497558594\n",
      "[0368 / 1000] - train loss : 36.09312438964844, valid_loss : 61.75373077392578\n",
      "[0369 / 1000] - train loss : 35.89595031738281, valid_loss : 61.582149505615234\n",
      "[0370 / 1000] - train loss : 35.701904296875, valid_loss : 61.41250991821289\n",
      "[0371 / 1000] - train loss : 35.51064682006836, valid_loss : 61.2449836730957\n",
      "[0372 / 1000] - train loss : 35.3224983215332, valid_loss : 61.07954025268555\n",
      "[0373 / 1000] - train loss : 35.13751983642578, valid_loss : 60.91585922241211\n",
      "[0374 / 1000] - train loss : 34.95566940307617, valid_loss : 60.75346755981445\n",
      "[0375 / 1000] - train loss : 34.77692413330078, valid_loss : 60.59292221069336\n",
      "[0376 / 1000] - train loss : 34.60124206542969, valid_loss : 60.434303283691406\n",
      "[0377 / 1000] - train loss : 34.42835998535156, valid_loss : 60.27756881713867\n",
      "[0378 / 1000] - train loss : 34.25824737548828, valid_loss : 60.12269592285156\n",
      "[0379 / 1000] - train loss : 34.09105682373047, valid_loss : 59.969669342041016\n",
      "[0380 / 1000] - train loss : 33.926780700683594, valid_loss : 59.817996978759766\n",
      "[0381 / 1000] - train loss : 33.76557540893555, valid_loss : 59.668006896972656\n",
      "[0382 / 1000] - train loss : 33.607112884521484, valid_loss : 59.51955032348633\n",
      "[0383 / 1000] - train loss : 33.45142364501953, valid_loss : 59.37255859375\n",
      "[0384 / 1000] - train loss : 33.29838180541992, valid_loss : 59.227169036865234\n",
      "[0385 / 1000] - train loss : 33.14802169799805, valid_loss : 59.083370208740234\n",
      "[0386 / 1000] - train loss : 33.000301361083984, valid_loss : 58.94114685058594\n",
      "[0387 / 1000] - train loss : 32.85517883300781, valid_loss : 58.80048751831055\n",
      "[0388 / 1000] - train loss : 32.71247100830078, valid_loss : 58.66126251220703\n",
      "[0389 / 1000] - train loss : 32.572261810302734, valid_loss : 58.52363967895508\n",
      "[0390 / 1000] - train loss : 32.43435287475586, valid_loss : 58.387603759765625\n",
      "[0391 / 1000] - train loss : 32.298946380615234, valid_loss : 58.25271987915039\n",
      "[0392 / 1000] - train loss : 32.16596221923828, valid_loss : 58.1191520690918\n",
      "[0393 / 1000] - train loss : 32.035343170166016, valid_loss : 57.98717498779297\n",
      "[0394 / 1000] - train loss : 31.9070987701416, valid_loss : 57.85680389404297\n",
      "[0395 / 1000] - train loss : 31.78116798400879, valid_loss : 57.72794723510742\n",
      "[0396 / 1000] - train loss : 31.657440185546875, valid_loss : 57.60071563720703\n",
      "[0397 / 1000] - train loss : 31.53603172302246, valid_loss : 57.47532272338867\n",
      "[0398 / 1000] - train loss : 31.416767120361328, valid_loss : 57.35097885131836\n",
      "[0399 / 1000] - train loss : 31.29964828491211, valid_loss : 57.22795867919922\n",
      "[0400 / 1000] - train loss : 31.184612274169922, valid_loss : 57.106666564941406\n",
      "[0401 / 1000] - train loss : 31.071680068969727, valid_loss : 56.98699188232422\n",
      "[0402 / 1000] - train loss : 30.960826873779297, valid_loss : 56.86881637573242\n",
      "[0403 / 1000] - train loss : 30.851930618286133, valid_loss : 56.75212478637695\n",
      "[0404 / 1000] - train loss : 30.744953155517578, valid_loss : 56.63715744018555\n",
      "[0405 / 1000] - train loss : 30.639801025390625, valid_loss : 56.523902893066406\n",
      "[0406 / 1000] - train loss : 30.536413192749023, valid_loss : 56.41221618652344\n",
      "[0407 / 1000] - train loss : 30.434768676757812, valid_loss : 56.30210876464844\n",
      "[0408 / 1000] - train loss : 30.33490753173828, valid_loss : 56.19302749633789\n",
      "[0409 / 1000] - train loss : 30.23667335510254, valid_loss : 56.08457946777344\n",
      "[0410 / 1000] - train loss : 30.1401309967041, valid_loss : 55.977294921875\n",
      "[0411 / 1000] - train loss : 30.045259475708008, valid_loss : 55.8713264465332\n",
      "[0412 / 1000] - train loss : 29.95196533203125, valid_loss : 55.76645278930664\n",
      "[0413 / 1000] - train loss : 29.860177993774414, valid_loss : 55.66244888305664\n",
      "[0414 / 1000] - train loss : 29.76988983154297, valid_loss : 55.559043884277344\n",
      "[0415 / 1000] - train loss : 29.681055068969727, valid_loss : 55.45628356933594\n",
      "[0416 / 1000] - train loss : 29.594566345214844, valid_loss : 55.35633087158203\n",
      "[0417 / 1000] - train loss : 29.509449005126953, valid_loss : 55.258811950683594\n",
      "[0418 / 1000] - train loss : 29.42570686340332, valid_loss : 55.1637077331543\n",
      "[0419 / 1000] - train loss : 29.343305587768555, valid_loss : 55.07086181640625\n",
      "[0420 / 1000] - train loss : 29.262237548828125, valid_loss : 54.980133056640625\n",
      "[0421 / 1000] - train loss : 29.182411193847656, valid_loss : 54.891353607177734\n",
      "[0422 / 1000] - train loss : 29.103883743286133, valid_loss : 54.80440902709961\n",
      "[0423 / 1000] - train loss : 29.02661895751953, valid_loss : 54.71915054321289\n",
      "[0424 / 1000] - train loss : 28.95063018798828, valid_loss : 54.63544464111328\n",
      "[0425 / 1000] - train loss : 28.87586212158203, valid_loss : 54.553192138671875\n",
      "[0426 / 1000] - train loss : 28.802236557006836, valid_loss : 54.47226333618164\n",
      "[0427 / 1000] - train loss : 28.72970199584961, valid_loss : 54.39265441894531\n",
      "[0428 / 1000] - train loss : 28.658306121826172, valid_loss : 54.31427764892578\n",
      "[0429 / 1000] - train loss : 28.588014602661133, valid_loss : 54.23698806762695\n",
      "[0430 / 1000] - train loss : 28.51883316040039, valid_loss : 54.16068649291992\n",
      "[0431 / 1000] - train loss : 28.45076560974121, valid_loss : 54.08526611328125\n",
      "[0432 / 1000] - train loss : 28.38378143310547, valid_loss : 54.01062774658203\n",
      "[0433 / 1000] - train loss : 28.317834854125977, valid_loss : 53.93646240234375\n",
      "[0434 / 1000] - train loss : 28.252904891967773, valid_loss : 53.8623161315918\n",
      "[0435 / 1000] - train loss : 28.188995361328125, valid_loss : 53.7859001159668\n",
      "[0436 / 1000] - train loss : 28.12614631652832, valid_loss : 53.70829391479492\n",
      "[0437 / 1000] - train loss : 28.064027786254883, valid_loss : 53.629638671875\n",
      "[0438 / 1000] - train loss : 28.00287628173828, valid_loss : 53.552677154541016\n",
      "[0439 / 1000] - train loss : 27.942598342895508, valid_loss : 53.477088928222656\n",
      "[0440 / 1000] - train loss : 27.883071899414062, valid_loss : 53.4015007019043\n",
      "[0441 / 1000] - train loss : 27.82427978515625, valid_loss : 53.32359313964844\n",
      "[0442 / 1000] - train loss : 27.76636505126953, valid_loss : 53.24460983276367\n",
      "[0443 / 1000] - train loss : 27.70923614501953, valid_loss : 53.16725158691406\n",
      "[0444 / 1000] - train loss : 27.65283203125, valid_loss : 53.091453552246094\n",
      "[0445 / 1000] - train loss : 27.59712028503418, valid_loss : 53.0144157409668\n",
      "[0446 / 1000] - train loss : 27.542192459106445, valid_loss : 52.938961029052734\n",
      "[0447 / 1000] - train loss : 27.487953186035156, valid_loss : 52.862266540527344\n",
      "[0448 / 1000] - train loss : 27.434316635131836, valid_loss : 52.78719711303711\n",
      "[0449 / 1000] - train loss : 27.381301879882812, valid_loss : 52.713623046875\n",
      "[0450 / 1000] - train loss : 27.328975677490234, valid_loss : 52.63861846923828\n",
      "[0451 / 1000] - train loss : 27.277137756347656, valid_loss : 52.56223678588867\n",
      "[0452 / 1000] - train loss : 27.225976943969727, valid_loss : 52.48763656616211\n",
      "[0453 / 1000] - train loss : 27.175403594970703, valid_loss : 52.41465759277344\n",
      "[0454 / 1000] - train loss : 27.125362396240234, valid_loss : 52.3431510925293\n",
      "[0455 / 1000] - train loss : 27.075672149658203, valid_loss : 52.273006439208984\n",
      "[0456 / 1000] - train loss : 27.026702880859375, valid_loss : 52.20119094848633\n",
      "[0457 / 1000] - train loss : 26.97815704345703, valid_loss : 52.12761688232422\n",
      "[0458 / 1000] - train loss : 26.929956436157227, valid_loss : 52.05235290527344\n",
      "[0459 / 1000] - train loss : 26.882339477539062, valid_loss : 51.978458404541016\n",
      "[0460 / 1000] - train loss : 26.835519790649414, valid_loss : 51.90591812133789\n",
      "[0461 / 1000] - train loss : 26.789207458496094, valid_loss : 51.834659576416016\n",
      "[0462 / 1000] - train loss : 26.743335723876953, valid_loss : 51.764549255371094\n",
      "[0463 / 1000] - train loss : 26.697805404663086, valid_loss : 51.695438385009766\n",
      "[0464 / 1000] - train loss : 26.652549743652344, valid_loss : 51.62728500366211\n",
      "[0465 / 1000] - train loss : 26.607702255249023, valid_loss : 51.56002426147461\n",
      "[0466 / 1000] - train loss : 26.56328010559082, valid_loss : 51.490509033203125\n",
      "[0467 / 1000] - train loss : 26.519216537475586, valid_loss : 51.4189567565918\n",
      "[0468 / 1000] - train loss : 26.47544288635254, valid_loss : 51.34870910644531\n",
      "[0469 / 1000] - train loss : 26.432086944580078, valid_loss : 51.27937316894531\n",
      "[0470 / 1000] - train loss : 26.38904571533203, valid_loss : 51.21086883544922\n",
      "[0471 / 1000] - train loss : 26.34633445739746, valid_loss : 51.14316177368164\n",
      "[0472 / 1000] - train loss : 26.303911209106445, valid_loss : 51.076210021972656\n",
      "[0473 / 1000] - train loss : 26.261920928955078, valid_loss : 51.0067024230957\n",
      "[0474 / 1000] - train loss : 26.22011375427246, valid_loss : 50.93809509277344\n",
      "[0475 / 1000] - train loss : 26.1787052154541, valid_loss : 50.8703498840332\n",
      "[0476 / 1000] - train loss : 26.137588500976562, valid_loss : 50.80342483520508\n",
      "[0477 / 1000] - train loss : 26.096744537353516, valid_loss : 50.73720932006836\n",
      "[0478 / 1000] - train loss : 26.056255340576172, valid_loss : 50.66834259033203\n",
      "[0479 / 1000] - train loss : 26.01593017578125, valid_loss : 50.60044860839844\n",
      "[0480 / 1000] - train loss : 25.97597312927246, valid_loss : 50.53327941894531\n",
      "[0481 / 1000] - train loss : 25.936281204223633, valid_loss : 50.46696090698242\n",
      "[0482 / 1000] - train loss : 25.896820068359375, valid_loss : 50.40143585205078\n",
      "[0483 / 1000] - train loss : 25.857650756835938, valid_loss : 50.33661651611328\n",
      "[0484 / 1000] - train loss : 25.818706512451172, valid_loss : 50.27242660522461\n",
      "[0485 / 1000] - train loss : 25.780031204223633, valid_loss : 50.20880889892578\n",
      "[0486 / 1000] - train loss : 25.741619110107422, valid_loss : 50.145530700683594\n",
      "[0487 / 1000] - train loss : 25.703569412231445, valid_loss : 50.0789909362793\n",
      "[0488 / 1000] - train loss : 25.665592193603516, valid_loss : 50.0129280090332\n",
      "[0489 / 1000] - train loss : 25.627885818481445, valid_loss : 49.94715881347656\n",
      "[0490 / 1000] - train loss : 25.590402603149414, valid_loss : 49.88170623779297\n",
      "[0491 / 1000] - train loss : 25.553115844726562, valid_loss : 49.8168830871582\n",
      "[0492 / 1000] - train loss : 25.51604652404785, valid_loss : 49.752685546875\n",
      "[0493 / 1000] - train loss : 25.47919273376465, valid_loss : 49.68904113769531\n",
      "[0494 / 1000] - train loss : 25.442564010620117, valid_loss : 49.625938415527344\n",
      "[0495 / 1000] - train loss : 25.40614128112793, valid_loss : 49.56329345703125\n",
      "[0496 / 1000] - train loss : 25.369924545288086, valid_loss : 49.501338958740234\n",
      "[0497 / 1000] - train loss : 25.333938598632812, valid_loss : 49.44041061401367\n",
      "[0498 / 1000] - train loss : 25.298166275024414, valid_loss : 49.37959671020508\n",
      "[0499 / 1000] - train loss : 25.262588500976562, valid_loss : 49.318607330322266\n",
      "[0500 / 1000] - train loss : 25.227174758911133, valid_loss : 49.25748825073242\n",
      "[0501 / 1000] - train loss : 25.19194221496582, valid_loss : 49.19624710083008\n",
      "[0502 / 1000] - train loss : 25.156879425048828, valid_loss : 49.134788513183594\n",
      "[0503 / 1000] - train loss : 25.121959686279297, valid_loss : 49.07325744628906\n",
      "[0504 / 1000] - train loss : 25.08719253540039, valid_loss : 49.01168441772461\n",
      "[0505 / 1000] - train loss : 25.052589416503906, valid_loss : 48.950096130371094\n",
      "[0506 / 1000] - train loss : 25.01814842224121, valid_loss : 48.88852310180664\n",
      "[0507 / 1000] - train loss : 24.983840942382812, valid_loss : 48.82705307006836\n",
      "[0508 / 1000] - train loss : 24.949687957763672, valid_loss : 48.76567840576172\n",
      "[0509 / 1000] - train loss : 24.915687561035156, valid_loss : 48.704402923583984\n",
      "[0510 / 1000] - train loss : 24.8818359375, valid_loss : 48.643253326416016\n",
      "[0511 / 1000] - train loss : 24.848119735717773, valid_loss : 48.58226013183594\n",
      "[0512 / 1000] - train loss : 24.814546585083008, valid_loss : 48.52144241333008\n",
      "[0513 / 1000] - train loss : 24.78112030029297, valid_loss : 48.460819244384766\n",
      "[0514 / 1000] - train loss : 24.747838973999023, valid_loss : 48.400691986083984\n",
      "[0515 / 1000] - train loss : 24.714691162109375, valid_loss : 48.3410530090332\n",
      "[0516 / 1000] - train loss : 24.681652069091797, valid_loss : 48.28188705444336\n",
      "[0517 / 1000] - train loss : 24.648754119873047, valid_loss : 48.22316360473633\n",
      "[0518 / 1000] - train loss : 24.61598777770996, valid_loss : 48.16481399536133\n",
      "[0519 / 1000] - train loss : 24.583354949951172, valid_loss : 48.10678482055664\n",
      "[0520 / 1000] - train loss : 24.550844192504883, valid_loss : 48.04915237426758\n",
      "[0521 / 1000] - train loss : 24.51846694946289, valid_loss : 47.991973876953125\n",
      "[0522 / 1000] - train loss : 24.486202239990234, valid_loss : 47.935203552246094\n",
      "[0523 / 1000] - train loss : 24.454143524169922, valid_loss : 47.87941360473633\n",
      "[0524 / 1000] - train loss : 24.422218322753906, valid_loss : 47.824440002441406\n",
      "[0525 / 1000] - train loss : 24.390405654907227, valid_loss : 47.77021026611328\n",
      "[0526 / 1000] - train loss : 24.358715057373047, valid_loss : 47.71637725830078\n",
      "[0527 / 1000] - train loss : 24.32715606689453, valid_loss : 47.66278839111328\n",
      "[0528 / 1000] - train loss : 24.295734405517578, valid_loss : 47.609336853027344\n",
      "[0529 / 1000] - train loss : 24.264440536499023, valid_loss : 47.5560417175293\n",
      "[0530 / 1000] - train loss : 24.23326301574707, valid_loss : 47.502967834472656\n",
      "[0531 / 1000] - train loss : 24.202177047729492, valid_loss : 47.449981689453125\n",
      "[0532 / 1000] - train loss : 24.171180725097656, valid_loss : 47.39706039428711\n",
      "[0533 / 1000] - train loss : 24.140308380126953, valid_loss : 47.344261169433594\n",
      "[0534 / 1000] - train loss : 24.109628677368164, valid_loss : 47.291954040527344\n",
      "[0535 / 1000] - train loss : 24.079059600830078, valid_loss : 47.240081787109375\n",
      "[0536 / 1000] - train loss : 24.04859161376953, valid_loss : 47.18858337402344\n",
      "[0537 / 1000] - train loss : 24.01823616027832, valid_loss : 47.13740921020508\n",
      "[0538 / 1000] - train loss : 23.987991333007812, valid_loss : 47.08648681640625\n",
      "[0539 / 1000] - train loss : 23.95783805847168, valid_loss : 47.03565979003906\n",
      "[0540 / 1000] - train loss : 23.927762985229492, valid_loss : 46.984901428222656\n",
      "[0541 / 1000] - train loss : 23.897789001464844, valid_loss : 46.93416213989258\n",
      "[0542 / 1000] - train loss : 23.867904663085938, valid_loss : 46.88350296020508\n",
      "[0543 / 1000] - train loss : 23.83810806274414, valid_loss : 46.83290100097656\n",
      "[0544 / 1000] - train loss : 23.808401107788086, valid_loss : 46.782325744628906\n",
      "[0545 / 1000] - train loss : 23.778770446777344, valid_loss : 46.73170471191406\n",
      "[0546 / 1000] - train loss : 23.74913787841797, valid_loss : 46.68098449707031\n",
      "[0547 / 1000] - train loss : 23.719388961791992, valid_loss : 46.630149841308594\n",
      "[0548 / 1000] - train loss : 23.689672470092773, valid_loss : 46.57917785644531\n",
      "[0549 / 1000] - train loss : 23.660003662109375, valid_loss : 46.52805709838867\n",
      "[0550 / 1000] - train loss : 23.630380630493164, valid_loss : 46.47679138183594\n",
      "[0551 / 1000] - train loss : 23.60080909729004, valid_loss : 46.425331115722656\n",
      "[0552 / 1000] - train loss : 23.571287155151367, valid_loss : 46.37364959716797\n",
      "[0553 / 1000] - train loss : 23.54180145263672, valid_loss : 46.32185745239258\n",
      "[0554 / 1000] - train loss : 23.512325286865234, valid_loss : 46.26996994018555\n",
      "[0555 / 1000] - train loss : 23.48284339904785, valid_loss : 46.21803665161133\n",
      "[0556 / 1000] - train loss : 23.453393936157227, valid_loss : 46.16606521606445\n",
      "[0557 / 1000] - train loss : 23.423980712890625, valid_loss : 46.114070892333984\n",
      "[0558 / 1000] - train loss : 23.394609451293945, valid_loss : 46.06187438964844\n",
      "[0559 / 1000] - train loss : 23.36515235900879, valid_loss : 46.009620666503906\n",
      "[0560 / 1000] - train loss : 23.33566665649414, valid_loss : 45.95740509033203\n",
      "[0561 / 1000] - train loss : 23.30621337890625, valid_loss : 45.90523910522461\n",
      "[0562 / 1000] - train loss : 23.276782989501953, valid_loss : 45.85315704345703\n",
      "[0563 / 1000] - train loss : 23.247386932373047, valid_loss : 45.80149841308594\n",
      "[0564 / 1000] - train loss : 23.218017578125, valid_loss : 45.75026321411133\n",
      "[0565 / 1000] - train loss : 23.188617706298828, valid_loss : 45.69940948486328\n",
      "[0566 / 1000] - train loss : 23.15924644470215, valid_loss : 45.648868560791016\n",
      "[0567 / 1000] - train loss : 23.129907608032227, valid_loss : 45.598201751708984\n",
      "[0568 / 1000] - train loss : 23.100276947021484, valid_loss : 45.54774475097656\n",
      "[0569 / 1000] - train loss : 23.070493698120117, valid_loss : 45.49748611450195\n",
      "[0570 / 1000] - train loss : 23.040672302246094, valid_loss : 45.447364807128906\n",
      "[0571 / 1000] - train loss : 23.010799407958984, valid_loss : 45.39739227294922\n",
      "[0572 / 1000] - train loss : 22.980920791625977, valid_loss : 45.34910583496094\n",
      "[0573 / 1000] - train loss : 22.951242446899414, valid_loss : 45.30229568481445\n",
      "[0574 / 1000] - train loss : 22.921550750732422, valid_loss : 45.256736755371094\n",
      "[0575 / 1000] - train loss : 22.89185333251953, valid_loss : 45.212188720703125\n",
      "[0576 / 1000] - train loss : 22.862150192260742, valid_loss : 45.168190002441406\n",
      "[0577 / 1000] - train loss : 22.83245849609375, valid_loss : 45.12458419799805\n",
      "[0578 / 1000] - train loss : 22.802818298339844, valid_loss : 45.081207275390625\n",
      "[0579 / 1000] - train loss : 22.773208618164062, valid_loss : 45.0379524230957\n",
      "[0580 / 1000] - train loss : 22.743627548217773, valid_loss : 44.99474334716797\n",
      "[0581 / 1000] - train loss : 22.714080810546875, valid_loss : 44.95151138305664\n",
      "[0582 / 1000] - train loss : 22.684572219848633, valid_loss : 44.90812683105469\n",
      "[0583 / 1000] - train loss : 22.655099868774414, valid_loss : 44.86452102661133\n",
      "[0584 / 1000] - train loss : 22.625675201416016, valid_loss : 44.82068634033203\n",
      "[0585 / 1000] - train loss : 22.596290588378906, valid_loss : 44.776588439941406\n",
      "[0586 / 1000] - train loss : 22.56694984436035, valid_loss : 44.73216247558594\n",
      "[0587 / 1000] - train loss : 22.5377254486084, valid_loss : 44.68741989135742\n",
      "[0588 / 1000] - train loss : 22.50856590270996, valid_loss : 44.64253616333008\n",
      "[0589 / 1000] - train loss : 22.479421615600586, valid_loss : 44.597434997558594\n",
      "[0590 / 1000] - train loss : 22.450313568115234, valid_loss : 44.55198669433594\n",
      "[0591 / 1000] - train loss : 22.421266555786133, valid_loss : 44.506309509277344\n",
      "[0592 / 1000] - train loss : 22.39223289489746, valid_loss : 44.46036911010742\n",
      "[0593 / 1000] - train loss : 22.363201141357422, valid_loss : 44.414119720458984\n",
      "[0594 / 1000] - train loss : 22.334245681762695, valid_loss : 44.36756134033203\n",
      "[0595 / 1000] - train loss : 22.305675506591797, valid_loss : 44.32072830200195\n",
      "[0596 / 1000] - train loss : 22.277118682861328, valid_loss : 44.2739372253418\n",
      "[0597 / 1000] - train loss : 22.24859046936035, valid_loss : 44.22727966308594\n",
      "[0598 / 1000] - train loss : 22.220151901245117, valid_loss : 44.18043899536133\n",
      "[0599 / 1000] - train loss : 22.191768646240234, valid_loss : 44.13356018066406\n",
      "[0600 / 1000] - train loss : 22.163482666015625, valid_loss : 44.08708190917969\n",
      "[0601 / 1000] - train loss : 22.13525390625, valid_loss : 44.04096603393555\n",
      "[0602 / 1000] - train loss : 22.107074737548828, valid_loss : 43.99518966674805\n",
      "[0603 / 1000] - train loss : 22.078941345214844, valid_loss : 43.9497184753418\n",
      "[0604 / 1000] - train loss : 22.050853729248047, valid_loss : 43.90449905395508\n",
      "[0605 / 1000] - train loss : 22.022815704345703, valid_loss : 43.859500885009766\n",
      "[0606 / 1000] - train loss : 21.9948787689209, valid_loss : 43.814720153808594\n",
      "[0607 / 1000] - train loss : 21.967023849487305, valid_loss : 43.7701301574707\n",
      "[0608 / 1000] - train loss : 21.93915367126465, valid_loss : 43.72567367553711\n",
      "[0609 / 1000] - train loss : 21.91132354736328, valid_loss : 43.68132400512695\n",
      "[0610 / 1000] - train loss : 21.88355255126953, valid_loss : 43.63705825805664\n",
      "[0611 / 1000] - train loss : 21.85582733154297, valid_loss : 43.59282684326172\n",
      "[0612 / 1000] - train loss : 21.828144073486328, valid_loss : 43.54862594604492\n",
      "[0613 / 1000] - train loss : 21.8005313873291, valid_loss : 43.50446701049805\n",
      "[0614 / 1000] - train loss : 21.77299690246582, valid_loss : 43.460323333740234\n",
      "[0615 / 1000] - train loss : 21.745508193969727, valid_loss : 43.41618347167969\n",
      "[0616 / 1000] - train loss : 21.718196868896484, valid_loss : 43.3720588684082\n",
      "[0617 / 1000] - train loss : 21.690975189208984, valid_loss : 43.32789993286133\n",
      "[0618 / 1000] - train loss : 21.66381072998047, valid_loss : 43.283729553222656\n",
      "[0619 / 1000] - train loss : 21.636699676513672, valid_loss : 43.2395133972168\n",
      "[0620 / 1000] - train loss : 21.609634399414062, valid_loss : 43.19524383544922\n",
      "[0621 / 1000] - train loss : 21.583065032958984, valid_loss : 43.157711029052734\n",
      "[0622 / 1000] - train loss : 21.55630874633789, valid_loss : 43.126007080078125\n",
      "[0623 / 1000] - train loss : 21.529251098632812, valid_loss : 43.092716217041016\n",
      "[0624 / 1000] - train loss : 21.5026798248291, valid_loss : 43.05787658691406\n",
      "[0625 / 1000] - train loss : 21.47618865966797, valid_loss : 43.02146530151367\n",
      "[0626 / 1000] - train loss : 21.449735641479492, valid_loss : 42.983524322509766\n",
      "[0627 / 1000] - train loss : 21.42331886291504, valid_loss : 42.944095611572266\n",
      "[0628 / 1000] - train loss : 21.39692497253418, valid_loss : 42.90325927734375\n",
      "[0629 / 1000] - train loss : 21.37055015563965, valid_loss : 42.8610954284668\n",
      "[0630 / 1000] - train loss : 21.344430923461914, valid_loss : 42.824764251708984\n",
      "[0631 / 1000] - train loss : 21.31822395324707, valid_loss : 42.79280090332031\n",
      "[0632 / 1000] - train loss : 21.29218292236328, valid_loss : 42.757911682128906\n",
      "[0633 / 1000] - train loss : 21.266246795654297, valid_loss : 42.721153259277344\n",
      "[0634 / 1000] - train loss : 21.240299224853516, valid_loss : 42.682342529296875\n",
      "[0635 / 1000] - train loss : 21.214353561401367, valid_loss : 42.6419792175293\n",
      "[0636 / 1000] - train loss : 21.188438415527344, valid_loss : 42.60015869140625\n",
      "[0637 / 1000] - train loss : 21.162532806396484, valid_loss : 42.55695724487305\n",
      "[0638 / 1000] - train loss : 21.136762619018555, valid_loss : 42.51862716674805\n",
      "[0639 / 1000] - train loss : 21.111019134521484, valid_loss : 42.4846305847168\n",
      "[0640 / 1000] - train loss : 21.085424423217773, valid_loss : 42.44832992553711\n",
      "[0641 / 1000] - train loss : 21.059921264648438, valid_loss : 42.409854888916016\n",
      "[0642 / 1000] - train loss : 21.03436851501465, valid_loss : 42.36940002441406\n",
      "[0643 / 1000] - train loss : 21.00876235961914, valid_loss : 42.32713317871094\n",
      "[0644 / 1000] - train loss : 20.98312759399414, valid_loss : 42.28326416015625\n",
      "[0645 / 1000] - train loss : 20.957866668701172, valid_loss : 42.24421691894531\n",
      "[0646 / 1000] - train loss : 20.932451248168945, valid_loss : 42.209468841552734\n",
      "[0647 / 1000] - train loss : 20.906768798828125, valid_loss : 42.17229461669922\n",
      "[0648 / 1000] - train loss : 20.881450653076172, valid_loss : 42.13290023803711\n",
      "[0649 / 1000] - train loss : 20.85608673095703, valid_loss : 42.09145736694336\n",
      "[0650 / 1000] - train loss : 20.830902099609375, valid_loss : 42.054447174072266\n",
      "[0651 / 1000] - train loss : 20.805513381958008, valid_loss : 42.02139663696289\n",
      "[0652 / 1000] - train loss : 20.780427932739258, valid_loss : 41.985572814941406\n",
      "[0653 / 1000] - train loss : 20.755292892456055, valid_loss : 41.9471321105957\n",
      "[0654 / 1000] - train loss : 20.730070114135742, valid_loss : 41.90631103515625\n",
      "[0655 / 1000] - train loss : 20.704730987548828, valid_loss : 41.8633918762207\n",
      "[0656 / 1000] - train loss : 20.67931365966797, valid_loss : 41.818626403808594\n",
      "[0657 / 1000] - train loss : 20.654497146606445, valid_loss : 41.778621673583984\n",
      "[0658 / 1000] - train loss : 20.629314422607422, valid_loss : 41.742919921875\n",
      "[0659 / 1000] - train loss : 20.603857040405273, valid_loss : 41.711063385009766\n",
      "[0660 / 1000] - train loss : 20.57815933227539, valid_loss : 41.68254470825195\n",
      "[0661 / 1000] - train loss : 20.553329467773438, valid_loss : 41.649566650390625\n",
      "[0662 / 1000] - train loss : 20.52838706970215, valid_loss : 41.61249542236328\n",
      "[0663 / 1000] - train loss : 20.50309944152832, valid_loss : 41.57170104980469\n",
      "[0664 / 1000] - train loss : 20.47759246826172, valid_loss : 41.52762222290039\n",
      "[0665 / 1000] - train loss : 20.451955795288086, valid_loss : 41.48067855834961\n",
      "[0666 / 1000] - train loss : 20.426586151123047, valid_loss : 41.43766403198242\n",
      "[0667 / 1000] - train loss : 20.401611328125, valid_loss : 41.39786148071289\n",
      "[0668 / 1000] - train loss : 20.376453399658203, valid_loss : 41.36119842529297\n",
      "[0669 / 1000] - train loss : 20.351131439208984, valid_loss : 41.3275260925293\n",
      "[0670 / 1000] - train loss : 20.325685501098633, valid_loss : 41.296443939208984\n",
      "[0671 / 1000] - train loss : 20.300548553466797, valid_loss : 41.2612190246582\n",
      "[0672 / 1000] - train loss : 20.275577545166016, valid_loss : 41.222225189208984\n",
      "[0673 / 1000] - train loss : 20.250377655029297, valid_loss : 41.179840087890625\n",
      "[0674 / 1000] - train loss : 20.224971771240234, valid_loss : 41.1345100402832\n",
      "[0675 / 1000] - train loss : 20.200075149536133, valid_loss : 41.09305953979492\n",
      "[0676 / 1000] - train loss : 20.17517852783203, valid_loss : 41.05512237548828\n",
      "[0677 / 1000] - train loss : 20.15003776550293, valid_loss : 41.020362854003906\n",
      "[0678 / 1000] - train loss : 20.12474822998047, valid_loss : 40.98845672607422\n",
      "[0679 / 1000] - train loss : 20.09933853149414, valid_loss : 40.95903396606445\n",
      "[0680 / 1000] - train loss : 20.074542999267578, valid_loss : 40.92527770996094\n",
      "[0681 / 1000] - train loss : 20.04966163635254, valid_loss : 40.88756561279297\n",
      "[0682 / 1000] - train loss : 20.024547576904297, valid_loss : 40.846309661865234\n",
      "[0683 / 1000] - train loss : 19.999174118041992, valid_loss : 40.801971435546875\n",
      "[0684 / 1000] - train loss : 19.973888397216797, valid_loss : 40.76142120361328\n",
      "[0685 / 1000] - train loss : 19.949007034301758, valid_loss : 40.724361419677734\n",
      "[0686 / 1000] - train loss : 19.923980712890625, valid_loss : 40.6904411315918\n",
      "[0687 / 1000] - train loss : 19.898757934570312, valid_loss : 40.65934753417969\n",
      "[0688 / 1000] - train loss : 19.87353515625, valid_loss : 40.624210357666016\n",
      "[0689 / 1000] - train loss : 19.84847068786621, valid_loss : 40.58494186401367\n",
      "[0690 / 1000] - train loss : 19.82324981689453, valid_loss : 40.54899597167969\n",
      "[0691 / 1000] - train loss : 19.798152923583984, valid_loss : 40.5161247253418\n",
      "[0692 / 1000] - train loss : 19.77328109741211, valid_loss : 40.47931671142578\n",
      "[0693 / 1000] - train loss : 19.74820327758789, valid_loss : 40.43898010253906\n",
      "[0694 / 1000] - train loss : 19.723297119140625, valid_loss : 40.403045654296875\n",
      "[0695 / 1000] - train loss : 19.698373794555664, valid_loss : 40.37107467651367\n",
      "[0696 / 1000] - train loss : 19.673242568969727, valid_loss : 40.34259796142578\n",
      "[0697 / 1000] - train loss : 19.648651123046875, valid_loss : 40.309600830078125\n",
      "[0698 / 1000] - train loss : 19.623870849609375, valid_loss : 40.272464752197266\n",
      "[0699 / 1000] - train loss : 19.59884262084961, valid_loss : 40.23163604736328\n",
      "[0700 / 1000] - train loss : 19.573772430419922, valid_loss : 40.19403076171875\n",
      "[0701 / 1000] - train loss : 19.549030303955078, valid_loss : 40.15935516357422\n",
      "[0702 / 1000] - train loss : 19.524181365966797, valid_loss : 40.1273078918457\n",
      "[0703 / 1000] - train loss : 19.49936294555664, valid_loss : 40.09794998168945\n",
      "[0704 / 1000] - train loss : 19.47481346130371, valid_loss : 40.064334869384766\n",
      "[0705 / 1000] - train loss : 19.45026206970215, valid_loss : 40.026668548583984\n",
      "[0706 / 1000] - train loss : 19.425491333007812, valid_loss : 39.98542404174805\n",
      "[0707 / 1000] - train loss : 19.401052474975586, valid_loss : 39.947715759277344\n",
      "[0708 / 1000] - train loss : 19.376625061035156, valid_loss : 39.913028717041016\n",
      "[0709 / 1000] - train loss : 19.352108001708984, valid_loss : 39.881080627441406\n",
      "[0710 / 1000] - train loss : 19.327577590942383, valid_loss : 39.85148239135742\n",
      "[0711 / 1000] - train loss : 19.303050994873047, valid_loss : 39.817474365234375\n",
      "[0712 / 1000] - train loss : 19.27874755859375, valid_loss : 39.77924728393555\n",
      "[0713 / 1000] - train loss : 19.254383087158203, valid_loss : 39.74385452270508\n",
      "[0714 / 1000] - train loss : 19.230100631713867, valid_loss : 39.71092224121094\n",
      "[0715 / 1000] - train loss : 19.205766677856445, valid_loss : 39.68014144897461\n",
      "[0716 / 1000] - train loss : 19.181777954101562, valid_loss : 39.6445426940918\n",
      "[0717 / 1000] - train loss : 19.157577514648438, valid_loss : 39.6046028137207\n",
      "[0718 / 1000] - train loss : 19.133316040039062, valid_loss : 39.56745529174805\n",
      "[0719 / 1000] - train loss : 19.10930824279785, valid_loss : 39.53282928466797\n",
      "[0720 / 1000] - train loss : 19.085203170776367, valid_loss : 39.50047302246094\n",
      "[0721 / 1000] - train loss : 19.06099510192871, valid_loss : 39.47001647949219\n",
      "[0722 / 1000] - train loss : 19.037092208862305, valid_loss : 39.434452056884766\n",
      "[0723 / 1000] - train loss : 19.01296043395996, valid_loss : 39.394283294677734\n",
      "[0724 / 1000] - train loss : 18.988752365112305, valid_loss : 39.35781478881836\n",
      "[0725 / 1000] - train loss : 18.96479606628418, valid_loss : 39.32468032836914\n",
      "[0726 / 1000] - train loss : 18.94070053100586, valid_loss : 39.293392181396484\n",
      "[0727 / 1000] - train loss : 18.91666030883789, valid_loss : 39.25697326660156\n",
      "[0728 / 1000] - train loss : 18.89266586303711, valid_loss : 39.222625732421875\n",
      "[0729 / 1000] - train loss : 18.8686466217041, valid_loss : 39.19013595581055\n",
      "[0730 / 1000] - train loss : 18.844724655151367, valid_loss : 39.15250778198242\n",
      "[0731 / 1000] - train loss : 18.82065773010254, valid_loss : 39.117034912109375\n",
      "[0732 / 1000] - train loss : 18.796695709228516, valid_loss : 39.08352279663086\n",
      "[0733 / 1000] - train loss : 18.772695541381836, valid_loss : 39.051727294921875\n",
      "[0734 / 1000] - train loss : 18.7489070892334, valid_loss : 39.01463317871094\n",
      "[0735 / 1000] - train loss : 18.72481918334961, valid_loss : 38.979591369628906\n",
      "[0736 / 1000] - train loss : 18.70092010498047, valid_loss : 38.946414947509766\n",
      "[0737 / 1000] - train loss : 18.6771297454834, valid_loss : 38.908226013183594\n",
      "[0738 / 1000] - train loss : 18.653276443481445, valid_loss : 38.87311553955078\n",
      "[0739 / 1000] - train loss : 18.629486083984375, valid_loss : 38.84099578857422\n",
      "[0740 / 1000] - train loss : 18.6055965423584, valid_loss : 38.81053161621094\n",
      "[0741 / 1000] - train loss : 18.581960678100586, valid_loss : 38.77460479736328\n",
      "[0742 / 1000] - train loss : 18.55801773071289, valid_loss : 38.73381423950195\n",
      "[0743 / 1000] - train loss : 18.534442901611328, valid_loss : 38.69559097290039\n",
      "[0744 / 1000] - train loss : 18.510892868041992, valid_loss : 38.66086196899414\n",
      "[0745 / 1000] - train loss : 18.48723793029785, valid_loss : 38.629512786865234\n",
      "[0746 / 1000] - train loss : 18.46352195739746, valid_loss : 38.59986877441406\n",
      "[0747 / 1000] - train loss : 18.439821243286133, valid_loss : 38.5716667175293\n",
      "[0748 / 1000] - train loss : 18.416105270385742, valid_loss : 38.54494857788086\n",
      "[0749 / 1000] - train loss : 18.39243507385254, valid_loss : 38.51251220703125\n",
      "[0750 / 1000] - train loss : 18.368871688842773, valid_loss : 38.48162841796875\n",
      "[0751 / 1000] - train loss : 18.345401763916016, valid_loss : 38.44514083862305\n",
      "[0752 / 1000] - train loss : 18.321945190429688, valid_loss : 38.41062545776367\n",
      "[0753 / 1000] - train loss : 18.298545837402344, valid_loss : 38.37792205810547\n",
      "[0754 / 1000] - train loss : 18.275135040283203, valid_loss : 38.34684753417969\n",
      "[0755 / 1000] - train loss : 18.251708984375, valid_loss : 38.31722640991211\n",
      "[0756 / 1000] - train loss : 18.228282928466797, valid_loss : 38.288856506347656\n",
      "[0757 / 1000] - train loss : 18.204862594604492, valid_loss : 38.261131286621094\n",
      "[0758 / 1000] - train loss : 18.181800842285156, valid_loss : 38.22684860229492\n",
      "[0759 / 1000] - train loss : 18.158367156982422, valid_loss : 38.187625885009766\n",
      "[0760 / 1000] - train loss : 18.13515281677246, valid_loss : 38.151039123535156\n",
      "[0761 / 1000] - train loss : 18.112140655517578, valid_loss : 38.11700439453125\n",
      "[0762 / 1000] - train loss : 18.088964462280273, valid_loss : 38.08529281616211\n",
      "[0763 / 1000] - train loss : 18.065677642822266, valid_loss : 38.05567169189453\n",
      "[0764 / 1000] - train loss : 18.04230308532715, valid_loss : 38.0278434753418\n",
      "[0765 / 1000] - train loss : 18.018842697143555, valid_loss : 38.00143814086914\n",
      "[0766 / 1000] - train loss : 17.99588966369629, valid_loss : 37.9696159362793\n",
      "[0767 / 1000] - train loss : 17.972843170166016, valid_loss : 37.932979583740234\n",
      "[0768 / 1000] - train loss : 17.949453353881836, valid_loss : 37.892127990722656\n",
      "[0769 / 1000] - train loss : 17.926013946533203, valid_loss : 37.85407638549805\n",
      "[0770 / 1000] - train loss : 17.90301513671875, valid_loss : 37.81865310668945\n",
      "[0771 / 1000] - train loss : 17.87991714477539, valid_loss : 37.7856559753418\n",
      "[0772 / 1000] - train loss : 17.856660842895508, valid_loss : 37.75603103637695\n",
      "[0773 / 1000] - train loss : 17.83331871032715, valid_loss : 37.7293586730957\n",
      "[0774 / 1000] - train loss : 17.810625076293945, valid_loss : 37.69859313964844\n",
      "[0775 / 1000] - train loss : 17.7875919342041, valid_loss : 37.664031982421875\n",
      "[0776 / 1000] - train loss : 17.764253616333008, valid_loss : 37.62606430053711\n",
      "[0777 / 1000] - train loss : 17.740711212158203, valid_loss : 37.586212158203125\n",
      "[0778 / 1000] - train loss : 17.717578887939453, valid_loss : 37.551395416259766\n",
      "[0779 / 1000] - train loss : 17.69452667236328, valid_loss : 37.521148681640625\n",
      "[0780 / 1000] - train loss : 17.671178817749023, valid_loss : 37.49502944946289\n",
      "[0781 / 1000] - train loss : 17.647584915161133, valid_loss : 37.47249984741211\n",
      "[0782 / 1000] - train loss : 17.624713897705078, valid_loss : 37.44627380371094\n",
      "[0783 / 1000] - train loss : 17.601661682128906, valid_loss : 37.41657257080078\n",
      "[0784 / 1000] - train loss : 17.578397750854492, valid_loss : 37.383670806884766\n",
      "[0785 / 1000] - train loss : 17.55493927001953, valid_loss : 37.347877502441406\n",
      "[0786 / 1000] - train loss : 17.53130531311035, valid_loss : 37.30955505371094\n",
      "[0787 / 1000] - train loss : 17.508197784423828, valid_loss : 37.2757682800293\n",
      "[0788 / 1000] - train loss : 17.485137939453125, valid_loss : 37.2461051940918\n",
      "[0789 / 1000] - train loss : 17.461942672729492, valid_loss : 37.220096588134766\n",
      "[0790 / 1000] - train loss : 17.438568115234375, valid_loss : 37.19727325439453\n",
      "[0791 / 1000] - train loss : 17.415054321289062, valid_loss : 37.17716979980469\n",
      "[0792 / 1000] - train loss : 17.392242431640625, valid_loss : 37.15254592895508\n",
      "[0793 / 1000] - train loss : 17.36956787109375, valid_loss : 37.123680114746094\n",
      "[0794 / 1000] - train loss : 17.346668243408203, valid_loss : 37.09092712402344\n",
      "[0795 / 1000] - train loss : 17.323610305786133, valid_loss : 37.05471420288086\n",
      "[0796 / 1000] - train loss : 17.300708770751953, valid_loss : 37.02225875854492\n",
      "[0797 / 1000] - train loss : 17.278209686279297, valid_loss : 36.99321365356445\n",
      "[0798 / 1000] - train loss : 17.25555992126465, valid_loss : 36.967193603515625\n",
      "[0799 / 1000] - train loss : 17.232789993286133, valid_loss : 36.94381332397461\n",
      "[0800 / 1000] - train loss : 17.209936141967773, valid_loss : 36.92266845703125\n",
      "[0801 / 1000] - train loss : 17.187849044799805, valid_loss : 36.89649963378906\n",
      "[0802 / 1000] - train loss : 17.165456771850586, valid_loss : 36.86570358276367\n",
      "[0803 / 1000] - train loss : 17.14276885986328, valid_loss : 36.83074188232422\n",
      "[0804 / 1000] - train loss : 17.11981201171875, valid_loss : 36.792118072509766\n",
      "[0805 / 1000] - train loss : 17.097726821899414, valid_loss : 36.757171630859375\n",
      "[0806 / 1000] - train loss : 17.07555389404297, valid_loss : 36.725582122802734\n",
      "[0807 / 1000] - train loss : 17.05316162109375, valid_loss : 36.696964263916016\n",
      "[0808 / 1000] - train loss : 17.030607223510742, valid_loss : 36.67107009887695\n",
      "[0809 / 1000] - train loss : 17.007888793945312, valid_loss : 36.64746856689453\n",
      "[0810 / 1000] - train loss : 16.985158920288086, valid_loss : 36.623756408691406\n",
      "[0811 / 1000] - train loss : 16.962404251098633, valid_loss : 36.60062026977539\n",
      "[0812 / 1000] - train loss : 16.93970489501953, valid_loss : 36.571075439453125\n",
      "[0813 / 1000] - train loss : 16.917152404785156, valid_loss : 36.535736083984375\n",
      "[0814 / 1000] - train loss : 16.894357681274414, valid_loss : 36.50193786621094\n",
      "[0815 / 1000] - train loss : 16.87177848815918, valid_loss : 36.469627380371094\n",
      "[0816 / 1000] - train loss : 16.84930992126465, valid_loss : 36.4387092590332\n",
      "[0817 / 1000] - train loss : 16.82681655883789, valid_loss : 36.40904235839844\n",
      "[0818 / 1000] - train loss : 16.804311752319336, valid_loss : 36.38007736206055\n",
      "[0819 / 1000] - train loss : 16.781890869140625, valid_loss : 36.34525680541992\n",
      "[0820 / 1000] - train loss : 16.75958824157715, valid_loss : 36.31327819824219\n",
      "[0821 / 1000] - train loss : 16.73736572265625, valid_loss : 36.283851623535156\n",
      "[0822 / 1000] - train loss : 16.715167999267578, valid_loss : 36.249847412109375\n",
      "[0823 / 1000] - train loss : 16.69293785095215, valid_loss : 36.21852493286133\n",
      "[0824 / 1000] - train loss : 16.6707820892334, valid_loss : 36.189605712890625\n",
      "[0825 / 1000] - train loss : 16.64858055114746, valid_loss : 36.16278839111328\n",
      "[0826 / 1000] - train loss : 16.62673568725586, valid_loss : 36.13086700439453\n",
      "[0827 / 1000] - train loss : 16.6044979095459, valid_loss : 36.093971252441406\n",
      "[0828 / 1000] - train loss : 16.58251953125, valid_loss : 36.060951232910156\n",
      "[0829 / 1000] - train loss : 16.560773849487305, valid_loss : 36.03153610229492\n",
      "[0830 / 1000] - train loss : 16.539011001586914, valid_loss : 36.00411605834961\n",
      "[0831 / 1000] - train loss : 16.51728630065918, valid_loss : 35.97833251953125\n",
      "[0832 / 1000] - train loss : 16.495508193969727, valid_loss : 35.95392990112305\n",
      "[0833 / 1000] - train loss : 16.474285125732422, valid_loss : 35.923580169677734\n",
      "[0834 / 1000] - train loss : 16.452638626098633, valid_loss : 35.88786697387695\n",
      "[0835 / 1000] - train loss : 16.430675506591797, valid_loss : 35.85430908203125\n",
      "[0836 / 1000] - train loss : 16.40922737121582, valid_loss : 35.822715759277344\n",
      "[0837 / 1000] - train loss : 16.387746810913086, valid_loss : 35.792903900146484\n",
      "[0838 / 1000] - train loss : 16.36621856689453, valid_loss : 35.764461517333984\n",
      "[0839 / 1000] - train loss : 16.344688415527344, valid_loss : 35.73731994628906\n",
      "[0840 / 1000] - train loss : 16.323320388793945, valid_loss : 35.70426940917969\n",
      "[0841 / 1000] - train loss : 16.30180549621582, valid_loss : 35.67286682128906\n",
      "[0842 / 1000] - train loss : 16.280309677124023, valid_loss : 35.642940521240234\n",
      "[0843 / 1000] - train loss : 16.258764266967773, valid_loss : 35.61430740356445\n",
      "[0844 / 1000] - train loss : 16.237468719482422, valid_loss : 35.579803466796875\n",
      "[0845 / 1000] - train loss : 16.215776443481445, valid_loss : 35.54698181152344\n",
      "[0846 / 1000] - train loss : 16.194334030151367, valid_loss : 35.515716552734375\n",
      "[0847 / 1000] - train loss : 16.17287254333496, valid_loss : 35.48582458496094\n",
      "[0848 / 1000] - train loss : 16.151403427124023, valid_loss : 35.45702362060547\n",
      "[0849 / 1000] - train loss : 16.13001823425293, valid_loss : 35.422061920166016\n",
      "[0850 / 1000] - train loss : 16.10859489440918, valid_loss : 35.38872528076172\n",
      "[0851 / 1000] - train loss : 16.087236404418945, valid_loss : 35.356868743896484\n",
      "[0852 / 1000] - train loss : 16.06586265563965, valid_loss : 35.32633590698242\n",
      "[0853 / 1000] - train loss : 16.044525146484375, valid_loss : 35.29805374145508\n",
      "[0854 / 1000] - train loss : 16.023143768310547, valid_loss : 35.271724700927734\n",
      "[0855 / 1000] - train loss : 16.00198745727539, valid_loss : 35.239967346191406\n",
      "[0856 / 1000] - train loss : 15.980378150939941, valid_loss : 35.203338623046875\n",
      "[0857 / 1000] - train loss : 15.958853721618652, valid_loss : 35.169437408447266\n",
      "[0858 / 1000] - train loss : 15.937456130981445, valid_loss : 35.138118743896484\n",
      "[0859 / 1000] - train loss : 15.915946960449219, valid_loss : 35.1091423034668\n",
      "[0860 / 1000] - train loss : 15.894404411315918, valid_loss : 35.082176208496094\n",
      "[0861 / 1000] - train loss : 15.872819900512695, valid_loss : 35.056766510009766\n",
      "[0862 / 1000] - train loss : 15.85133171081543, valid_loss : 35.032745361328125\n",
      "[0863 / 1000] - train loss : 15.829904556274414, valid_loss : 35.008548736572266\n",
      "[0864 / 1000] - train loss : 15.809097290039062, valid_loss : 34.97627639770508\n",
      "[0865 / 1000] - train loss : 15.787610054016113, valid_loss : 34.93730545043945\n",
      "[0866 / 1000] - train loss : 15.766347885131836, valid_loss : 34.898929595947266\n",
      "[0867 / 1000] - train loss : 15.745387077331543, valid_loss : 34.86308670043945\n",
      "[0868 / 1000] - train loss : 15.724424362182617, valid_loss : 34.829803466796875\n",
      "[0869 / 1000] - train loss : 15.703414916992188, valid_loss : 34.79905700683594\n",
      "[0870 / 1000] - train loss : 15.68237018585205, valid_loss : 34.770164489746094\n",
      "[0871 / 1000] - train loss : 15.661294937133789, valid_loss : 34.7415885925293\n",
      "[0872 / 1000] - train loss : 15.640196800231934, valid_loss : 34.71318435668945\n",
      "[0873 / 1000] - train loss : 15.619108200073242, valid_loss : 34.68483352661133\n",
      "[0874 / 1000] - train loss : 15.59803581237793, valid_loss : 34.65638732910156\n",
      "[0875 / 1000] - train loss : 15.576974868774414, valid_loss : 34.62786102294922\n",
      "[0876 / 1000] - train loss : 15.555925369262695, valid_loss : 34.599002838134766\n",
      "[0877 / 1000] - train loss : 15.534945487976074, valid_loss : 34.56975555419922\n",
      "[0878 / 1000] - train loss : 15.514001846313477, valid_loss : 34.53934860229492\n",
      "[0879 / 1000] - train loss : 15.493101119995117, valid_loss : 34.50788497924805\n",
      "[0880 / 1000] - train loss : 15.472167015075684, valid_loss : 34.47556686401367\n",
      "[0881 / 1000] - train loss : 15.451520919799805, valid_loss : 34.4372673034668\n",
      "[0882 / 1000] - train loss : 15.430768966674805, valid_loss : 34.40099334716797\n",
      "[0883 / 1000] - train loss : 15.41012954711914, valid_loss : 34.366634368896484\n",
      "[0884 / 1000] - train loss : 15.38948917388916, valid_loss : 34.33399200439453\n",
      "[0885 / 1000] - train loss : 15.368867874145508, valid_loss : 34.302955627441406\n",
      "[0886 / 1000] - train loss : 15.348265647888184, valid_loss : 34.27337646484375\n",
      "[0887 / 1000] - train loss : 15.327653884887695, valid_loss : 34.24506378173828\n",
      "[0888 / 1000] - train loss : 15.307028770446777, valid_loss : 34.21775817871094\n",
      "[0889 / 1000] - train loss : 15.28640365600586, valid_loss : 34.19110870361328\n",
      "[0890 / 1000] - train loss : 15.265941619873047, valid_loss : 34.163082122802734\n",
      "[0891 / 1000] - train loss : 15.245540618896484, valid_loss : 34.13352966308594\n",
      "[0892 / 1000] - train loss : 15.225049018859863, valid_loss : 34.102779388427734\n",
      "[0893 / 1000] - train loss : 15.204523086547852, valid_loss : 34.07107925415039\n",
      "[0894 / 1000] - train loss : 15.184045791625977, valid_loss : 34.03922653198242\n",
      "[0895 / 1000] - train loss : 15.163673400878906, valid_loss : 34.007347106933594\n",
      "[0896 / 1000] - train loss : 15.143400192260742, valid_loss : 33.9767951965332\n",
      "[0897 / 1000] - train loss : 15.123156547546387, valid_loss : 33.94745635986328\n",
      "[0898 / 1000] - train loss : 15.102910995483398, valid_loss : 33.91920852661133\n",
      "[0899 / 1000] - train loss : 15.082674026489258, valid_loss : 33.89192581176758\n",
      "[0900 / 1000] - train loss : 15.062675476074219, valid_loss : 33.85675048828125\n",
      "[0901 / 1000] - train loss : 15.042431831359863, valid_loss : 33.82189178466797\n",
      "[0902 / 1000] - train loss : 15.022396087646484, valid_loss : 33.787471771240234\n",
      "[0903 / 1000] - train loss : 15.002388000488281, valid_loss : 33.75349807739258\n",
      "[0904 / 1000] - train loss : 14.982420921325684, valid_loss : 33.71976089477539\n",
      "[0905 / 1000] - train loss : 14.962520599365234, valid_loss : 33.687644958496094\n",
      "[0906 / 1000] - train loss : 14.942645072937012, valid_loss : 33.65711212158203\n",
      "[0907 / 1000] - train loss : 14.922794342041016, valid_loss : 33.626686096191406\n",
      "[0908 / 1000] - train loss : 14.903007507324219, valid_loss : 33.59636306762695\n",
      "[0909 / 1000] - train loss : 14.883230209350586, valid_loss : 33.56612014770508\n",
      "[0910 / 1000] - train loss : 14.863481521606445, valid_loss : 33.537315368652344\n",
      "[0911 / 1000] - train loss : 14.843786239624023, valid_loss : 33.50843811035156\n",
      "[0912 / 1000] - train loss : 14.824159622192383, valid_loss : 33.479434967041016\n",
      "[0913 / 1000] - train loss : 14.804574012756348, valid_loss : 33.4503059387207\n",
      "[0914 / 1000] - train loss : 14.785038948059082, valid_loss : 33.422306060791016\n",
      "[0915 / 1000] - train loss : 14.765530586242676, valid_loss : 33.39397048950195\n",
      "[0916 / 1000] - train loss : 14.746106147766113, valid_loss : 33.3658447265625\n",
      "[0917 / 1000] - train loss : 14.72667407989502, valid_loss : 33.3365592956543\n",
      "[0918 / 1000] - train loss : 14.707266807556152, valid_loss : 33.30760192871094\n",
      "[0919 / 1000] - train loss : 14.68789005279541, valid_loss : 33.27955627441406\n",
      "[0920 / 1000] - train loss : 14.66856575012207, valid_loss : 33.2523307800293\n",
      "[0921 / 1000] - train loss : 14.649291038513184, valid_loss : 33.224483489990234\n",
      "[0922 / 1000] - train loss : 14.63003158569336, valid_loss : 33.19597625732422\n",
      "[0923 / 1000] - train loss : 14.610725402832031, valid_loss : 33.16678237915039\n",
      "[0924 / 1000] - train loss : 14.591269493103027, valid_loss : 33.13709259033203\n",
      "[0925 / 1000] - train loss : 14.571852684020996, valid_loss : 33.10831069946289\n",
      "[0926 / 1000] - train loss : 14.552419662475586, valid_loss : 33.08033752441406\n",
      "[0927 / 1000] - train loss : 14.532968521118164, valid_loss : 33.053016662597656\n",
      "[0928 / 1000] - train loss : 14.513567924499512, valid_loss : 33.02477264404297\n",
      "[0929 / 1000] - train loss : 14.494218826293945, valid_loss : 32.995059967041016\n",
      "[0930 / 1000] - train loss : 14.474847793579102, valid_loss : 32.9647102355957\n",
      "[0931 / 1000] - train loss : 14.455534934997559, valid_loss : 32.933807373046875\n",
      "[0932 / 1000] - train loss : 14.436284065246582, valid_loss : 32.903785705566406\n",
      "[0933 / 1000] - train loss : 14.417055130004883, valid_loss : 32.87458801269531\n",
      "[0934 / 1000] - train loss : 14.39783763885498, valid_loss : 32.84611129760742\n",
      "[0935 / 1000] - train loss : 14.378643989562988, valid_loss : 32.81826400756836\n",
      "[0936 / 1000] - train loss : 14.359529495239258, valid_loss : 32.78976058959961\n",
      "[0937 / 1000] - train loss : 14.340475082397461, valid_loss : 32.75996780395508\n",
      "[0938 / 1000] - train loss : 14.321430206298828, valid_loss : 32.72957992553711\n",
      "[0939 / 1000] - train loss : 14.302457809448242, valid_loss : 32.698707580566406\n",
      "[0940 / 1000] - train loss : 14.283512115478516, valid_loss : 32.66743087768555\n",
      "[0941 / 1000] - train loss : 14.264612197875977, valid_loss : 32.635982513427734\n",
      "[0942 / 1000] - train loss : 14.245774269104004, valid_loss : 32.60575485229492\n",
      "[0943 / 1000] - train loss : 14.226954460144043, valid_loss : 32.57665252685547\n",
      "[0944 / 1000] - train loss : 14.208402633666992, valid_loss : 32.551204681396484\n",
      "[0945 / 1000] - train loss : 14.189837455749512, valid_loss : 32.52887725830078\n",
      "[0946 / 1000] - train loss : 14.171231269836426, valid_loss : 32.5084342956543\n",
      "[0947 / 1000] - train loss : 14.15265941619873, valid_loss : 32.48955154418945\n",
      "[0948 / 1000] - train loss : 14.134078979492188, valid_loss : 32.46810531616211\n",
      "[0949 / 1000] - train loss : 14.11567211151123, valid_loss : 32.444068908691406\n",
      "[0950 / 1000] - train loss : 14.097254753112793, valid_loss : 32.41831588745117\n",
      "[0951 / 1000] - train loss : 14.0787992477417, valid_loss : 32.3911018371582\n",
      "[0952 / 1000] - train loss : 14.060389518737793, valid_loss : 32.36651611328125\n",
      "[0953 / 1000] - train loss : 14.042010307312012, valid_loss : 32.34423828125\n",
      "[0954 / 1000] - train loss : 14.023626327514648, valid_loss : 32.32008361816406\n",
      "[0955 / 1000] - train loss : 14.00534725189209, valid_loss : 32.294166564941406\n",
      "[0956 / 1000] - train loss : 13.987160682678223, valid_loss : 32.271881103515625\n",
      "[0957 / 1000] - train loss : 13.968927383422852, valid_loss : 32.24874496459961\n",
      "[0958 / 1000] - train loss : 13.950786590576172, valid_loss : 32.22334671020508\n",
      "[0959 / 1000] - train loss : 13.932677268981934, valid_loss : 32.19976043701172\n",
      "[0960 / 1000] - train loss : 13.914609909057617, valid_loss : 32.17378616333008\n",
      "[0961 / 1000] - train loss : 13.896621704101562, valid_loss : 32.14957046508789\n",
      "[0962 / 1000] - train loss : 13.87865161895752, valid_loss : 32.12677001953125\n",
      "[0963 / 1000] - train loss : 13.860758781433105, valid_loss : 32.101226806640625\n",
      "[0964 / 1000] - train loss : 13.842889785766602, valid_loss : 32.07451629638672\n",
      "[0965 / 1000] - train loss : 13.825095176696777, valid_loss : 32.05072021484375\n",
      "[0966 / 1000] - train loss : 13.807296752929688, valid_loss : 32.02809143066406\n",
      "[0967 / 1000] - train loss : 13.789617538452148, valid_loss : 32.00299835205078\n",
      "[0968 / 1000] - train loss : 13.771917343139648, valid_loss : 31.97565460205078\n",
      "[0969 / 1000] - train loss : 13.754210472106934, valid_loss : 31.94629669189453\n",
      "[0970 / 1000] - train loss : 13.736671447753906, valid_loss : 31.920814514160156\n",
      "[0971 / 1000] - train loss : 13.719101905822754, valid_loss : 31.89860725402832\n",
      "[0972 / 1000] - train loss : 13.701525688171387, valid_loss : 31.87786293029785\n",
      "[0973 / 1000] - train loss : 13.684136390686035, valid_loss : 31.85431671142578\n",
      "[0974 / 1000] - train loss : 13.666760444641113, valid_loss : 31.828081130981445\n",
      "[0975 / 1000] - train loss : 13.649360656738281, valid_loss : 31.799453735351562\n",
      "[0976 / 1000] - train loss : 13.631937026977539, valid_loss : 31.768712997436523\n",
      "[0977 / 1000] - train loss : 13.61464786529541, valid_loss : 31.74016761779785\n",
      "[0978 / 1000] - train loss : 13.597437858581543, valid_loss : 31.713756561279297\n",
      "[0979 / 1000] - train loss : 13.580219268798828, valid_loss : 31.689273834228516\n",
      "[0980 / 1000] - train loss : 13.562997817993164, valid_loss : 31.666494369506836\n",
      "[0981 / 1000] - train loss : 13.545731544494629, valid_loss : 31.645179748535156\n",
      "[0982 / 1000] - train loss : 13.52864933013916, valid_loss : 31.621082305908203\n",
      "[0983 / 1000] - train loss : 13.511575698852539, valid_loss : 31.59442901611328\n",
      "[0984 / 1000] - train loss : 13.494462966918945, valid_loss : 31.56550407409668\n",
      "[0985 / 1000] - train loss : 13.477363586425781, valid_loss : 31.535974502563477\n",
      "[0986 / 1000] - train loss : 13.460341453552246, valid_loss : 31.50991439819336\n",
      "[0987 / 1000] - train loss : 13.44336223602295, valid_loss : 31.486967086791992\n",
      "[0988 / 1000] - train loss : 13.426370620727539, valid_loss : 31.465517044067383\n",
      "[0989 / 1000] - train loss : 13.40953540802002, valid_loss : 31.441181182861328\n",
      "[0990 / 1000] - train loss : 13.392704963684082, valid_loss : 31.41420555114746\n",
      "[0991 / 1000] - train loss : 13.375839233398438, valid_loss : 31.3848819732666\n",
      "[0992 / 1000] - train loss : 13.359107971191406, valid_loss : 31.3577823638916\n",
      "[0993 / 1000] - train loss : 13.342426300048828, valid_loss : 31.33256721496582\n",
      "[0994 / 1000] - train loss : 13.325751304626465, valid_loss : 31.309024810791016\n",
      "[0995 / 1000] - train loss : 13.30907917022705, valid_loss : 31.28708839416504\n",
      "[0996 / 1000] - train loss : 13.29242992401123, valid_loss : 31.26651382446289\n",
      "[0997 / 1000] - train loss : 13.275972366333008, valid_loss : 31.243059158325195\n",
      "[0998 / 1000] - train loss : 13.259509086608887, valid_loss : 31.21691131591797\n",
      "[0999 / 1000] - train loss : 13.242998123168945, valid_loss : 31.18837547302246\n",
      "[1000 / 1000] - train loss : 13.2264404296875, valid_loss : 31.1577205657959\n",
      "학습에 걸린 시간 4.317446708679199초\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(boston_model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "import time\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    ## train\n",
    "    # 1. model change to train mode\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0\n",
    "    # 2. batch 단위로 학습 - 반복문\n",
    "    for X_train, y_train in train_loader :\n",
    "        # 3. X, y를 device로 이동\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 4. 추론\n",
    "        pred = boston_model(X_train) #model.forward(X_train)\n",
    "        # 5. loss 계산\n",
    "        loss = loss_fn(pred, y_train) # (추론한 값, 정답)\n",
    "        # 6. parameter들의 gradient 구함\n",
    "        loss.backward()\n",
    "        # 7. parameter update\n",
    "        optimizer.step()\n",
    "        # 8. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 로그\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    ## Evaluation\n",
    "    # 1. eval mode\n",
    "    boston_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    # 2. gradient 구하지않게 처리\n",
    "    with torch.no_grad() :\n",
    "        for X_valid, y_valid in test_loader : \n",
    "            # 3. device 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            # 4. 추론\n",
    "            pred_valid = boston_model(X_valid)\n",
    "            # 5. 검증 평가\n",
    "            loss_valid = loss_fn(pred_valid, y_valid)\n",
    "            valid_loss += loss_valid.item()\n",
    "\n",
    "        valid_loss = valid_loss / len(test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f'[{epoch+1:04d} / {epochs}] - train loss : {train_loss}, valid_loss : {valid_loss}')\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print(f'학습에 걸린 시간 {e-s}초')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), train_losses, label='train_set')\n",
    "plt.plot(range(epochs), valid_losses, label='valid_set')\n",
    "\n",
    "plt.title('학습 loss')\n",
    "plt.ylim(10, 50)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 모델 저장\n",
    "saved_path = 'saved_models/boston_model.pth'\n",
    "torch.save(boston_model, saved_path)\n",
    "\n",
    "# load\n",
    "model = torch.load(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.6305, -0.4996,  0.9989, -0.2729,  1.0700, -0.4969,  1.1033, -0.9469,\n",
      "         1.6874,  1.5421,  0.7927, -3.8659,  1.0856])\n",
      "tensor([[ 8.6305, -0.4996,  0.9989, -0.2729,  1.0700, -0.4969,  1.1033, -0.9469,\n",
      "          1.6874,  1.5421,  0.7927, -3.8659,  1.0856]])\n",
      "예상 집 값 :  4.205865859985352\n"
     ]
    }
   ],
   "source": [
    "summary(model, (10, 13))\n",
    "\n",
    "# application에서 새로운 데이터 추론\n",
    "new_data = testset[10][0]\n",
    "print(new_data)\n",
    "new_data = new_data.reshape(1, -1)\n",
    "print(new_data)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    y_pred = model(new_data)\n",
    "    print('예상 집 값 : ', y_pred.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 분류 (Classification)\n",
    "\n",
    "### Fashion MNIST Dataset - 다중분류(Multi-Class Classification) 문제\n",
    "\n",
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋.\n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Feature**이미지는 28x28 크기이며 Gray scale이다.\n",
    "- **Target**은 총 10개의 class로 구성되어 있으며 각 class의 class 이름은 다음과 같다.\n",
    "\n",
    "| 레이블 | 클래스       |\n",
    "|--------|--------------|\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trousers    |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#data load\n",
    "data_root = 'datasets/fashion_mnist'\n",
    "trainset = FashionMNIST(root=data_root, train=True, download=True)\n",
    "testset = FashionMNIST(root=data_root, train=False, download=True)\n",
    "\n",
    "# train을 분리해서 trainset/validset 나누기\n",
    "trainset, validset = random_split(trainset, [50000, 10000])\n",
    "len(trainset), len(validset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input image 확인\n",
    "idx = 0\n",
    "x, y = testset[idx]\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "# plt.imshow(x, cmap='gray')  # 0 : black, 255 : white\n",
    "plt.imshow(x, cmap='Greys')  # 0 : white, 255 : black\n",
    "plt.title(f'{y} - {testset.classes[y]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 40 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data load\n",
    "# transform 지정 : 전처리 로직 추가\n",
    "data_root = 'datasets/fashion_mnist'\n",
    "trainset = FashionMNIST(root=data_root, train=True, download=True, transform=ToTensor())\n",
    "testset = FashionMNIST(root=data_root, train=False, download=True, transform=ToTensor())\n",
    "\n",
    "trainset, validset = random_split(trainset, [50000, 10000])\n",
    "\n",
    "#dataloader 생성\n",
    "train_loader = DataLoader(trainset, batch_size=256, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(validset, batch_size=256)\n",
    "test_loader = DataLoader(testset, batch_size=256)\n",
    "\n",
    "print(len(train_loader), len(valid_loader), len(test_loader))  # step 수,\n",
    "trainset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 구현\n",
    "class FashionMNMISTModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(784, 1024) # 첫번째 linear - in_features=입력 feature 갯수 (1 * 28 * 28)\n",
    "        self.lr2 = nn.Linear(1024, 512)\n",
    "        self.lr3 = nn.Linear(512, 256)\n",
    "        self.lr4 = nn.Linear(256, 128)\n",
    "        self.lr5 = nn.Linear(128, 64)\n",
    "        self.lr6 = nn.Linear(64, 10) # 마지막 linear - out_features = 출력 결과 갯수 (다중 분류 : 클래스 갯수) => class별 정답일 확률\n",
    "        # hidden layer에 적용할 activation 함수\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X) :\n",
    "        # torch.flatten(X, start_dim=1) # (256, 1, 28, 28) -> 전체를 1차원으로 바꿈 하지만 (1, 28, 28)을 flatten 시켜야해서 start_dim 설정\n",
    "        X = nn.Flatten()(X) # start_dim = 1로해서 flatten 처리를 함\n",
    "        X = self.lr1(X)  # 1차원으로 바꿔줘야함\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr3(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr4(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr5(X)\n",
    "        X = self.relu(X)\n",
    "        output = self.lr6(X)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNMISTModel(\n",
       "  (lr1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (lr2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (lr3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (lr4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (lr5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lr6): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 구조 확인\n",
    "# 모델 생성\n",
    "f_model = FashionMNMISTModel().to(device)\n",
    "f_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNMISTModel                       [256, 10]                 --\n",
       "├─Linear: 1-1                            [256, 1024]               803,840\n",
       "├─ReLU: 1-2                              [256, 1024]               --\n",
       "├─Linear: 1-3                            [256, 512]                524,800\n",
       "├─ReLU: 1-4                              [256, 512]                --\n",
       "├─Linear: 1-5                            [256, 256]                131,328\n",
       "├─ReLU: 1-6                              [256, 256]                --\n",
       "├─Linear: 1-7                            [256, 128]                32,896\n",
       "├─ReLU: 1-8                              [256, 128]                --\n",
       "├─Linear: 1-9                            [256, 64]                 8,256\n",
       "├─ReLU: 1-10                             [256, 64]                 --\n",
       "├─Linear: 1-11                           [256, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 1,501,770\n",
       "Trainable params: 1,501,770\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 384.45\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 4.08\n",
       "Params size (MB): 6.01\n",
       "Estimated Total Size (MB): 10.89\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(f_model, (256, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 저장\n",
    "- 학습 도중 가장 좋은 성능의 모델이 나올 수 있다\n",
    "- 모델을 저장할 경우 epoch 단위로 저장\n",
    "  1. 모든 에폭이 끝나고 모델을 저장\n",
    "  2. 가장 성능이 좋은 시점의 모델을 저장\n",
    "      - best socre와 현재 epoch의 성능을 비교해서 성능 개선이 있으면 덮어쓰기로 저장\n",
    "#### 조기종료 (Early stopping) #####\n",
    "- 학습 도중 성능 개선이 없으면 중간 epoch에서 학습 멈춤\n",
    "- epoch을 길게 잡아주고 성능이 개선될 때 마다 저장, 특정 횟수의 epoch동안 성능 개선이 없으면 조기 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m f_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     30\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;66;03m# 현재 epoch의 train loss 저장할 변수\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# device로 이동\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tain\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 추론\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torchvision/datasets/mnist.py:142\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    138\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "# optimizer , loss 함수\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "optimizer = optim.Adam(f_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# crossEntropyLoss() - 다중 분류 시 사용하는 loss 함수\n",
    "# (모델예측값, 정답)\n",
    "# 1. 모델 예측값을 softmax() 함수 이용해서 확률값을 반환\n",
    "# 2. 정답은 OHE 처리\n",
    "# 3. 1, 2에서 변환한 값으로 loss 계산\n",
    "\n",
    "# train\n",
    "import time\n",
    "##############################\n",
    "# epoch 도중 성능이 개선되면 모델 저장\n",
    "# 조기종료\n",
    "##############################\n",
    "\n",
    "save_path = 'saved_models/fashion_mnist_model.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_acc_list = []\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs) :\n",
    "    f_model.train()\n",
    "    train_loss = 0.0 # 현재 epoch의 train loss 저장할 변수\n",
    "    for X_train, y_train in train_loader :\n",
    "        # device로 이동\n",
    "        X_train, y_tain = X_train.to(device), y_train.to(device)\n",
    "        # 추론\n",
    "        pred = f_model(X_train) # forward 호출 (call에서 호출되게되어있음)\n",
    "        # loss\n",
    "        loss = loss_fn(pred, y_train)\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "        # 파라미터들 update\n",
    "        optimizer.step()\n",
    "        # 파라미터의 gradient값 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # loss값 train 변수에 누적\n",
    "        train_loss = train_loss + loss.item()\n",
    "    train_loss = train_loss / len(train_loader) # step수로 나눔, 현재 epoch의 평균 train loss 계산\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    ################# 검증 ###############\n",
    "    f_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    # weight 업데이트는 하지않는다. 추론할때 grad_fn을 구할 필요가 없음\n",
    "    with torch.no_grad() :\n",
    "        for X_valid, y_valid in valid_loader :\n",
    "            # 1. device로 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            # 2. 추론\n",
    "            pred_valid = f_model(X_valid)\n",
    "            # 3. 검증작업\n",
    "            # loss 누적\n",
    "            valid_loss = valid_loss + loss_fn(pred_valid, y_valid).item()\n",
    "            # 정확도 계산\n",
    "            pred_class = pred_valid.argmax(dim=-1) # (256, 10) -> 10개 당 max값의 index 조회 (256, 1)\n",
    "            valid_acc = valid_acc + torch.sum(pred_class == y_valid).item()\n",
    "        #검증 결과 계산\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_acc = valid_acc / len(valid_loader.dataset) # 총 데이터 갯수로 나누기 (맞은 갯수 / 총 갯수)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        print(f'[{epoch} / {epochs}] - train_loss : {train_loss}, valid_loss : {valid_loss}, valid_acc : {valid_acc}')\n",
    "\n",
    "e = time.time()\n",
    "print(f'학습에 걸린 시간 : {e-s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(20), train_losses, label='Train set')\n",
    "plt.plot(range(20), valid_losses, label='Validation set')\n",
    "plt.title('loss')\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.legend\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(20), valid_acc_list)\n",
    "plt.title('Validation accuracy')\n",
    "plt.grid(True, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmin(valid_losses) # loss가 가장 작은 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 / 100] - train_loss : 0.48690268053458285, valid_loss : 0.47548875883221625, valid_acc : 0.8423\n",
      ">>>>>>>>>>>> 모델 저장 : [EPOCH : 1] inf에서 0.47548875883221625가 개선됨 <<<<<<<<<<<<<<<<\n",
      "[1 / 100] - train_loss : 0.40246741015177506, valid_loss : 0.40207309871912, valid_acc : 0.8517\n",
      ">>>>>>>>>>>> 모델 저장 : [EPOCH : 2] 0.47548875883221625에서 0.40207309871912가 개선됨 <<<<<<<<<<<<<<<<\n",
      "[2 / 100] - train_loss : 0.37983794548572636, valid_loss : 0.3707826644182205, valid_acc : 0.8685\n",
      ">>>>>>>>>>>> 모델 저장 : [EPOCH : 3] 0.40207309871912에서 0.3707826644182205가 개선됨 <<<<<<<<<<<<<<<<\n",
      "[3 / 100] - train_loss : 0.35615588411306726, valid_loss : 0.38788650184869766, valid_acc : 0.8591\n",
      "[4 / 100] - train_loss : 0.3503996475384786, valid_loss : 0.37361818701028826, valid_acc : 0.8695\n",
      "[5 / 100] - train_loss : 0.34260724095197825, valid_loss : 0.35371297113597394, valid_acc : 0.8747\n",
      ">>>>>>>>>>>> 모델 저장 : [EPOCH : 6] 0.3707826644182205에서 0.35371297113597394가 개선됨 <<<<<<<<<<<<<<<<\n",
      "[6 / 100] - train_loss : 0.3331233927836785, valid_loss : 0.35902916453778744, valid_acc : 0.874\n",
      "[7 / 100] - train_loss : 0.33585390333945936, valid_loss : 0.36739266850054264, valid_acc : 0.8729\n",
      "[8 / 100] - train_loss : 0.3181886047889025, valid_loss : 0.3919459030032158, valid_acc : 0.863\n",
      "[9 / 100] - train_loss : 0.3676553334181125, valid_loss : 0.43973966762423516, valid_acc : 0.864\n",
      "[10 / 100] - train_loss : 0.35014656827999996, valid_loss : 0.37649255730211734, valid_acc : 0.8717\n",
      "==============11에서 조기종료 =====================\n",
      "학습에 걸린 시간 : 90.18473410606384\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "# optimizer , loss 함수\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "optimizer = optim.Adam(f_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# crossEntropyLoss() - 다중 분류 시 사용하는 loss 함수\n",
    "# (모델예측값, 정답)\n",
    "# 1. 모델 예측값을 softmax() 함수 이용해서 확률값을 반환\n",
    "# 2. 정답은 OHE 처리\n",
    "# 3. 1, 2에서 변환한 값으로 loss 계산\n",
    "\n",
    "# train\n",
    "import time\n",
    "##############################\n",
    "# epoch 도중 성능이 개선되면 모델 저장\n",
    "# 조기종료\n",
    "##############################\n",
    "\n",
    "save_path = 'saved_models/fashion_mnist_model.pt'\n",
    "best_score = torch.inf # validation loss 기준으로 저장 여부를 확인. loss가 낮아지면 성능이 개선\n",
    "\n",
    "# 조기종료 관련 변수\n",
    "patience = 5 # 성능이 개선되는지 몇 epoch동안 기다릴건지 저장\n",
    "trigger_cnt = 0 # 성능이 개선되는지 몇 epoch 기다리는지 확인할 변수\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_acc_list = []\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs) :\n",
    "    f_model.train()\n",
    "    train_loss = 0.0 # 현재 epoch의 train loss 저장할 변수\n",
    "    for X_train, y_train in train_loader :\n",
    "        # device로 이동\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 추론\n",
    "        pred = f_model(X_train) # forward 호출 (call에서 호출되게되어있음)\n",
    "        # loss\n",
    "        loss = loss_fn(pred, y_train)\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "        # 파라미터들 update\n",
    "        optimizer.step()\n",
    "        # 파라미터의 gradient값 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # loss값 train 변수에 누적\n",
    "        train_loss = train_loss + loss.item()\n",
    "    train_loss = train_loss / len(train_loader) # step수로 나눔, 현재 epoch의 평균 train loss 계산\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    ################# 검증 ###############\n",
    "    f_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    # weight 업데이트는 하지않는다. 추론할때 grad_fn을 구할 필요가 없음\n",
    "    with torch.no_grad() :\n",
    "        for X_valid, y_valid in valid_loader :\n",
    "            # 1. device로 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            # 2. 추론\n",
    "            pred_valid = f_model(X_valid)\n",
    "            # 3. 검증작업\n",
    "            # loss 누적\n",
    "            valid_loss = valid_loss + loss_fn(pred_valid, y_valid).item()\n",
    "            # 정확도 계산\n",
    "            pred_class = pred_valid.argmax(dim=-1) # (256, 10) -> 10개 당 max값의 index 조회 (256, 1)\n",
    "            valid_acc = valid_acc + torch.sum(pred_class == y_valid).item()\n",
    "        #검증 결과 계산\n",
    "        valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_acc = valid_acc / len(valid_loader.dataset) # 총 데이터 갯수로 나누기 (맞은 갯수 / 총 갯수)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        print(f'[{epoch} / {epochs}] - train_loss : {train_loss}, valid_loss : {valid_loss}, valid_acc : {valid_acc}')\n",
    "\n",
    "    ############ 검증 완료 -> 모델 저장 ###############\n",
    "    if valid_loss < best_score :\n",
    "       #log 출력\n",
    "        print(f'>>>>>>>>>>>> 모델 저장 : [EPOCH : {epoch+1}] {best_score}에서 {valid_loss}가 개선됨 <<<<<<<<<<<<<<<<')\n",
    " \n",
    "        torch.save(f_model, save_path)\n",
    "        best_score = valid_loss # best score 현재 loss로 저장\n",
    "\n",
    "        trigger_cnt = 0\n",
    "    else :\n",
    "        trigger_cnt = trigger_cnt + 1\n",
    "        \n",
    "    if trigger_cnt == patience :\n",
    "        print(f'=============={epoch+1}에서 조기종료 =====================')\n",
    "        break\n",
    "   \n",
    "e = time.time()\n",
    "print(f'학습에 걸린 시간 : {e-s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss : 0.39084650576114655, test_acc : 0.864\n"
     ]
    }
   ],
   "source": [
    "## 저장된 모델 이용해 최종평가\n",
    "# testset data\n",
    "\n",
    "#model load\n",
    "best_model = torch.load(save_path)\n",
    "\n",
    "best_model\n",
    "\n",
    "# eval mode 변경\n",
    "best_model.eval()\n",
    "# device 이동\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "#평과결과 저장할 변수\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "with torch.no_grad() :\n",
    "    for X_test, y_test in test_loader :\n",
    "        # device 변경\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        #추론\n",
    "        pred_test = best_model(X_test)\n",
    "        #평가\n",
    "        test_loss = test_loss + loss_fn(pred_test, y_test).item()\n",
    "        #accuracy (맞은 갯수 누적)\n",
    "        pred_test_class = pred_test.argmax(dim=-1)\n",
    "        test_acc = test_acc + torch.sum(y_test == pred_test_class).item()\n",
    "\n",
    "    #검증 결과\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = test_acc / len(test_loader.dataset)\n",
    "    print(f'test_loss : {test_loss}, test_acc : {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.8171, 0.8645, 0.9967], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([9, 4, 3]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## 새로운 데이터 예측\n",
    "# test set 3개 조회하여 추론\n",
    "\n",
    "# testset[[0, 2, 3]] # slicing /fancy indexing 안됨\n",
    "# 3개의 데이터 저장할 변수\n",
    "new_data = torch.empty(3, 1, 28, 28) # (1, 28, 28) X 3의 빈 tensor 만들어달라\n",
    "\n",
    "new_data[0] = testset[0][0]\n",
    "new_data[1] = testset[10][0]\n",
    "new_data[2] = testset[100][0]\n",
    "\n",
    "p = best_model(new_data)\n",
    "p.shape\n",
    "\n",
    "p_class = p.argmax(dim=-1)\n",
    "p_class\n",
    "\n",
    "p = nn.Softmax(dim=-1)(p)\n",
    "p.max(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict (model, X, device='cpu') :\n",
    "    #model로 x를 추정한 결과 반환 [class_index, 정답의 확률]\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad () :\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        proba = nn.Softmax(dim=-1)(pred) # 모델 추론값을 높은 확률로 변환\n",
    "        m = proba.max(dim=-1)\n",
    "        return m.indices, m.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([9, 4, 3]), tensor([0.8171, 0.8645, 0.9967]))\n"
     ]
    }
   ],
   "source": [
    "result = predict(best_model, new_data, device=device)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9-Ankle boot-0.8170903325080872\n",
      "4-Coat-0.8644721508026123\n",
      "3-Dress-0.996675968170166\n"
     ]
    }
   ],
   "source": [
    "for class_index, prob in zip(*result) :\n",
    "    print(class_index.item(), testset.classes[class_index], prob.item(), sep='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 위스콘신 유방암 데이터셋 - 이진분류(Binary Classification) 문제\n",
    "\n",
    "-   **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "-   위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "-   Feature\n",
    "    -   종양에 대한 다양한 측정값들\n",
    "-   Target의 class\n",
    "    -   0 - malignant(악성종양)\n",
    "    -   1 - benign(양성종양)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 전처리\n",
    "Scaler = StandardScaler()\n",
    "X_train_scaled = Scaler.fit_transform(X_train)\n",
    "X_test_scaled = Scaler.fit_transform(X_test)\n",
    "\n",
    "classes = np.array(['악성', '양성'])\n",
    "class_to_idx = {'악성':0, '양성':1}\n",
    "\n",
    "#data set \n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "\n",
    "trainset.classes = classes\n",
    "trainset.class_to_idx = class_to_idx\n",
    "\n",
    "#data loader\n",
    "train_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class BreastCancerModel(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(30, 32) # X.shape 30임\n",
    "        self.lr2 = nn.Linear(32, 8)\n",
    "        self.lr3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logisitic = nn.Sigmoid() # 입력값을 0~1사이 실수로 반환\n",
    "        \n",
    "    def forward(self, X) :\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        X = self.relu(X)\n",
    "        output = self.lr3(X)\n",
    "        output = self.logisitic(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4146],\n",
       "        [0.4284],\n",
       "        [0.4243],\n",
       "        [0.4118],\n",
       "        [0.3946],\n",
       "        [0.4225],\n",
       "        [0.4201],\n",
       "        [0.4435],\n",
       "        [0.4279],\n",
       "        [0.4313]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_model = BreastCancerModel().to(device)\n",
    "summary(b_model, (10, 30), device=device)\n",
    "\n",
    "#dummy data 출력\n",
    "dummy_x = torch.randn(10, 30)\n",
    "result = b_model(dummy_x)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result > 0.5).type(torch.int32) #bool -> int (True : 1 / False : 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_model = b_model.to(device)\n",
    "optimizer = optim.Adam(b_model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()  # 함수이름 binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0001/1000] train loss: 0.4739006608724594, valid loss: 0.4652341306209564, valid_acc: 0.9210526315789473\n",
      ">>>>>>>>> 모델 저장: 1, inf에서 0.4652341306209564로 loss가 개선되어 저장함.\n",
      "[0002/1000] train loss: 0.45880429446697235, valid loss: 0.44607359170913696, valid_acc: 0.9210526315789473\n",
      ">>>>>>>>> 모델 저장: 2, 0.4652341306209564에서 0.44607359170913696로 loss가 개선되어 저장함.\n",
      "[0003/1000] train loss: 0.4355704188346863, valid loss: 0.4272115230560303, valid_acc: 0.9122807017543859\n",
      ">>>>>>>>> 모델 저장: 3, 0.44607359170913696에서 0.4272115230560303로 loss가 개선되어 저장함.\n",
      "[0004/1000] train loss: 0.4149974286556244, valid loss: 0.40876054763793945, valid_acc: 0.9122807017543859\n",
      ">>>>>>>>> 모델 저장: 4, 0.4272115230560303에서 0.40876054763793945로 loss가 개선되어 저장함.\n",
      "[0005/1000] train loss: 0.3971170485019684, valid loss: 0.39075788855552673, valid_acc: 0.9122807017543859\n",
      ">>>>>>>>> 모델 저장: 5, 0.40876054763793945에서 0.39075788855552673로 loss가 개선되어 저장함.\n",
      "[0006/1000] train loss: 0.38408946990966797, valid loss: 0.3732738494873047, valid_acc: 0.9210526315789473\n",
      ">>>>>>>>> 모델 저장: 6, 0.39075788855552673에서 0.3732738494873047로 loss가 개선되어 저장함.\n",
      "[0007/1000] train loss: 0.36122260987758636, valid loss: 0.35649651288986206, valid_acc: 0.9210526315789473\n",
      ">>>>>>>>> 모델 저장: 7, 0.3732738494873047에서 0.35649651288986206로 loss가 개선되어 저장함.\n",
      "[0008/1000] train loss: 0.3484724462032318, valid loss: 0.34041035175323486, valid_acc: 0.9210526315789473\n",
      ">>>>>>>>> 모델 저장: 8, 0.35649651288986206에서 0.34041035175323486로 loss가 개선되어 저장함.\n",
      "[0009/1000] train loss: 0.32215416431427, valid loss: 0.325040340423584, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 9, 0.34041035175323486에서 0.325040340423584로 loss가 개선되어 저장함.\n",
      "[0010/1000] train loss: 0.3197919726371765, valid loss: 0.3104783892631531, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 10, 0.325040340423584에서 0.3104783892631531로 loss가 개선되어 저장함.\n",
      "[0011/1000] train loss: 0.2997497618198395, valid loss: 0.2967641353607178, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 11, 0.3104783892631531에서 0.2967641353607178로 loss가 개선되어 저장함.\n",
      "[0012/1000] train loss: 0.2816477119922638, valid loss: 0.28380411863327026, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 12, 0.2967641353607178에서 0.28380411863327026로 loss가 개선되어 저장함.\n",
      "[0013/1000] train loss: 0.2701772302389145, valid loss: 0.2717493176460266, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 13, 0.28380411863327026에서 0.2717493176460266로 loss가 개선되어 저장함.\n",
      "[0014/1000] train loss: 0.2595698684453964, valid loss: 0.26048344373703003, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 14, 0.2717493176460266에서 0.26048344373703003로 loss가 개선되어 저장함.\n",
      "[0015/1000] train loss: 0.2457887977361679, valid loss: 0.24994808435440063, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 15, 0.26048344373703003에서 0.24994808435440063로 loss가 개선되어 저장함.\n",
      "[0016/1000] train loss: 0.2424154430627823, valid loss: 0.23995600640773773, valid_acc: 0.9298245614035088\n",
      ">>>>>>>>> 모델 저장: 16, 0.24994808435440063에서 0.23995600640773773로 loss가 개선되어 저장함.\n",
      "[0017/1000] train loss: 0.22216596454381943, valid loss: 0.23070639371871948, valid_acc: 0.9385964912280702\n",
      ">>>>>>>>> 모델 저장: 17, 0.23995600640773773에서 0.23070639371871948로 loss가 개선되어 저장함.\n",
      "[0018/1000] train loss: 0.22109948843717575, valid loss: 0.22203363478183746, valid_acc: 0.9385964912280702\n",
      ">>>>>>>>> 모델 저장: 18, 0.23070639371871948에서 0.22203363478183746로 loss가 개선되어 저장함.\n",
      "[0019/1000] train loss: 0.2103612720966339, valid loss: 0.2138126790523529, valid_acc: 0.9473684210526315\n",
      ">>>>>>>>> 모델 저장: 19, 0.22203363478183746에서 0.2138126790523529로 loss가 개선되어 저장함.\n",
      "[0020/1000] train loss: 0.20211216062307358, valid loss: 0.2060428410768509, valid_acc: 0.9473684210526315\n",
      ">>>>>>>>> 모델 저장: 20, 0.2138126790523529에서 0.2060428410768509로 loss가 개선되어 저장함.\n",
      "[0021/1000] train loss: 0.19412799179553986, valid loss: 0.19878970086574554, valid_acc: 0.9473684210526315\n",
      ">>>>>>>>> 모델 저장: 21, 0.2060428410768509에서 0.19878970086574554로 loss가 개선되어 저장함.\n",
      "[0022/1000] train loss: 0.1886892169713974, valid loss: 0.1920182853937149, valid_acc: 0.956140350877193\n",
      ">>>>>>>>> 모델 저장: 22, 0.19878970086574554에서 0.1920182853937149로 loss가 개선되어 저장함.\n",
      "[0023/1000] train loss: 0.17696105688810349, valid loss: 0.18558606505393982, valid_acc: 0.956140350877193\n",
      ">>>>>>>>> 모델 저장: 23, 0.1920182853937149에서 0.18558606505393982로 loss가 개선되어 저장함.\n",
      "[0024/1000] train loss: 0.17191371321678162, valid loss: 0.17955707013607025, valid_acc: 0.956140350877193\n",
      ">>>>>>>>> 모델 저장: 24, 0.18558606505393982에서 0.17955707013607025로 loss가 개선되어 저장함.\n",
      "[0025/1000] train loss: 0.16752475500106812, valid loss: 0.17389951646327972, valid_acc: 0.956140350877193\n",
      ">>>>>>>>> 모델 저장: 25, 0.17955707013607025에서 0.17389951646327972로 loss가 개선되어 저장함.\n",
      "[0026/1000] train loss: 0.16259053349494934, valid loss: 0.1686355471611023, valid_acc: 0.956140350877193\n",
      ">>>>>>>>> 모델 저장: 26, 0.17389951646327972에서 0.1686355471611023로 loss가 개선되어 저장함.\n",
      "[0027/1000] train loss: 0.1561434045433998, valid loss: 0.1637481451034546, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 27, 0.1686355471611023에서 0.1637481451034546로 loss가 개선되어 저장함.\n",
      "[0028/1000] train loss: 0.14395543932914734, valid loss: 0.15927860140800476, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 28, 0.1637481451034546에서 0.15927860140800476로 loss가 개선되어 저장함.\n",
      "[0029/1000] train loss: 0.1403895616531372, valid loss: 0.15511660277843475, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 29, 0.15927860140800476에서 0.15511660277843475로 loss가 개선되어 저장함.\n",
      "[0030/1000] train loss: 0.1365927755832672, valid loss: 0.15129035711288452, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 30, 0.15511660277843475에서 0.15129035711288452로 loss가 개선되어 저장함.\n",
      "[0031/1000] train loss: 0.13068966194987297, valid loss: 0.14776350557804108, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 31, 0.15129035711288452에서 0.14776350557804108로 loss가 개선되어 저장함.\n",
      "[0032/1000] train loss: 0.13412661105394363, valid loss: 0.14452247321605682, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 32, 0.14776350557804108에서 0.14452247321605682로 loss가 개선되어 저장함.\n",
      "[0033/1000] train loss: 0.12920934706926346, valid loss: 0.14151304960250854, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 33, 0.14452247321605682에서 0.14151304960250854로 loss가 개선되어 저장함.\n",
      "[0034/1000] train loss: 0.11837730556726456, valid loss: 0.1387002021074295, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 34, 0.14151304960250854에서 0.1387002021074295로 loss가 개선되어 저장함.\n",
      "[0035/1000] train loss: 0.11998270079493523, valid loss: 0.1361149251461029, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 35, 0.1387002021074295에서 0.1361149251461029로 loss가 개선되어 저장함.\n",
      "[0036/1000] train loss: 0.11632771790027618, valid loss: 0.1337077021598816, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 36, 0.1361149251461029에서 0.1337077021598816로 loss가 개선되어 저장함.\n",
      "[0037/1000] train loss: 0.12252449244260788, valid loss: 0.13145531713962555, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 37, 0.1337077021598816에서 0.13145531713962555로 loss가 개선되어 저장함.\n",
      "[0038/1000] train loss: 0.1119219958782196, valid loss: 0.12938374280929565, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 38, 0.13145531713962555에서 0.12938374280929565로 loss가 개선되어 저장함.\n",
      "[0039/1000] train loss: 0.11102734133601189, valid loss: 0.127432718873024, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 39, 0.12938374280929565에서 0.127432718873024로 loss가 개선되어 저장함.\n",
      "[0040/1000] train loss: 0.10404029861092567, valid loss: 0.12556830048561096, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 40, 0.127432718873024에서 0.12556830048561096로 loss가 개선되어 저장함.\n",
      "[0041/1000] train loss: 0.10724161937832832, valid loss: 0.12382076680660248, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 41, 0.12556830048561096에서 0.12382076680660248로 loss가 개선되어 저장함.\n",
      "[0042/1000] train loss: 0.10062618553638458, valid loss: 0.12218823283910751, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 42, 0.12382076680660248에서 0.12218823283910751로 loss가 개선되어 저장함.\n",
      "[0043/1000] train loss: 0.10291316360235214, valid loss: 0.12068210542201996, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 43, 0.12218823283910751에서 0.12068210542201996로 loss가 개선되어 저장함.\n",
      "[0044/1000] train loss: 0.09871198982000351, valid loss: 0.11928088217973709, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 44, 0.12068210542201996에서 0.11928088217973709로 loss가 개선되어 저장함.\n",
      "[0045/1000] train loss: 0.10159950703382492, valid loss: 0.11793366074562073, valid_acc: 0.956140350877193\n",
      ">>>>>>>>> 모델 저장: 45, 0.11928088217973709에서 0.11793366074562073로 loss가 개선되어 저장함.\n",
      "[0046/1000] train loss: 0.09179164469242096, valid loss: 0.11675091087818146, valid_acc: 0.9736842105263158\n",
      ">>>>>>>>> 모델 저장: 46, 0.11793366074562073에서 0.11675091087818146로 loss가 개선되어 저장함.\n",
      "[0047/1000] train loss: 0.09412818774580956, valid loss: 0.11565817147493362, valid_acc: 0.9736842105263158\n",
      ">>>>>>>>> 모델 저장: 47, 0.11675091087818146에서 0.11565817147493362로 loss가 개선되어 저장함.\n",
      "[0048/1000] train loss: 0.08768847584724426, valid loss: 0.11466057598590851, valid_acc: 0.9736842105263158\n",
      ">>>>>>>>> 모델 저장: 48, 0.11565817147493362에서 0.11466057598590851로 loss가 개선되어 저장함.\n",
      "[0049/1000] train loss: 0.08699672669172287, valid loss: 0.11375930160284042, valid_acc: 0.9736842105263158\n",
      ">>>>>>>>> 모델 저장: 49, 0.11466057598590851에서 0.11375930160284042로 loss가 개선되어 저장함.\n",
      "[0050/1000] train loss: 0.08593140169978142, valid loss: 0.1129424124956131, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 50, 0.11375930160284042에서 0.1129424124956131로 loss가 개선되어 저장함.\n",
      "[0051/1000] train loss: 0.08413403481245041, valid loss: 0.11216717213392258, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 51, 0.1129424124956131에서 0.11216717213392258로 loss가 개선되어 저장함.\n",
      "[0052/1000] train loss: 0.08616790175437927, valid loss: 0.11149376630783081, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 52, 0.11216717213392258에서 0.11149376630783081로 loss가 개선되어 저장함.\n",
      "[0053/1000] train loss: 0.0827358141541481, valid loss: 0.11090528964996338, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 53, 0.11149376630783081에서 0.11090528964996338로 loss가 개선되어 저장함.\n",
      "[0054/1000] train loss: 0.0833943672478199, valid loss: 0.11034522950649261, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 54, 0.11090528964996338에서 0.11034522950649261로 loss가 개선되어 저장함.\n",
      "[0055/1000] train loss: 0.08014731854200363, valid loss: 0.1098380908370018, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 55, 0.11034522950649261에서 0.1098380908370018로 loss가 개선되어 저장함.\n",
      "[0056/1000] train loss: 0.07726754620671272, valid loss: 0.10940689593553543, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 56, 0.1098380908370018에서 0.10940689593553543로 loss가 개선되어 저장함.\n",
      "[0057/1000] train loss: 0.0690467432141304, valid loss: 0.10901007056236267, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 57, 0.10940689593553543에서 0.10901007056236267로 loss가 개선되어 저장함.\n",
      "[0058/1000] train loss: 0.06814772635698318, valid loss: 0.1086595430970192, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 58, 0.10901007056236267에서 0.1086595430970192로 loss가 개선되어 저장함.\n",
      "[0059/1000] train loss: 0.07506240904331207, valid loss: 0.10832454264163971, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 59, 0.1086595430970192에서 0.10832454264163971로 loss가 개선되어 저장함.\n",
      "[0060/1000] train loss: 0.07262068055570126, valid loss: 0.10807813704013824, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 60, 0.10832454264163971에서 0.10807813704013824로 loss가 개선되어 저장함.\n",
      "[0061/1000] train loss: 0.06249615363776684, valid loss: 0.10790135711431503, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 61, 0.10807813704013824에서 0.10790135711431503로 loss가 개선되어 저장함.\n",
      "[0062/1000] train loss: 0.07065543904900551, valid loss: 0.10774395614862442, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 62, 0.10790135711431503에서 0.10774395614862442로 loss가 개선되어 저장함.\n",
      "[0063/1000] train loss: 0.0702265053987503, valid loss: 0.10766522586345673, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 63, 0.10774395614862442에서 0.10766522586345673로 loss가 개선되어 저장함.\n",
      "[0064/1000] train loss: 0.06745239906013012, valid loss: 0.10763297975063324, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 64, 0.10766522586345673에서 0.10763297975063324로 loss가 개선되어 저장함.\n",
      "[0065/1000] train loss: 0.06571068242192268, valid loss: 0.1076110303401947, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 65, 0.10763297975063324에서 0.1076110303401947로 loss가 개선되어 저장함.\n",
      "[0066/1000] train loss: 0.064858578145504, valid loss: 0.10753611475229263, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 66, 0.1076110303401947에서 0.10753611475229263로 loss가 개선되어 저장함.\n",
      "[0067/1000] train loss: 0.06777732446789742, valid loss: 0.10749223083257675, valid_acc: 0.9649122807017544\n",
      ">>>>>>>>> 모델 저장: 67, 0.10753611475229263에서 0.10749223083257675로 loss가 개선되어 저장함.\n",
      "[0068/1000] train loss: 0.06744462624192238, valid loss: 0.10749933123588562, valid_acc: 0.9649122807017544\n",
      "[0069/1000] train loss: 0.055224135518074036, valid loss: 0.10751568526029587, valid_acc: 0.9649122807017544\n",
      "[0070/1000] train loss: 0.06497648358345032, valid loss: 0.10751938074827194, valid_acc: 0.9649122807017544\n",
      "[0071/1000] train loss: 0.06478426046669483, valid loss: 0.10754591226577759, valid_acc: 0.9649122807017544\n",
      "[0072/1000] train loss: 0.0632910504937172, valid loss: 0.10766109079122543, valid_acc: 0.9649122807017544\n",
      "[0073/1000] train loss: 0.05452859215438366, valid loss: 0.10777361690998077, valid_acc: 0.9649122807017544\n",
      "[0074/1000] train loss: 0.058014895766973495, valid loss: 0.10787332057952881, valid_acc: 0.9649122807017544\n",
      "[0075/1000] train loss: 0.05892382934689522, valid loss: 0.10798919200897217, valid_acc: 0.956140350877193\n",
      "[0076/1000] train loss: 0.057804957032203674, valid loss: 0.10812629759311676, valid_acc: 0.956140350877193\n",
      "[0077/1000] train loss: 0.059853559359908104, valid loss: 0.10829184949398041, valid_acc: 0.956140350877193\n",
      "========77에서 조기종료함. 0.10749223083257675에서 개선이 안됨========\n",
      "걸린시간(초):  0.4577219486236572\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "epochs = 1000\n",
    "\n",
    "#### 모델 학습(train) 로직 작성\n",
    "#### 검증 결과 -> train_loss, valid_loss, valid_accuracy\n",
    "#### 모델 성능이 개선될 떄 마다 저장.\n",
    "#### 조기종료 - 10 epoch 동안 성능 개선이 없으면 조기종료\n",
    "best_score = torch.inf \n",
    "patience = 10\n",
    "trigger_cnt = 0\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_acc_list = []\n",
    "\n",
    "s = time.time()\n",
    "for epoch in range(epochs):\n",
    "    ######## train ########\n",
    "    b_model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_train, y_train in train_loader:\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        pred = b_model(X_train)  # 양성(positive)일 확률\n",
    "        loss = loss_fn(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = train_loss + loss.item()\n",
    "    train_loss = train_loss/len(train_loader)  # 반복문 돈 횟수만큼 나눠줌.\n",
    "    # train_losses.append(train_loss)\n",
    "\n",
    "    ######## 검증 #########\n",
    "    b_model.eval()\n",
    "    valid_loss=0.0\n",
    "    valid_acc=0.0\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            pred_test = b_model(X_test)  # 양성일 확률\n",
    "            # 검증(loss, accuracy)\n",
    "            # loss\n",
    "            valid_loss = valid_loss + loss_fn(pred_test, y_test).item()\n",
    "            # 이진분류에서 정확도\n",
    "            valid_acc += torch.sum((pred_test > 0.5).type(torch.int32) == y_test).item()\n",
    "\n",
    "        # 검증결과 계산(평균)\n",
    "        valid_loss = valid_loss / len(test_loader)\n",
    "        valid_acc = valid_acc / len(test_loader.dataset)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        print_log = \"[{:04d}/{}] train loss: {}, valid loss: {}, valid_acc: {}\"\n",
    "        print(print_log.format(epoch+1, epochs, train_loss, valid_loss, valid_acc))\n",
    "    ################### 검증 완료 -> 모델 저장 ##################\n",
    "    if valid_loss < best_score:\n",
    "        torch.save(b_model, save_path)\n",
    "        # log 출력\n",
    "        save_log = \">>>>>>>>> 모델 저장: {}, {}에서 {}로 loss가 개선되어 저장함.\"\n",
    "        print(save_log.format(epoch+1, best_score, valid_loss))\n",
    "        best_score = valid_loss\n",
    "\n",
    "        trigger_cnt = 0\n",
    "    else:\n",
    "        trigger_cnt += 1\n",
    "        if patience == trigger_cnt:\n",
    "            print(f\"========{epoch+1}에서 조기종료함. {best_score}에서 개선이 안됨========\")\n",
    "            break\n",
    "\n",
    "e = time.time()\n",
    "print(\"걸린시간(초): \", e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 유형별 구현 정리\n",
    "\n",
    "## 공통\n",
    "\n",
    "-   Input layer(첫번째 Layer)의 in_features\n",
    "    -   입력데이터의 feature(속성) 개수에 맞춰준다.\n",
    "-   Hidden layer 수\n",
    "    -   경험적(art)으로 정한다.\n",
    "    -   Hidden layer에 Linear를 사용하는 경우 보통 feature 수를 줄여 나간다. (핵심특성들을 추출해나가는 과정의 개념.)\n",
    "\n",
    "## 회귀 모델\n",
    "\n",
    "-   output layer의 출력 unit개수(out_features)\n",
    "    -   정답의 개수\n",
    "    -   ex\n",
    "        -   집값: 1\n",
    "        -   아파트가격, 단독가격, 빌라가격: 3 => y의 개수에 맞춘다.\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   일반적으로 **None**\n",
    "    -   값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을 경우\n",
    "        -   ex) 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)\n",
    "-   loss함수\n",
    "    -   MSELoss\n",
    "-   평가지표\n",
    "    -   MSE, RMSE, R square($R^2$)\n",
    "\n",
    "## 다중분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   정답 class(고유값)의 개수\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Softmax: 클래스별 확률을 출력\n",
    "-   loss함수\n",
    "    -   **categrocial crossentropy**\n",
    "    -   파이토치 함수\n",
    "        -   **CrossEntropyLoss** = NLLLoss(정답) + LogSoftmax(모델 예측값)\n",
    "        -   **NLLLoss**\n",
    "            -   정답을 OneHot Encoding 처리 후 Loss를 계산한다.\n",
    "            -   입력으로 LogSoftmax 처리한 모델 예측값과 onehot encoding 안 된 정답을 받는다.\n",
    "        -   **LogSoftmax**\n",
    "            -   입력값에 Softmax 계산후 그 Log를 계산한다.\n",
    "                -   NLLLoss의 모델 예측값 입력값으로 처리할 때 사용한다.\n",
    "\n",
    "```python\n",
    "pred = model(input)\n",
    "loss1 = nn.NLLLoss(nn.LogSoftmax(dim=-1)(pred), y)\n",
    "# or\n",
    "loss2 = nn.CrossEntropyLoss()(pred, y)\n",
    "```\n",
    "\n",
    "## 이진분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   1개 (positive일 확률)\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Sigmoid(Logistic)\n",
    "-   loss 함수\n",
    "    -   **Binary crossentropy**\n",
    "    -   파이토치 함수: **BCELoss**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
