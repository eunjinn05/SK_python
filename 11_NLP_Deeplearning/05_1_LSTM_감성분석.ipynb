{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93c9500-6e1f-445a-a5e1-95e687853ae8",
   "metadata": {},
   "source": [
    "# Pytorch의 nn.Embedding\n",
    "- Pytorch의 Embedding Layer는 word2vec과 마찬가지로 word embedding vector를 찾는 **Lookup Table**이다.\n",
    "    - 단어의 **정수의 고유 index**가 입력으로 들어오면 Embedding Layer의 **그 index의 Vector**를 출력한다.\n",
    "    - 모델이 학습되는 동안 모델이 풀려는 문제에 맞는 값으로 Embedding Layer의 vector들이 업데이트 된다.\n",
    "    - Word2Vec의 embedding vector 학습을 nn.Embedding은 자신이 포함된 모델을 학습 하는 과정에서 한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfee4328-7019-4f85-bb71-d7f172024ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d936d8f7-a203-4452-978f-e2da81b7dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10, 5, padding_idx=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 4.1567e-01, -4.1612e-01,  1.4438e+00, -4.3451e-01,  4.4722e-01],\n",
       "        [ 1.2987e+00,  2.2073e+00,  1.2578e+00, -1.0437e+00,  7.5841e-01],\n",
       "        [ 7.8317e-01, -1.6177e+00, -1.8964e+00,  9.7235e-02,  8.5216e-01],\n",
       "        [ 1.1935e+00,  1.4814e-01, -6.6420e-01, -1.7455e-01,  1.4450e+00],\n",
       "        [-6.3060e-01, -4.3839e-01, -1.0186e+00,  4.8676e-01,  6.0084e-01],\n",
       "        [ 5.0924e-01,  1.7963e+00,  2.0019e-01,  7.1484e-01, -1.8524e-03],\n",
       "        [-6.8214e-01,  1.6606e+00,  1.4089e-01,  9.6041e-01, -1.3932e+00],\n",
       "        [ 2.9771e-01,  3.9424e-01, -6.8769e-01,  8.0933e-01,  2.5316e-01],\n",
       "        [ 2.0457e-01,  6.9192e-01, -1.3371e+00,  4.8452e-01,  1.2761e+00]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_layer = nn.Embedding(\n",
    "    num_embeddings=10,   #vocab size (총 단어 갯수)\n",
    "    embedding_dim=5,  #embedding vector의 차원\n",
    "    padding_idx=0,  #padding 토큰의 index를 지정 (0인경우 pad는 자리만 채운 토큰이므로 학습하지않게함)  [PAD] 토큰 : 문장들의 토큰 갯수를 맞추기 위해서 사용하는 토큰, 모든 문장의 토큰수를 10개로 할 경우 10개가 안되는 토큰은 [PAD] 토큰으로 채움\n",
    ")\n",
    "\n",
    "print(e_layer)  # [10:단어수, 5:embedding차원]\n",
    "e_layer.weight # word embedding vector들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d166e4e-4ced-4bc8-a188-12358670ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "#입력 값 - 정수 tensor(LongTensor-int64)를 입력\n",
    "## 한개 문서 : [1, 10, 7, 5] -> 토큰 index 문서 구성하는 토큰 idx들을 1차원으로 묶어서 전달\n",
    "\n",
    "input_data = torch.tensor([[1, 3, 2, 7]], dtype=torch.int64)\n",
    "e = e_layer(input_data)\n",
    "print(e.shape) # [1, 4, 5] -> 문서수, 토큰수, embedding vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59e731-381d-4a79-83c4-1fc20ba006e1",
   "metadata": {},
   "source": [
    "# 네이버 영화 댓글 감성분석(Sentiment Analysis)\n",
    "\n",
    "## 감성분석(Sentiment Analysis) 이란\n",
    "입력된 텍스트가 **긍적적인 글**인지 **부정적인**인지 또는 **중립적인** 글인지 분석하는 것을 감성(감정) 분석이라고 한다.   \n",
    "이를 통해 기업이 고객이 자신들의 기업 또는 제품에 대해 어떤 의견을 가지고 있는지 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7034ada-08b9-4163-b18d-ce429aef275b",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader 생성\n",
    "\n",
    "## Korpora에서 Naver 영화 댓글 dataset 가져오기\n",
    "- https://ko-nlp.github.io/Korpora/ko-docs/corpuslist/nsmc.html\n",
    "- http://github.com/e9t/nsmc/\n",
    "    - input: 영화댓글\n",
    "    - output: 0(부정적댓글), 1(긍정적댓글)\n",
    "### API\n",
    "- **corpus 가져오기**\n",
    "    - `Korpora.load('nsmc')`\n",
    "- **text/label 조회**\n",
    "    - `corpus.get_all_texts()` : 전체 corpus의 text들을 tuple로 반환\n",
    "    - `corpus.get_all_labels()`: 전체 corpus의 label들을 list로 반환\n",
    "- **train/test set 나눠서 조회**\n",
    "    - `corpus.train`\n",
    "    - `corpus.test`\n",
    "    - `LabeledSentenceKorpusData` 객체에 text와 label들을 담아서 제공.\n",
    "        - `LabeledSentenceKorpusData.texts`: text들 tuple로 반환.\n",
    "        - `LabeledSentenceKorpusData.labels`: label들 list로 반환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e2ea3-6123-4ebd-8e98-27b1db6406ed",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0534360-f0ee-48ce-bb46-24c004d63272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seong-eunjin/Korpora/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /Users/seong-eunjin/Korpora/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus = Korpora.load('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232a23c0-06c5-49b7-b601-1ede1198b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다')\n",
      "[0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "all_input = corpus.get_all_texts()  # 댓글들 전체\n",
    "all_labels = corpus.get_all_labels() # output 0 부정 / 1 긍정\n",
    "\n",
    "print(all_input[:5])\n",
    "print(all_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eed8aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.train: size=150000\n",
       "  - NSMC.train.texts : list[str]\n",
       "  - NSMC.train.labels : list[int]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a51fcbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NSMC.test: size=50000\n",
       "  - NSMC.test.texts : list[str]\n",
       "  - NSMC.test.labels : list[int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34757ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e7e54a-548d-4bf3-81aa-357ab249f41a",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "1. 형태소 단위 token화(분절)를 먼저 한다.\n",
    "    - konlpy로 token화 한 뒤 다시 한 문장으로 만든다.\n",
    "2. 1에서 처리한 corpus를 BPE 로 token화\n",
    "   \n",
    "### 전처리 함수\n",
    "\n",
    "#### 형태소 단위 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0bbb39b-9f49-4d29-a969-4839c01f430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Mecab\n",
    "import string\n",
    "import re\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    1. 영문 -> 소문자로 변환\n",
    "    2. 구두점 제거\n",
    "    3. 형태소 기반 토큰화\n",
    "    4. 형태소로 토큰화 한 뒤 다시 하나의 문자열로 묶어서 반환.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "    tokens = Okt().morphs(text, stem=True) # stem 원형복원\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35f45e2f-f76a-4011-b963-d7feb214cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 더빙 진짜 짜증나다 목소리\n"
     ]
    }
   ],
   "source": [
    "print(text_preprocessing('아 더빙.. 진짜 짜증나네요 목소리'))\n",
    "\n",
    "train_inputs = corpus.train.texts\n",
    "train_texts = [text_preprocessing(txt) for txt in train_inputs]\n",
    "train_labels = corpus.train.labels\n",
    "\n",
    "test_input = corpus.test.texts\n",
    "test_texts = [text_preprocessing(txt) for txt in test_input]\n",
    "test_labels = corpus.test.labels\n",
    "\n",
    "# 전처리 된 input 합치기\n",
    "all_texts = train_texts + test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14b00812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "os.makedirs('datasets/nsmc', exist_ok=True)\n",
    "with open('datasets/nsmc/train.pkl', 'wb') as fw :\n",
    "    pickle.dump({'input':train_texts, 'output':train_labels}, fw)\n",
    "\n",
    "with open('datasets/nsmc/testset.pkl', 'wb') as fw2 :\n",
    "    pickle.dump({'input':test_texts, 'output':test_labels}, fw2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e519a68-d3a0-4481-bcf7-b121d8ba813f",
   "metadata": {},
   "source": [
    "### 토큰화\n",
    "- Subword 방식 토큰화 적용\n",
    "- Byte Pair Encoding 방식으로 huggingface tokenizer 사용\n",
    "    - BPE: 토큰을 글자 단위로 나눈뒤 가장 자주 등장하는 글자 쌍(byte paire)를 찾아 합친뒤 어휘사전에 추가한다.\n",
    "    - https://huggingface.co/docs/tokenizers/quicktour\n",
    "    - `pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c46fb56-1bff-45f8-ba76-30276c89286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    BPE(unk_token='[UNK]')\n",
    ")\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    special_tokens=['[PAD]', '[UNK]'],\n",
    "    continuing_subword_prefix='##'\n",
    ")\n",
    "\n",
    "#학습\n",
    "tokenizer.train_from_iterator(all_texts, trainer=trainer) #학습코드가 메모리에 있을때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f162bdf-fac9-4468-a264-c656e4b3164d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26738"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de9b29e1-384a-44e8-ab19-e53f0c1303c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]의 id : 0\n",
      "아\n"
     ]
    }
   ],
   "source": [
    "print(f'[PAD]의 id : {tokenizer.token_to_id('[PAD]')}')\n",
    "print(tokenizer.id_to_token(1986))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e998d38b-e762-4fd5-b37f-8f8a2b6f5849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1986, 5881, 5426, 5667, 6087]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "all_texts[1023]\n",
    "r = tokenizer.encode(all_texts[0])\n",
    "r.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "334a1233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아 더빙 진짜 짜증나다 목소리'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1986, 5881, 5426, 5667, 6087])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9871bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('saved_model/nsmc', exist_ok=True)\n",
    "tokenizer.save('saved_model/nsmc/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f81bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아', '더빙', '진짜', '짜증나다', '목소리']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_tokenizer = Tokenizer.from_file('saved_model/nsmc/tokenizer.json')\n",
    "load_tokenizer.encode(all_texts[0]).tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5c31d-633c-4a31-8f66-dff2ecf8e86a",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff27280e-dfb7-4947-9192-777e6984286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        texts: list - 댓글 목록. 리스트에 댓글들을 담아서 받는다. [\"댓글\", \"댓글\", ...]\n",
    "        labels: list - 댓글 감정 목록. \n",
    "        max_length: 개별 댓글의 token 개수. 모든 댓글의 토큰수를 max_length에 맞춘다.\n",
    "        tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = [self.__pad_token_sequences(tokenizer.encode(text).ids) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    ###########################################################################################\n",
    "    # id로 구성된 개별 문장 tokenizer list를 받아서 패딩 추가 [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "    ############################################################################################\n",
    "    def __pad_token_sequences(self, token_sequences):\n",
    "        \"\"\"\n",
    "        id로 구성된 개별 문서(댓글)의 token_id list를 받아서 max_length 길이에 맞추는 메소드\n",
    "        max_length 보다 토큰수가 적으면 [PAD] 추가, 많으면 max_length 크기로 줄인다.\n",
    "            ex) [20, 2, 1] => [20, 2, 1, 0, 0, 0, ..]\n",
    "        \"\"\"\n",
    "        pad_token = self.tokenizer.token_to_id('[PAD]')\n",
    "        seq_len = len(token_sequences) # 문장의 토큰 갯수\n",
    "        result = None\n",
    "        if seq_len > self.max_length :\n",
    "            result = token_sequences[:self.max_length]\n",
    "        else :\n",
    "            result = token_sequences + ([pad_token] * (self.max_length - seq_len))\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        idx 번째 text와 label을 학습 가능한 type으로 변환해서 반환\n",
    "        Parameter\n",
    "            idx: int 조회할 index\n",
    "        Return\n",
    "            tuple: (torch.LongTensor, torch.FloatTensor) - 댓글 토큰_id 리스트, 정답 Label\n",
    "        \"\"\"\n",
    "        txt = self.texts[idx] # idx번째 댓글\n",
    "        label = self.labels[idx] #idx번째 정답\n",
    "        # (input, output) : input : embedding layer에 입력으로 들어감 -> LongTensor\n",
    "        # output : [label] loss함수에 입력할 때 (batch, 1)\n",
    "        return (torch.tensor(txt, dtype=torch.int64), torch.tensor([label], dtype=torch.float32))\n",
    "\n",
    "# BCELoss() : 정답 shape (batch,1)\n",
    "# CrossEntropyLoss() : 정답 shape (batch,)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9515bb6f-703d-4277-b645-8869267f5297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 11, 10, 9, 22]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [ len(tokenizer.encode(text)) for text in all_texts]  # 모든 문장들의 토큰 갯수 리스트\n",
    "a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9eb1c4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 89)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(a), max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "161f63e1-4ba8-4ccc-8c62-422b1f30b431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  540, 11354,   506,  2408,  5414,  5426,  2408,  5414,   119,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dataset 생성\n",
    "\n",
    "import numpy as np\n",
    "np.quantile(a, q=0.9)\n",
    "\n",
    "# max_token 수를 30개로 세팅\n",
    "MAX_TOKEN = 30\n",
    "train_set = NSMCDataset(train_texts, train_labels, MAX_TOKEN, tokenizer)\n",
    "test_set = NSMCDataset(test_texts, test_labels, MAX_TOKEN, tokenizer)\n",
    "\n",
    "train_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee963d58-0008-4fa6-b426-e3e332591f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eea5f00e-70fa-475b-8e2d-d06d120e3bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2343, 782)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5f038-b32c-4e4e-82c8-956c7cbe0c4d",
   "metadata": {},
   "source": [
    "# 모델링\n",
    "- Embedding Layer를 이용해 Word Embedding Vector를 추출한다.\n",
    "- LSTM을 이용해 Feature 추출\n",
    "- Linear + Sigmoid로 댓글 긍정일 확률 출력\n",
    "  \n",
    "![outline](figures/rnn/RNN_outline.png)\n",
    "\n",
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f647d0c2-829a-41f6-a922-ee33b2450f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88afd6-5c8f-4ade-b930-86c9425e86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverCommentClassifier(nn.Module) :\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, bidirectional=True, droupout_rate=0.2) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = vocab_size, # vocab 총 단어 수\n",
    "            embedding_dim=embedding_dim, #embedding vector의 차원 수\n",
    "            padding_idx = 0  #[PAD]의 index padding에 대한 weight는 학습하지않는다\n",
    "        )\n",
    "\n",
    "        # embedding layer의 출력 shape (64:batch, seq_len:문장의 토큰수, embedding vector 차원수) -> (64, 30, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=droupout_rate\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(droupout_rate)\n",
    "        # 입력 : lstm의 마지막 timestep의 output\n",
    "        if bidirectional == True :\n",
    "            i_features = hidden_size * 2\n",
    "        else :\n",
    "            i_features = hidden_size\n",
    "        self.classifier = nn.Linear(i_features, 1) # out_features : 1 - 긍정일 확률 \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X) :\n",
    "        embedding_vector = self.embedding(X)\n",
    "        # embedding_vector : [batch_size, seq_len, embedding_dim] -> [seq_len, batch_size, embedding]\n",
    "        embedding_vector = embedding_vector.transpose(1, 0)  # batch축과 seq_len 축 바꾸기\n",
    "        output, _ = self.lstm(embedding_vector)  # output = [seq_len, batch_size, hidden_size] -> 마지막 output 추출\n",
    "        output = output[-1]\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        last_output = self.sigmoid(output)\n",
    "        return last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e9cbbd8-d8b4-4a51-ac7a-5765722f139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((3, 2, 5))\n",
    "print(a.shape)\n",
    "b = a.transpose(1,0)  # 위치 바꾸기\n",
    "b.shape\n",
    "c = a.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a0171-371e-4cb5-9bd5-ad1e3c14480d",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7830d2b5-d5ed-4b53-a442-bebc83077aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaverCommentClassifier(\n",
    "     vocab_size=tokenizer.get_vocab_size(),\n",
    "     embedding_dim=100,\n",
    "     hidden_size=32,\n",
    "     num_layers=2,\n",
    "     bidirectional=True,\n",
    "     droupout_rate=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae04a25a-c6dc-4833-883b-54972a16c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5323],\n",
      "        [0.5309],\n",
      "        [0.5148],\n",
      "        [0.5180],\n",
      "        [0.5177],\n",
      "        [0.5227],\n",
      "        [0.5275],\n",
      "        [0.5209],\n",
      "        [0.5332],\n",
      "        [0.5259],\n",
      "        [0.5181],\n",
      "        [0.5197],\n",
      "        [0.5209],\n",
      "        [0.5209],\n",
      "        [0.5152],\n",
      "        [0.5213],\n",
      "        [0.5161],\n",
      "        [0.5211],\n",
      "        [0.5202],\n",
      "        [0.5262],\n",
      "        [0.5268],\n",
      "        [0.5276],\n",
      "        [0.5241],\n",
      "        [0.5188],\n",
      "        [0.5280],\n",
      "        [0.5250],\n",
      "        [0.5199],\n",
      "        [0.5212],\n",
      "        [0.5257],\n",
      "        [0.5373],\n",
      "        [0.5329],\n",
      "        [0.5275],\n",
      "        [0.5273],\n",
      "        [0.5227],\n",
      "        [0.5214],\n",
      "        [0.5261],\n",
      "        [0.5164],\n",
      "        [0.5309],\n",
      "        [0.5232],\n",
      "        [0.5203],\n",
      "        [0.5236],\n",
      "        [0.5174],\n",
      "        [0.5335],\n",
      "        [0.5322],\n",
      "        [0.5241],\n",
      "        [0.5274],\n",
      "        [0.5116],\n",
      "        [0.5244],\n",
      "        [0.5414],\n",
      "        [0.5161],\n",
      "        [0.5199],\n",
      "        [0.5245],\n",
      "        [0.5116],\n",
      "        [0.5223],\n",
      "        [0.5201],\n",
      "        [0.5116],\n",
      "        [0.5280],\n",
      "        [0.5184],\n",
      "        [0.5258],\n",
      "        [0.5233],\n",
      "        [0.5133],\n",
      "        [0.5193],\n",
      "        [0.5380],\n",
      "        [0.5258]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 전에 추론\n",
    "X, y = next(iter(train_loader))\n",
    "y_hat = model(X)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe5b3d-8b2f-4164-9b97-29e6aadced5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d229e-9a99-4a28-9dfd-9582686ba052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdd5885-8150-4529-ad3a-84931a8824c5",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a1bf6-d8eb-42d0-996e-f975e93888af",
   "metadata": {},
   "source": [
    "### Train/Test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46099bec-eee3-4cef-921b-ce9ee6cf0f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer, device='cpu'):\n",
    "    # 1 epoch 학습\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for X, y in dataloader :\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 추론\n",
    "        pred = model(X)\n",
    "        # loss 계산\n",
    "        loss = loss_fn(pred, y)\n",
    "        # gradient 계산\n",
    "        loss.backward()\n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # 파라미터 초기화\n",
    "        optimizer.zero_grad()\n",
    "        loss_list.append(loss.item())\n",
    "    return sum(loss_list)/len(dataloader)  # 1 에폭 train loss: step loss들 평균을 반환.\n",
    "    \n",
    "def test(model, dataloader, loss_fn, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    acc_list = 0\n",
    "    with torch.no_grad() :\n",
    "        for X, y in dataloader :\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_proba = model(X) #  양성일 확률\n",
    "            pred_label = (pred_proba > 0.5).type(torch.int32) # bool -> int (False: 0, True: 1)\n",
    "            loss = loss_fn(pred_proba, y)\n",
    "            loss_list.append(loss.item())\n",
    "            acc_list += (y == pred_label).sum().item()\n",
    "        return sum(loss_list), acc_list/len(dataloader.dataset) # 검증 loss, accuracy \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de42da4-8991-4bcb-9ce9-18155459dec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c8853d0-b137-47bb-8f0d-fc4f05700cf2",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd806dda-5058-4c44-a3f4-28cadc8a90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 1\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "816e18c2-17bf-4621-b630-b47e16c34bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.28239506408031606, val_loss = 297.4188789278269, val_acc = 0.83802\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs) :\n",
    "    #학습\n",
    "    train_loss = train(model, train_loader, loss_fn, optimizer, device)\n",
    "    val_loss, val_acc = test(model, test_loader, loss_fn, device)\n",
    "    print(f'train loss = {train_loss}, val_loss = {val_loss}, val_acc = {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32690441-482a-46b1-b91b-b85329d2141f",
   "metadata": {},
   "source": [
    "## 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80c16618-1517-4371-8e37-ae3cf03428b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'saved_model/nsmc_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d41e8-0715-4f50-aa37-11a8e142706a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3de7ed5-f7f6-4206-b16f-f8535a03405c",
   "metadata": {},
   "source": [
    "# 서비스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827bdaa3-008d-4a93-aee6-0877e829ef32",
   "metadata": {},
   "source": [
    "## 전처리 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1a2661a9-3964-4117-b273-e5d8bd4194b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # 한 문장(문서) 전처리\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]+\", ' ', text)\n",
    "    return ' '.join(okt.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "315603df-159b-4317-9fb4-7897546b7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_token_sequences(token_sequences, max_length):\n",
    "    \"\"\"padding 처리 메소드.\"\"\"\n",
    "    pad_token = tokenizer.token_to_id('[PAD]')  \n",
    "    seq_length = len(token_sequences)           \n",
    "    result = None\n",
    "    if seq_length > max_length:                 \n",
    "        result = token_sequences[:max_length]\n",
    "    else:                                            \n",
    "        result = token_sequences + ([pad_token] * (max_length - seq_length))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d73070a-0ee5-4f35-996c-b0d11ba08516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_preprocessing(text_list):\n",
    "    \"\"\"\n",
    "    모델에 입력할 수있는 input data를 생성\n",
    "    Parameter:\n",
    "        text_list: list - 추론할 댓글리스트\n",
    "    Return\n",
    "        torch.LongTensor - 댓글 token_id tensor\n",
    "    \"\"\"\n",
    "    text_list = [text_preprocessing(text) for text in text_list]\n",
    "    # 토큰화\n",
    "    tokens = [ tokenizer.encode(text).ids for text in text_list]\n",
    "    # 토큰 + 패딩\n",
    "    pad_tokens = [pad_token_sequences(token, MAX_TOKEN) for token in tokens]\n",
    "    return torch.tensor(pad_tokens, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e19997-6b61-446f-ac72-376cd34ee495",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a6fb00ab-60d0-45f2-bb16-505a5f5cc056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 30])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_list = [\"아 진짜 재미없다.\", \"여기 식당 먹을만 해요\", \"이걸 영화라고 만들었냐?\", \"기대 안하고 봐서 그런지 괜찮은데.\", \"이걸 영화라고 만들었나?\", \"아! 뭐야 진짜.\", \"재미있는데.\", \"연기 짱 좋아. 한번 더 볼 의향도 있다.\", \"뭐 그럭저럭\"]\n",
    "input_tensor = predict_data_preprocessing(comment_list)\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "499c330d-69ff-43fb-9ee9-ad1c6054f9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 진짜 재미없다. 부정적 댓글 0.9841747879981995\n",
      "여기 식당 먹을만 해요 긍정적 댓글 0.7784380316734314\n",
      "이걸 영화라고 만들었냐? 부정적 댓글 0.9288060069084167\n",
      "기대 안하고 봐서 그런지 괜찮은데. 긍정적 댓글 0.8463358283042908\n",
      "이걸 영화라고 만들었나? 부정적 댓글 0.9288060069084167\n",
      "아! 뭐야 진짜. 부정적 댓글 0.8867478966712952\n",
      "재미있는데. 긍정적 댓글 0.9645829200744629\n",
      "연기 짱 좋아. 한번 더 볼 의향도 있다. 긍정적 댓글 0.9739835858345032\n",
      "뭐 그럭저럭 부정적 댓글 0.9695420861244202\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad() :\n",
    "    pred = model(input_tensor)\n",
    "    for txt, prob in zip(comment_list, pred) :\n",
    "        label = '긍정적 댓글' if prob.item() > 0.5 else '부정적 댓글'\n",
    "        p = prob.item() if prob.item() > 0.5 else (1-prob).item()\n",
    "        print(txt, label, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5273ac02-81f3-4391-b802-f9dca1f8d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "킥 부정적 댓글 0.9816281795501709\n"
     ]
    }
   ],
   "source": [
    "comment_list = [input(\"댓글 :\")]\n",
    "input_tensor = predict_data_preprocessing(comment_list)\n",
    "# 추론\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad() :\n",
    "    pred = model(input_tensor)\n",
    "    for txt, prob in zip(comment_list, pred) :\n",
    "        label = '긍정적 댓글' if prob.item() > 0.5 else '부정적 댓글'\n",
    "        p = prob.item() if prob.item() > 0.5 else (1-prob).item()\n",
    "        print(txt, label, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0510a47-9189-4c2a-a6e9-33b04971f2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0cbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
