{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23870109-6d31-4db7-b782-b699fe671867",
   "metadata": {},
   "source": [
    "# GRU(Gated Recurrent Units) 모델\n",
    "- https://arxiv.org/pdf/1406.1078\n",
    "- LSTM의 RNN의 한계점인 기억력 소실문제를 해결하여 긴 sequence의 데이터에서도 좋은 성능을 내는 모델이다. 그러나 복잡한 구조로 parameter가 많아지게 되었고 연산량이 많은 문제점이 있다.\n",
    "    - parameter가 많아지면서 데이터양이 부족할 경우 과대적합이 발생하고 연산량이 많아 학습에 많은 시간이 걸리게 된다.\n",
    "- LSTM의 이런 문제를 개선하기 위한 변형 모델이 GRU이다.\n",
    "\n",
    "## LSTM과 차이\n",
    "1. LSTM은 forget gate, input gate, output gete 세개의 Gate연산을 함. GRU는 **reset gate와 update gate** 로 흐름을 제어한다.\n",
    "2. LSTM은 이전 처리결과로 Cell State, Hidden State 두개가 있었는데 이것을 하나로 합쳐 **Hidden State**로 출력한다.\n",
    "\n",
    "## GRU 성능\n",
    "- GRU는 적은 파라미터 수와 연산비용이 적게 드는 것에 비해 LSTM과 비슷한 성능을 내는 것으로 알려졌다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18729a02-f014-4082-826f-824b9d7adbf6",
   "metadata": {},
   "source": [
    "## GRU Cell 구조\n",
    "\n",
    "![gru_cell](figures/rnn/23_gru_cell.png)    \n",
    "[이미지 Source](https://www.oreilly.com/library/view/advanced-deep-learning/9781789956177/8ad9dc41-3237-483e-8f6b-7e5f653dc693.xhtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9740062-d77e-4b55-946d-4135af3c8e28",
   "metadata": {},
   "source": [
    "- **Reset Gate**\n",
    "    - 이전 timestep까지의 hidden state(feature)를 현재 timestep의 hidden state(feature) 계산시 얼마나 반영할 지 비율을 결정하는 gate.\n",
    "    - $r_{t} = \\sigma(h_{t-1}\\cdot U_{r} + X_{t}\\cdot W_{r})$\n",
    "        - $U_{r},\\, W_{r}$ 는 파라미터\n",
    "        - $\\sigma$: sigmoid(logisic) 함수\n",
    "- **Update Gate**\n",
    "    - 현재 timestep의 hidden state($h_t$)를 계산할 때 이전 time step까지 정보($h_{t-1}$)와 현재 time step의 정보($X_t$)를 각각 얼마나 반영할지 비율을 정의한다.\n",
    "    - $z_{t} = \\sigma(h_{t-1}\\cdot U_{z} + X_{t}\\cdot W_{z})$\n",
    "        - $U_{z},\\, W_{z}$ 는 파라미터\n",
    "        - $\\sigma$: sigmoid(logisic) 함수\n",
    "    - $h_t$를 계산할 때 $z_{t}$ 는 이전 정보인 $h_{t-1}$을 얼마나 반영할지 $1-z_{t}$는 현재 정보를 얼마나 반영할 지를 정한다.\n",
    "- **Cell의 출력값인 $h_t$ 계산**\n",
    "    - $z_{t}\\times h_{t-1} + tanh(h_{t-1} * r_{t}+X_{t}\\cdot W)\\times(1-z_{t})$\n",
    "    - 이전 정보에는 $z_t$를 곱해 얼마나 $h_t$ 에 더할지 연산\n",
    "    - 현재 정보($X_t$)에는 이전 정보를 일부를 반영한다. 이전 정보를 얼마나 반영할지를 reset gate 결과를 곱해 결정한다. 활성화 함수 tanh를 이용해 비선형성을 추가 한 결과에 $1-z_t$를 곱한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6f7c4-0cfb-4ac8-b7dd-834a44c18880",
   "metadata": {},
   "source": [
    "## Pytorch GRU\n",
    "- `nn.GRU` 클래스 이용\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "- **입력**\n",
    "    - **input**: (seq_length, batch, hidden_size) shape의 tensor. (batch_first=False), batch_first=True이면 `seq_length`와 `batch` 위치가 바뀐다.\n",
    "    - **hidden**: (D * num_layers, batch, hidden_size) shape의 Tensor. D(양방향:2, 단방향:1), hidden은 생략하면 0이 입력됨.\n",
    "- **출력** - output과 hidden state가 반환된다.\n",
    "    - **output**\n",
    "        - 모든 sequence의 처리결과들을 모아서 제공.\n",
    "        - shape: (seq_length, batch, D * hidden_size) : D(양방향:2, 단방향:1), batch_first=True이면 `seq_length`와 `batch` 위치가 바뀐다.\n",
    "    - **hidden**\n",
    "        - 마지막 time step 처리결과\n",
    "        - shape: (D * num_layers, batch, hidden) : D(양방향:2, 단방향:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7d86e6-0a72-4fde-9e57-51417b1ed782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 256])\n",
      "torch.Size([1, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# GRU 입출력 확인\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# dummy data\n",
    "input_data = torch.randn((20, 2, 10))  # (20 : seq, 2 : batch, 10 : 개별 timestep의 입력 feature수)\n",
    "\n",
    "gru1 = nn.GRU(\n",
    "    input_size=10,\n",
    "    hidden_size=256,\n",
    "    num_layers=1, # 몇 층 쌓을지\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "out1, hidden = gru1(input_data)\n",
    "\n",
    "print(out1.shape)  # 모든 timestep의 hidden state값 묶어서 반환  [20:seq_len, 2:batch, 256:hidden_size]\n",
    "print(hidden.shape)  # 마지막 timestep 처리 hidden state 값 [1:seq len, 2, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b0319-660e-42f0-a390-6ec71ade0b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 512])\n",
      "torch.Size([2, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "# bidirectional True\n",
    "\n",
    "gru2 = nn.GRU(\n",
    "    input_size=10,\n",
    "    hidden_size=256,\n",
    "    num_layers=1, # 몇 층 쌓을지\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "out2, hidden2 = gru2(input_data)\n",
    "\n",
    "print(out2.shape)  # 512 양방향 hidden state 합침\n",
    "print(hidden2.shape)  # 2 정방향/역방향 2개, 256 : 따로 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780b5bac-e226-40e3-9e8c-96e3cf9ad9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 256])\n",
      "torch.Size([4, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "#multi layer\n",
    "\n",
    "gru3 = nn.GRU(\n",
    "    input_size=10,\n",
    "    hidden_size=256,\n",
    "    num_layers=4, # 몇 층 쌓을지\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "out3, hidden3 = gru3(input_data)\n",
    "\n",
    "# [20, 2, 256] - 마지막 GRU Layer가 출력한 결과들이 최종 featrue이므로 그것을 모아서 반환 (layers가 몇개든 out의 shape 동일)\n",
    "print(out3.shape)\n",
    "# layer갯수만큼 출력함 (4) - 각 layer의 마지막 hidden state들을 모아서 반환\n",
    "print(hidden3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5c96a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2, 512])\n",
      "torch.Size([8, 2, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi layer, 양방향\n",
    "\n",
    "gru4 = nn.GRU(\n",
    "    input_size=10,\n",
    "    hidden_size=256,\n",
    "    num_layers=4, # 몇 층 쌓을지\n",
    "    bidirectional=True\n",
    ")\n",
    "\n",
    "out4, hidden4 = gru4(input_data)\n",
    "print(out4.shape), print(hidden4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389555c-17d0-41ed-abfc-42046bf8aeaa",
   "metadata": {},
   "source": [
    "# Encoder-Decoder 구조\n",
    "- 두개의 네트워크를 연결한 구조\n",
    "- Encoder network는 입력을 이해하고 Decoder network는 (Encoder의 이해를 바탕으로) 출력을 생성한다.\n",
    "\n",
    "## Seq2Seq\n",
    "- Encoder-Decoder 구조를 RNN 계열에 적용한 모델.\n",
    "- Encoder는 입력 Sequence의 전체 의미(특징)을 표현하는 **context vector**를 출력한다.\n",
    "    - **Context Vector는**\n",
    "        - 번역의 경우 번역할 대상 문장에서 **번역 결과를 만들때 필요한 feature들**을 가지고 있다.\n",
    "        - Chatbot의 경우 입력된 질문에서 **답변을 만들때 필요한 feature들**을 가지고 있다.\n",
    "- Decoder는 Encoder가 출력한 Context Vector를 입력받아 결과 sequence를 생성한다.\n",
    "    - **결과 sequence는**\n",
    "        - **번역**의 경우 번역 문장을 생성한다.\n",
    "        - **chatbot**의 경우 질문에 대한 답변을 생성한다.\n",
    "\n",
    "![seq2seq](figures/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887d108-2ac6-425e-9643-d351e44282c7",
   "metadata": {},
   "source": [
    "# Seq2Seq 를 이용한 Chatbot 모델 구현\n",
    "- Encoder를 이용해 질문의 특성을 추출하고 Decoder를 이용해 답변을 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec7aee-1d3b-4990-b934-a4254a6e17ef",
   "metadata": {},
   "source": [
    "# Chatbot Dataset\n",
    "\n",
    "- https://github.com/songys/Chatbot_data\n",
    "- columns\n",
    "    - Q: 질문\n",
    "    - A: 답\n",
    "    - label: 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2\n",
    "- **Download**\n",
    "\n",
    "![dataset](figures/chatbot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa75cf8-9cd9-4a72-a610-4392b80ca6b5",
   "metadata": {},
   "source": [
    "# Chatbot Dataset Loading 및 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd0c3c-2b8f-4a4c-b90c-a0fa6d828c4c",
   "metadata": {},
   "source": [
    "## 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ed2c29e-1aef-48c3-87ba-a89cc8ed6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests로 받기\n",
    "import requests\n",
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/songys/Chatbot_data/refs/heads/master/ChatbotData.csv'\n",
    "res = requests.get(url)\n",
    "\n",
    "if res.status_code == 200 :\n",
    "    with open('data/chat_bot_data.csv', 'wt', encoding='utf-8') as fw :\n",
    "        fw.write(res.text)\n",
    "else :\n",
    "    print(f'불러오지 못함 {url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc98856-b66d-4e2e-bc8a-27e3d34dcfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/chat_bot_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e670d063-8c71-4ddf-b6b0-15631c7f58da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A\n",
       "0                       12시 땡!                하루가 또 가네요.\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.\n",
       "...                        ...                       ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.\n",
       "\n",
       "[11823 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.info())\n",
    "\n",
    "df.drop(columns='label', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aad072-2245-41e8-9863-a0b451262fdd",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e0b8d-ee13-488b-ae6e-2a7d4189fd6c",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### Subword방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d080f3aa-6d49-464f-9469-91ef9bcd351c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12시 땡! 하루가 또 가네요.',\n",
       " '1지망 학교 떨어졌어 위로해 드립니다.',\n",
       " '3박4일 놀러가고 싶다 여행은 언제나 좋죠.',\n",
       " '3박4일 정도 놀러가고 싶다 여행은 언제나 좋죠.',\n",
       " 'PPL 심하네 눈살이 찌푸려지죠.',\n",
       " 'SD카드 망가졌어 다시 새로 사는 게 마음 편해요.',\n",
       " 'SD카드 안돼 다시 새로 사는 게 마음 편해요.',\n",
       " 'SNS 맞팔 왜 안하지ㅠㅠ 잘 모르고 있을 수도 있어요.',\n",
       " 'SNS 시간낭비인 거 아는데 매일 하는 중 시간을 정하고 해보세요.',\n",
       " 'SNS 시간낭비인데 자꾸 보게됨 시간을 정하고 해보세요.',\n",
       " 'SNS보면 나만 빼고 다 행복해보여 자랑하는 자리니까요.',\n",
       " '가끔 궁금해 그 사람도 그럴 거예요.',\n",
       " '가끔 뭐하는지 궁금해 그 사람도 그럴 거예요.',\n",
       " '가끔은 혼자인게 좋다 혼자를 즐기세요.',\n",
       " '가난한 자의 설움 돈은 다시 들어올 거예요.',\n",
       " '가만 있어도 땀난다 땀을 식혀주세요.',\n",
       " '가상화폐 쫄딱 망함 어서 잊고 새출발 하세요.',\n",
       " '가스불 켜고 나갔어 빨리 집에 돌아가서 끄고 나오세요.',\n",
       " '가스불 켜놓고 나온거 같아 빨리 집에 돌아가서 끄고 나오세요.',\n",
       " '가스비 너무 많이 나왔다. 다음 달에는 더 절약해봐요.',\n",
       " '가스비 비싼데 감기 걸리겠어 따뜻하게 사세요!',\n",
       " '가스비 장난 아님 다음 달에는 더 절약해봐요.',\n",
       " '가장 확실한 건 뭘까? 가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.',\n",
       " '가족 여행 가기로 했어 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족 여행 고고 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족 여행 어디로 가지? 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족 있어? 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
       " '가족관계 알려 줘 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
       " '가족끼리 여행간다. 더 가까워질 기회가 되겠네요.',\n",
       " '가족들 보고 싶어 저도요.',\n",
       " '가족들이랑 서먹해 다들 바빠서 이야기할 시간이 부족했나봐요.',\n",
       " '가족들이랑 서먹해졌어 다들 바빠서 이야기할 시간이 부족했나봐요.',\n",
       " '가족들이랑 어디 가지? 온 가족이 모두 마음에 드는 곳으로 가보세요.',\n",
       " '가족들이랑 여행 갈거야 좋은 생각이에요.',\n",
       " '가족여행 가야지 더 가까워질 기회가 되겠네요.',\n",
       " '가족이 누구야? 저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요',\n",
       " '가족이랑 여행 가려고 좋은 생각이에요.',\n",
       " '가족한테 스트레스 풀었어 정말 후회할 습관이에요.',\n",
       " '가출할까? 무모한 결정을 내리지 마세요.',\n",
       " '가출해도 갈 데가 없어 선생님이나 기관에 연락해보세요.',\n",
       " '간만에 떨리니까 좋더라 떨리는 감정은 그 자체로 소중해요.',\n",
       " '간만에 쇼핑 중 득템했길 바라요.',\n",
       " '간만에 휴식 중 휴식도 필요하죠.',\n",
       " '간식 뭐 먹을까 단짠으로 두 개 사는게 진리죠.',\n",
       " '간식 추천 단짠으로 두 개 사는게 진리죠.',\n",
       " '간장치킨 시켜야지 맛있게 드세요.',\n",
       " '간접흡연 싫어 저도 싫어요.',\n",
       " '갈까 말까 고민 돼 가세요.',\n",
       " '갈까 말까? 가세요.',\n",
       " '감 말랭이 먹고 싶다. 맛있게 드세요.',\n",
       " '감 말랭이 먹어야지 맛있게 드세요.',\n",
       " '감기 같애 병원가세요.',\n",
       " '감기 걸린 것 같아 이럴 때 잘 쉬는 게 중요해요.',\n",
       " '감기 기운이 있어 이럴 때 잘 쉬는 게 중요해요.',\n",
       " '감기 들 거 같애 이럴 때 잘 쉬는 게 중요해요.',\n",
       " '감기가 오려나 따뜻하게 관리하세요.',\n",
       " '감기약이 없어 병원가세요.',\n",
       " '감기인거 같애 병원가세요.',\n",
       " '감미로운 목소리 좋아 저도 듣고 싶네요.',\n",
       " '감정이 쓰레기통처럼 엉망진창이야 자신을 더 사랑해주세요.',\n",
       " '감정컨트롤을 못하겠어 그건 습관이에요.',\n",
       " '감정컨트롤이 안돼 그건 습관이에요.',\n",
       " '감히 나를 무시하는 애가 있어 콕 집어서 물어보세요.',\n",
       " '갑자기 나쁜 생각이 막 들더라 좋은 생각만 하세요.',\n",
       " '갑자기 눈물 나 마음이 아픈가요.',\n",
       " '갑자기 물어봐서 당황했어 갑작스러웠나봐요.',\n",
       " '갑자기 불편한 사이가 된 거 같아 관계의 변화가 왔나봅니다.',\n",
       " '강렬한 첫인상 남겨야 하는데 처음 3초가 중요해요. 당신의 매력을 어필해보세요.',\n",
       " '강아지 키우고 싶어 책임질 수 있을 때 키워 보세요.',\n",
       " '강아지 키우고 싶은데 역시 안돼겠지 먼저 생활패턴을 살펴 보세요.',\n",
       " '강아지 키울 수 있을까 먼저 생활패턴을 살펴 보세요.',\n",
       " '강아지 키울까 책임질 수 있을 때 키워 보세요.',\n",
       " '강원도 가서 살까? 아름다운 곳이죠.',\n",
       " '같이 게임하자고 해도 되나? 안 될 것도 없죠.',\n",
       " '같이 놀러갈 친구가 없어 혼자도 좋아요.',\n",
       " '같이 먹었는데 나만 살찐 거 같아 연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요.',\n",
       " '같이 수영장 가기로 했어 즐거운 시간 보내고 오세요!',\n",
       " '같이 있으면 힘든데 붙잡고 싶어 질질 끌지 마세요.',\n",
       " '같이 피씨방 가자고 해볼까? 말해보세요.',\n",
       " '같이 할 수 있는 취미 생활 뭐 있을까 함께하면 서로를 더 많이 알게 될 거예요.',\n",
       " '개강룩 입어볼까 개시해보세요.',\n",
       " '개강옷 예쁘게 입어 볼까 개시해보세요.',\n",
       " '개강이다 곧 방학이예요.',\n",
       " '개강이라니 방학이 참 짧죠.',\n",
       " '개같은 상황 벗어나는 게 좋겠네요.',\n",
       " '개같이 되버렸어. 벗어나는 게 좋겠네요.',\n",
       " '개기름 꼈어 세수하고 오세요.',\n",
       " '개념도 놓고 옴 그게 제일 중요한 건데요.',\n",
       " '개념이 없어 그게 제일 중요한 건데요.',\n",
       " '개당황 다음부터는 더 많이 아세요.',\n",
       " '개당황했잖아 갑자기 물어 봐서 갑작스러웠나봐요.',\n",
       " '개인적인 업무까지 다 시켜 공적인 일부터 하세요.',\n",
       " '개인적인 일도 다 시켜 공적인 일부터 하세요.',\n",
       " '개졸려 낮잠을 잠깐 자도 괜찮아요.',\n",
       " '개좋아 저도 좋아해주세요.',\n",
       " '개학하니까 좋다 친구들이 보고싶었나봐요.',\n",
       " '걔 너무 싫다 되도록 만나지 마세요.',\n",
       " '걔는 누굴 닮아서 그런거니? 당신이요.',\n",
       " '걔랑 같은 반 됐으면 좋겠다 당신의 운을 믿어보세요.',\n",
       " '거지 같이 일해 놓고 갔어 일 못하는 사람이 있으면 옆에 있는 사람이 더 힘들죠.',\n",
       " '거지됐어 밥 사줄 친구를 찾아 보세요~',\n",
       " '거짓말 했어 선의의 거짓말이길 바라요.',\n",
       " '거짓말을 나도 모르게 자꾸 해 거짓말은 할수록 늘어요.',\n",
       " '거짓말을 하게 돼 거짓말은 할수록 늘어요.',\n",
       " '거짓말이 거짓말을 낳아 진실된 말을 하려고 노력해보세요.',\n",
       " '걱정 없이 살고파 누구나 걱정은 있어요.',\n",
       " '걱정 좀 없이 살고 싶다. 누구나 걱정은 있어요.',\n",
       " '건강 관리 운동을 해보세요.',\n",
       " '건강 빨리 회복해야지 세상의 무엇보다 건강이 제일 중요해요.',\n",
       " '건강검진 왔어 주기적으로 해주는 게 좋죠.',\n",
       " '건강검진하러 옴 주기적으로 해주는 게 좋죠.',\n",
       " '건강이 최고 가장 중요한 목표네요.',\n",
       " '건강이 최고인 것 같아 가장 중요한 목표네요.',\n",
       " '건강하게 다이어트 하는 방법 적게 먹고 많이 움직이세요.',\n",
       " '건강한 다이어트법 적게 먹고 많이 움직이세요.',\n",
       " '건너건너 아는 사람인데 연락해도 될까? 모르는 사이라 당황할 수도 있어요.',\n",
       " '건물주 되고싶어 이룰 수 있을 거예요.',\n",
       " '건물주가 짱인데 이룰 수 있을 거예요.',\n",
       " '건방져 기분이 나쁘셨나봐요.',\n",
       " '건조기 살까봐 있으면 편하대요.',\n",
       " '건조하네 눈을 깜빡거려 보세요.',\n",
       " '걸레질도 해야 돼 청소를 좋아하시나봐요.',\n",
       " '걸어 가고 있는데 깜깜해서 무서워 안전 귀가 하세요.',\n",
       " '겁난다 용기 내보세요.',\n",
       " '게으른 동료가 있어 피해를 안 준다면 무시하세요.',\n",
       " '게임 같이 하자고 할까? 안 될 것도 없죠.',\n",
       " '게임 때문에 시간 다갔어 게임할때는 시간이 더 빨리 가요.',\n",
       " '게임 때문에 폰이 점점 느려지는듯 정리해보세요.',\n",
       " '게임 재미있어. 게임하세요!',\n",
       " '게임 지겨워 다른 게임해보세요.',\n",
       " '게임도 이제 재미없어 다른 게임해보세요.',\n",
       " '게임하고 싶어 게임하세요!',\n",
       " '게임하다 시간 다갔어 게임할때는 시간이 더 빨리 가요.',\n",
       " '겨울 지나 봄이야 마음에도 봄이 오길 바라요.',\n",
       " '겨울에는 온천이지! 몸은 뜨겁고 머리는 차갑게!',\n",
       " '겨울이 가고 봄이 올거야 마음에도 봄이 오길 바라요.',\n",
       " '격려 좀 해줘 잘하실 거예요!',\n",
       " '격려가 필요해. 잘하실 거예요!',\n",
       " '견과류 챙겨 먹어야지. 건강 생각해서 챙겨드세요.',\n",
       " '결국 이런 운명이라니 슬프다 좋은 운명도 있을거예요.',\n",
       " '결정 못하겠어. 결정하기 힘드시겠네요.',\n",
       " '결정은 빠르면 빠를 수록 좋겠지? 자신을 위한 결정을 내리길 바라요.',\n",
       " '결정은 빠를수록 좋겠지? 자신을 위한 결정을 내리길 바라요.',\n",
       " '결정을 못 내리겠어. 어떻해 결정은 빠르면 빠를수록 좋을 거예요.',\n",
       " '결정적인 물증이 없어 안타깝네요. 증거를 지금이라도 모아봐요.',\n",
       " '결혼 했는데. 좋겠어요.',\n",
       " '결혼 했어 좋겠어요.',\n",
       " '결혼도 다 돈이다. 많이 들지만 줄일 수 있을 거예요.',\n",
       " '결혼식 가기 귀찮아 경조사는 참석하는게 좋아요.',\n",
       " '결혼식 또 가야돼 경조사는 참석하는게 좋아요.',\n",
       " '결혼식때 하객이 없을 까봐 걱정돼 생각보다 신경 안 씁니다.',\n",
       " '결혼식이 너무 많아 인맥이 넓으신가봐요.',\n",
       " '결혼이나 하지 왜 자꾸 나한테 화 내냐구! 힘들겠네요.',\n",
       " '결혼준비 돈 많이 들겠지 많이 들지만 줄일 수 있을 거예요.',\n",
       " '결혼준비하는데 돈 얼마나 드나 욕심에 따라 천지 차이일 거예요.',\n",
       " '결혼하는데 돈 많이 드네 허례허식이에요.',\n",
       " '결혼하는데 돈 얼마나 들까 욕심에 따라 천지 차이일 거예요.',\n",
       " '결혼하면 좋아? 해봐요.',\n",
       " '결혼하면 좋을까 서로 노력하면 행복할 거예요.',\n",
       " '결혼하면 행복할까? 서로 노력하면 행복할 거예요.',\n",
       " '결혼하면 행복해? 사람마다 행복의 크기가 다르겠지만 행복할 거예요.',\n",
       " '결혼하면 행복해질까? 사람마다 행복의 크기가 다르겠지만 행복할 거예요.',\n",
       " '결혼할까 능력이 있으면 하면 되죠.',\n",
       " '결혼해도 되나 능력이 있으면 하면 되죠.',\n",
       " '결혼해도 될까 이사람이다 싶은 사람이랑 하세요.',\n",
       " '결혼해야 하나 해봐요.',\n",
       " '경쟁이 너무 치열해 점점 치열해지는 것 같아요.',\n",
       " '계속 공부해도 될까 확신이 없나봐요.',\n",
       " '계속 도전하는 거 귀찮아 안정적인 걸 좋아하나봐요.',\n",
       " '계속 방학이면 좋을텐데 방학은 참 짧아요.',\n",
       " '계속 보고 싶어 보러 가세요.',\n",
       " '계속 보고 싶으면 어떡해? 보러 가세요.',\n",
       " '계속 속이 진짜 안 좋아 계속 좋지 않으면 병원에 가보세요.',\n",
       " '계속 엇갈리는 느낌 타이밍이 안 맞았나봐요.',\n",
       " '계속 학생하고 싶어 이제 취업 하셔야죠.',\n",
       " '계속 한숨만 나와 뇌세포에 에너지를 공급하려는 자연스러운 현상이에요. 에너지가 부족한가봐요.',\n",
       " '고3은 공부만 해야겠지. 공부가 최우선이죠.',\n",
       " '고3이니까 공부해야겠지 공부가 최우선이죠.',\n",
       " '고구마 다이어트 해야지 너무 무리하지는 마세요.',\n",
       " '고구마만 먹고 다이어트 해야지 너무 무리하지는 마세요.',\n",
       " '고기 구워 먹고 싶다. 저기압에는 고기앞이죠.',\n",
       " '고기 먹고 싶어 저기압에는 고기앞이죠.',\n",
       " '고데기 망했어 연습이 필요해요.',\n",
       " '고데기 했는데 망했어 연습이 필요해요.',\n",
       " '고독한 밤 혼자가 아니에요.',\n",
       " '고마운 사람들이 많아 인복이 많나봐요.',\n",
       " '고무신 거꾸로 신으면 어쩌지 너무 걱정하지 마세요.',\n",
       " '고민 있어 네 말씀하세요.',\n",
       " '고민 좀 들어줄래 네 말씀하세요.',\n",
       " '고백하고 후회하면 어떡하지 후회는 후회를 낳을뿐이에요. 용기 내세요.',\n",
       " '고시원 너무 답답해 돈을 모아서 다른 곳으로 이사갈 수 있을 거예요.',\n",
       " '고시원 답답해 돈을 모아서 다른 곳으로 이사갈 수 있을 거예요.',\n",
       " '고시원에서 나가고 싶어 더 좋은 곳에서 살 수 있을 거예요.',\n",
       " '고시원에서 탈출하고 싶어 더 좋은 곳에서 살 수 있을 거예요.',\n",
       " '고양이 동영상 보는 중 완전 귀엽죠?',\n",
       " '고양이 키우고 싶어 자신을 먼저 키우세요.',\n",
       " '고양이 키우고 싶어 가족들과 상의해보세요.',\n",
       " '고의는 아닌데 실수를 한 거 같아 용서를 구하세요.',\n",
       " '고집 센 사람 피할 수 있으면 피하세요.',\n",
       " '고집하고는 피할 수 있으면 피하세요.',\n",
       " '골프 못 치는데 처음부터 잘하는 사람은 없어요.',\n",
       " '골프 배워야 돼 시간내서 가보세요.',\n",
       " '골프 어려워 처음부터 잘하는 사람은 없어요.',\n",
       " '골프치러 가야돼 시간내서 가보세요.',\n",
       " '곱창 먹고 싶어. 미리 미리 충전해주세요.',\n",
       " '곱창 생각나 미리 미리 충전해주세요.',\n",
       " '공무원 괜찮겠지 안정적이고 좋죠.',\n",
       " '공무원 되고 싶다 준비해보세요.',\n",
       " '공무원 되면 좋겠다 준비해보세요.',\n",
       " '공무원 시험 공부 힘들다 합격 기원해요!',\n",
       " '공무원 시험 죽을 거 같아 철밥통 되기가 어디 쉽겠어요.',\n",
       " '공무원 시험 힘들어ㅠㅠ 철밥통 되기가 어디 쉽겠어요.',\n",
       " '공무원 준비할까 시작이 반이에요. 어서 준비하세요.',\n",
       " '공무원이 좋지? 안정적이고 좋죠.',\n",
       " '공복이라 신경이 예민해져 자연스러운 현상이에요.',\n",
       " '공복이라 예민해 자연스러운 현상이에요.',\n",
       " '공복이면 예민함? 보이는 게 없죠.',\n",
       " '공부 계속해도 될까 지금처럼 잘될 거예요.',\n",
       " '공부 꼭 해야 할까 미래의 배우자가 달라져요.',\n",
       " '공부 때려치워야 하나 확신이 없나봐요.',\n",
       " '공부 시작해도 될까 공부는 언제나 좋죠.',\n",
       " '공부 왜 해야 돼? 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공부 잘 안돼 같이 수다 떨면서 놀까요?',\n",
       " '공부 잘하고 싶어 나만의 공부방법을 찾아보세요.',\n",
       " '공부 좀 더 할 걸 지금도 늦지 않았어요.',\n",
       " '공부 하기 싫다 같이 수다 떨면서 놀까요?',\n",
       " '공부는 내 체질이 아닌 것 같아 확신이 없나봐요.',\n",
       " '공부로 먹고 살 수 있을까 지금처럼 잘될 거예요.',\n",
       " '공부방법이 잘못된걸까? 나한테 맞는 공부 방법 찾는 게 시급하네요.',\n",
       " '공부하기 싫어 잠시 쉬어도 돼요.',\n",
       " '공부하기 싫은 날 잠시 쉬어도 돼요.',\n",
       " '공부하는 낙이 없어 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공부하는 이유? 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공부하는 이유가 없어 공부하면 더 많은 선택을 할 수 있죠.',\n",
       " '공시 준비 힘들어 합격 기원해요!',\n",
       " '공시 준비 힘들어 잘 될 거예요.',\n",
       " '공시 준비중 좋은 결과 있을 거예요!',\n",
       " '공시 준비하는데 힘들다 잘 될 거예요.',\n",
       " '공시생이야 좋은 결과 있을 거예요!',\n",
       " '공연 보고 싶어 친구와 같이 가보세요.',\n",
       " '공연 보러 가고 싶어 친구와 같이 가보세요.',\n",
       " '공책 필기 나만 힘들어? 성향 차이가 좀 있기는 하죠.',\n",
       " '공황장애 생겼어. 꾸준히 약 먹고 치료해보세요.',\n",
       " '공황장애 있어 꾸준히 약 먹고 치료해보세요.',\n",
       " '공휴일에는 집이 최고 피로 풀고 좋죠.',\n",
       " '공휴일에는 집콕 피로 풀고 좋죠.',\n",
       " '과거는 잊고 앞으로 나아 가야지 오늘이 중요하죠.',\n",
       " '과거는 중요하지 않아 오늘이 중요하죠.',\n",
       " '과식해서 소화가 안돼 소화제 챙겨드세요.',\n",
       " '과식했나 봐 과식은 금물이에요.',\n",
       " '과식했다 소화제 드세요.',\n",
       " '과외비 부담되겠지? 안된다고 하면 거짓말이겠지요.',\n",
       " '과외비 비싸? 안된다고 하면 거짓말이겠지요.',\n",
       " '과일 먹고 자야지 제철과일이 정말 좋아요.',\n",
       " '과일 먹어야지. 건강 생각해서 챙겨드세요.',\n",
       " '과일 안 먹게 돼 그래도 먹으려고 노력해보세요.',\n",
       " '과일 잘 안 먹게 돼 그래도 먹으려고 노력해보세요.',\n",
       " '과일 챙겨 먹어야지 제철과일이 정말 좋아요.',\n",
       " '관계가 계속 애매하다. 인간 관계도 정리가 필요해요.',\n",
       " '관심 끄라고 하고 싶다. 무관심이 필요할 때가 있죠.',\n",
       " '관심 좀 안 가졌으면 무관심이 필요할 때가 있죠.',\n",
       " '관절염 같애 계단 조심하세요.',\n",
       " '관절염인가 계단 조심하세요.',\n",
       " '광고가 안 끝나 채널을 돌려보세요.',\n",
       " '괜찮아지고 있어 괜찮아지고 있어 다행이에요.',\n",
       " '괜찮은 사람인데 사귀긴 싫어 남자사람친구, 여자사람친구 하세요.',\n",
       " '괜히 건들지 말라고 많이 지쳤나봐요.',\n",
       " '괜히 기다렸어 누군가를 기다린다는게 쉬운게 아니죠.',\n",
       " '괜히 농담해서 망했다 늦지 않았어요.',\n",
       " '괜히 아까운 시간 버렸다 그 것도 다 경험이라고 생각하세요.',\n",
       " '괜히 창피해 그럴 필요 없어요.',\n",
       " '괴물이 되어 가는 느낌이 들어 그렇지 않아요.',\n",
       " '교보문고 왔어 마음에 드는 책을 잘 찾아보세요.',\n",
       " '교양 수업 재밌어 저도 듣고 싶어요.',\n",
       " '교양수업 시간에 마음에 드는 애 있어 같은 조가 되길 바랄게요.',\n",
       " '교양수업 은근 재미져 지식 쌓는 재미가 있죠.',\n",
       " '교양수업에서 마음에 드는 애 있어 같은 조가 되길 바랄게요.',\n",
       " '교양수업이 재미있어 지식 쌓는 재미가 있죠.',\n",
       " '교양이 전공보다 재미있어 저도 듣고 싶어요.',\n",
       " '교직이수 가능할까 학점 관리하세요.',\n",
       " '교통사고 났었어. 보험 처리하세요.',\n",
       " '교통사고 당했어 보험 처리하세요.',\n",
       " '교회 가기 싫어 왜 그럴까요?',\n",
       " '교회 갔다 만났어 좋은 만남이었길 바라요.',\n",
       " '교회에서 만났어 좋은 만남이었길 바라요.',\n",
       " '구박하면서 엄청 일 시켜 일을 몰라서 그런가봐요.',\n",
       " '군대 갔다 올 때까지 기다릴 수 있을까 자신의 삶을 살다보면 기다릴 수 있을 거예요.',\n",
       " '군대 기다려 주려고 부담스러워하지 않는다면 기다려도 좋을 것 같아요.',\n",
       " '군대 기다려도 될까 부담스러워하지 않는다면 기다려도 좋을 것 같아요.',\n",
       " '군대 기다리면 부담스러워할까 너무 걱정하지 마세요.',\n",
       " '군대 기다릴 수 있을까 자신의 삶을 살다보면 기다릴 수 있을 거예요.',\n",
       " '군대 언제 끝나나 군대 시계는 멈추지 않아요.',\n",
       " '군대 전역 기다려 군대 시계는 멈추지 않아요.',\n",
       " '굿모닝 좋은 아침이에요.',\n",
       " '궁금하면 오백원 안 궁금해요.',\n",
       " '궁금하지? 안 궁금해요.',\n",
       " '궁금해 자세히 말씀해주세요.',\n",
       " '궁금해 알려줘 자세히 말씀해주세요.',\n",
       " '귀 아파 병원에 가세요.',\n",
       " '귀가 가려워 누가 욕하고 있나봐요.',\n",
       " '귀가 간지러 누가 욕하고 있나봐요.',\n",
       " '귀가 윙윙거려 병원에 가세요.',\n",
       " '귀농 어때? 생각하기는 쉬운데 실천하기는 어려운 것 같아요.',\n",
       " '그 사람이 나 안 좋아하는 거 같아 슬픈 이야기네요.',\n",
       " '그 사람이 나 좋아해줬으면 좋겠다 저도 간절히 기도 할게요.',\n",
       " '그 사람이 행복했으면 좋겠다 그렇게 될 수 있을 거예요.',\n",
       " '그 시절엔 다 그랬지 추억에 잠길 때도 필요해요.',\n",
       " '그냥 고백할걸 후회는 후회를 낳을뿐이에요. 용기 내세요.',\n",
       " '그냥 공무원이 좋을 듯 시작이 반이에요. 어서 준비하세요.',\n",
       " '그냥 내버려 둬 주었으면 아무도 없는 곳으로 여행을 떠나보세요.',\n",
       " '그냥 선 볼까? 많이 만나보세요.',\n",
       " '그냥 쉬고 싶다 잠깐 바람 쐬고 오세요.',\n",
       " '그냥 씹어야겠다. 다른 사람이 답답할 거예요.',\n",
       " '그냥 이렇게 살고 싶어 살고 싶은대로 사세요.',\n",
       " '그냥 자는 거 아니지? 많이 피곤한가봐요.',\n",
       " '그냥 잘못했다고 하면 될거 같은데 자꾸 변명해 피할 수 있으면 피하세요.',\n",
       " '그냥 택시 타야지. 조심히 오세요.',\n",
       " '그냥 할까? 실천이 말보다 낫죠.',\n",
       " '그냥 혼자 밥이나 먹어야지 밥심으로 사는 거죠.',\n",
       " '그냥 혼자 있는게 좋아 혼자만 있지 마세요.',\n",
       " '그동안 잘 지냈나요? 안부를 물어주시다니 감사합니다.',\n",
       " '그땐 그랬지 추억에 잠길 때도 필요해요.',\n",
       " '그래 그러자 괜찮은 선택이길 바라요.',\n",
       " '그래 이제 결정했어 좋은 결과 있을 거예요.',\n",
       " '그래도 좀 기대했는데 기쁜 마음으로 베풀고 보답을 바라지 마세요.',\n",
       " '그런 말을 왜 하지 다른 사람 말은 신경쓰지 마세요.',\n",
       " '그런 사람인가보다 해야하나봐 대인배시군요.',\n",
       " '그런 사람인갑다 해야지 대인배시군요.',\n",
       " '그런 친구 아니었는데 너무 귀찮게 하네 친구가 좋아하나봐요.',\n",
       " '그렇게 갈 거면서 이야기를 해보세요.',\n",
       " '그렇게 오래 살았는데도 이해를 못하겠어 온전한 이해는 없어요.',\n",
       " '그렇게 할래 괜찮은 선택이길 바라요.',\n",
       " '그림 잘 그리고 싶다 학원을 다니거나 연습하면 잘할 수 있을 거예요.',\n",
       " '그림 좀 잘 그렸으면 좋겠다 학원을 다니거나 연습하면 잘할 수 있을 거예요.',\n",
       " '그만 두고 나오고 싶어 뒷감당 자신 있으면 하세요.',\n",
       " '그만 먹어야 하는데 조금만 드세요',\n",
       " '그만 살고싶어 당신을 소중하게 생각하세요.',\n",
       " '그저 그런 하루 그런 하루도 감사한 마음을 가져보세요.',\n",
       " '근사한 곳 알아 냈어 좋은 사람과 함께 가세요.',\n",
       " '근육 있으면 멋있을텐데 저 말씀이신가요?',\n",
       " '금값 알아? 비싸요.',\n",
       " '금값 어때 비싸요.',\n",
       " '금사빠인가 호의인지 호감인지 헷갈리나요?',\n",
       " '금수저 물고 태어나면 좋겠지? 뭔가 안풀리는 일이 있나봐요.',\n",
       " '금수저로 태어났으면 아이를 금수저로 만들어주세요.',\n",
       " '금수저로 태어났으면 좋았을텐데 아이를 금수저로 만들어주세요.',\n",
       " '금연이 쉽지 않아 자신을 이겨야해요.',\n",
       " '기 빨렸어 너무 긴장했나봐요.',\n",
       " '기념일 다 챙기는거 귀찮아 기념일 챙겨주면 좋아할거예요.',\n",
       " '기념일 또 까먹었어 달력에 적어보세요.',\n",
       " '기념일 못챙겼어 달력에 적어보세요.',\n",
       " '기념일 챙기기 귀찮아 기념일 챙겨주면 좋아할거예요.',\n",
       " '기능 좀 알려줘봐봐 당신의 삶을 응원해 드릴 수 있어요라고 감히 말해 봅니다.',\n",
       " '기다리는 것도 지쳐 기다리지 마세요.',\n",
       " '기다리라고 말 못하겠어 상대방의 선택에 맡겨보세요.',\n",
       " '기다림이 습관이 됐나봐 좋은 분이시군요',\n",
       " '기대가 무너졌어 베풀되 보답을 바라지 마세요.',\n",
       " '기대가 부담스러운데 떨쳐낼 수 있는 방법 있을까? 자신을 사랑할수록 외부의 인정은 필요 없어요.',\n",
       " '기대하고 있었는데 상대에게 바라는 기대는 자신을 슬프게 해요.',\n",
       " '기대하지 말걸 베풀되 보답을 바라지 마세요.',\n",
       " '기대했는데 기쁜 마음으로 베풀고 보답을 바라지 마세요.',\n",
       " '기댈 수 있는 사람 의지할 수 있는 사람이 곁에 있다는 건 큰 행운일 거예요.',\n",
       " '기력이 없어 자신의 감정을 주변 사람들에게 터놓고 이야기해보세요.',\n",
       " '기름값 올랐어. 대중교통을 이용해주세요.',\n",
       " '기본이 뭔지도 모르는 것 같아. 각자가 생각하는 기본이 다를 수도 있어요.',\n",
       " '기본이 안 되어 있어 각자가 생각하는 기본이 다를 수도 있어요.',\n",
       " '기부 좀 했어요 좋은 일 하셨네요.',\n",
       " '기부했어 좋은 일 하셨네요.',\n",
       " '기분 꿀꿀해 내일은 오늘보다 나을 거예요.',\n",
       " '기분 나쁜 농담을 계속하고 있어 정색 한번 해주세요.',\n",
       " '기분 울적해서 좀 걷고 있어 걷다보면 조금 정리가 될 거예요.',\n",
       " '기분 전환 하고 싶어 저랑 함께 해요.',\n",
       " '기분 전환이 필요해 저랑 함께 해요.',\n",
       " '기분이 그지 같아 신나는 음악 들어보세요.',\n",
       " '기분이 더러워 경쾌한 음악 들어보세요.',\n",
       " '기분이 묘해 왜일까요?',\n",
       " '기분이 이상해 무슨 이유인지 생각해보세요.',\n",
       " '기숙사 괜찮을까 혼자 사는 것보다 불편하겠죠.',\n",
       " '기숙사 떨어졌어 다음 학기에는 학점 관리를 더 열심히 해봐요.',\n",
       " '기숙사 사는거 어떨까? 혼자 사는 것보다 불편하겠죠.',\n",
       " '기숙사 살면 불편해? 혼자 사는 것보다 불편하겠죠.',\n",
       " '기숙사 안됐어 다음 학기에는 학점 관리를 더 열심히 해봐요.',\n",
       " '기술 배울까 기술을 많이 알면 도움이 되겠죠.',\n",
       " '기차 타고 여행 가고 싶어 꿈꾸던 여행이네요.',\n",
       " '기차여행 가고 싶어 꿈꾸던 여행이네요.',\n",
       " '기침도 못하겠어 답답한 상황이네요.',\n",
       " '기침도 편하게 못해 답답한 상황이네요.',\n",
       " '기프트콘 받았어! 좋겠어요!',\n",
       " '기프트콘 선물 괜찮을까? 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기프트콘 선물해볼까? 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기프트콘 주면 좋아할까? 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기프트콘으로 선물 받았어 좋겠네요.',\n",
       " '기프트콘으로 선물 해야겠다 직접 주는 게 더 좋을 것 같아요.',\n",
       " '기회를 놓쳤어 더 좋은 기회가 올 거예요.',\n",
       " '기회를 못 잡았어 더 좋은 기회가 올 거예요.',\n",
       " '기획사니까 당연히 예쁜 애들 많겠지 연예인을 준비하니 일반인보다 다 예쁘겠죠.',\n",
       " '기획사에 예쁜 애들 많겠지 연예인을 준비하니 일반인보다 다 예쁘겠죠.',\n",
       " '긴 머리 관리 어렵다. 그래서 저는 못 기르고 잘라요.',\n",
       " '긴 머리 관리하는 거 힘들다 그래서 저는 못 기르고 잘라요.',\n",
       " '긴 시간이 걸렸지만 괜찮아. 괜찮아지고 있어 다행이에요.',\n",
       " '긴장 푸는 법 알려줘 크게 숨한 번 쉬어 보세요',\n",
       " '긴장돼 크게 숨한 번 쉬어 보세요',\n",
       " '긴장돼서 땀나네 미리 긴장하지 마세요.',\n",
       " '길거리에서 연락처 물어보면 줘도 되나 마음에 들면 줘보세요.',\n",
       " '길에서 담배 피우는 사람 싫어 저도 싫어요.',\n",
       " '길에서 번호 따였어 잘 해보세요.',\n",
       " '길에서 전번 물어보면 줘도 되나 마음에 들면 줘보세요.',\n",
       " '길에서 헌팅 당했어 잘 해보세요.',\n",
       " '길은 멀고 해는 진다 그래도 넘을 수 있을 거예요.',\n",
       " '길이 미끄러워서 미끄러질뻔했어 조심하세요.',\n",
       " '길이 안보여 너무 낙담하지 마세요.',\n",
       " '길이 얼어서 미끄러질뻔했어 조심하세요.',\n",
       " '길이 얼었어 미끄러우니 조심하세요.',\n",
       " '김떡순 먹고 싶어. 건강을 위해 조금씩 드세요.',\n",
       " '김치도 없네 마트 갑시다.',\n",
       " '김치볶음밥 먹어야지 맛있는 식사시간 되시길 바랄게요.',\n",
       " '김치볶음밥이나 만들어 먹어야지 맛있는 식사시간 되시길 바랄게요.',\n",
       " '김치찌개 먹고 싶어 맛있죠!',\n",
       " '까아 오빠들 컴백한다 기다렸나봐요.',\n",
       " '깜깜한데 전기 안들어오네 조금만 기다리면 다시 전기가 들어올거예요.',\n",
       " '깡 마른 거 같아 적당해요.',\n",
       " '꼴 사나워질 것 같은데 스스로 단단해지세요.',\n",
       " '꽃 받고 싶다 제가 드리고 싶네요.',\n",
       " '꽃 사고 싶어 집안 분위기가 바뀔 거예요.',\n",
       " '꽃 살까? 집안 분위기가 바뀔 거예요.',\n",
       " '꽃 선물 좋아할까 꽃 선물은 언제나 좋죠.',\n",
       " '꽃 선물해 볼까 꽃 선물은 언제나 좋죠.',\n",
       " '꽃 예쁘게 말렸어 솜씨가 좋으시네요.',\n",
       " '꽃게탕 맛있다. 기분 좋아 보이세요.',\n",
       " '꽃게탕 진짜 밥도둑 기분 좋아 보이세요.',\n",
       " '꽃꽂이 배우는 중 마음의 안정을 취하기 좋은 취미네요.',\n",
       " '꽃꽂이 배우니까 좋다 마음의 안정을 취하기 좋은 취미네요.',\n",
       " '꽃놀이 가고 싶어 벚꽃 계절이 다가왔네요.',\n",
       " '꽃다발 말려봐야지 거꾸로 해서 드라이플라워 만들어보세요.',\n",
       " '꽃다발 말리면 에쁘겠지. 거꾸로 해서 드라이플라워 만들어보세요.',\n",
       " '꽃다발 받았어 부러워요!',\n",
       " '꽃다발 샀어 멋진 선물이네요.',\n",
       " '꽃다발 선물 괜찮지? 센스있는 선물이에요.',\n",
       " '꽃다발 선물 받았어 부러워요!',\n",
       " '꽃다발 선물 어때? 센스있는 선물이에요.',\n",
       " '꽃다발 준비했어 멋진 선물이네요.',\n",
       " '꽃바구니 선물이랑 과일 바구니 선물 뭐가 좋아? 받는 사람이 부럽네요.',\n",
       " '꽃바구니가 좋을까 과일바구니까 좋을까 받는 사람이 부럽네요.',\n",
       " '꽃선물 받고 어 제가 드리고 싶네요.',\n",
       " '꿀잼 저도 즐거워요',\n",
       " '꿈은 많은데 차근차근 이뤄보아요.',\n",
       " '꿈이 너무 많아 차근차근 이뤄보아요.',\n",
       " '꿈이 너무 무서웠어 요즘 예민한가봐요.',\n",
       " '꿈이 다양해 많으면 많을 수록 좋죠.',\n",
       " '꿈이 두 개야 더 많아도 괜찮아요.',\n",
       " '꿈이 없어 거창하지 않아도 돼요.',\n",
       " '꿈이 이루어질까? 현실을 꿈처럼 만들어봐요.',\n",
       " '꿈이 자꾸 바뀌어 많으면 많을 수록 좋죠.',\n",
       " '꿈이 현실이었으면 현실을 꿈처럼 만들어봐요.',\n",
       " '끝나니까 허무하다 뜻대로 되는게 많지 않죠.',\n",
       " '끝나면 좋을 줄 알았는데. 마음이 허전하신가봐요.',\n",
       " '낌새가 이상하더니 딱 걸렸어 잘 해결되길 바라요.',\n",
       " '낌새가 있더니 딱 걸렸어 잘 해결되길 바라요.',\n",
       " '나 감정쓰레기통이었나봐 자신을 더 사랑해주세요.',\n",
       " '나 갖고 장난친건가 아니길 바라요.',\n",
       " '나 같은 사람은 동물 키우면 안되겠지 잘 아시네요.',\n",
       " '나 같이 예쁜 애를 왜 갈구지 애정표현일 지도 몰라요.',\n",
       " '나 거짓말 못하겠어 얼굴에 다 티가 나네요.',\n",
       " '나 결정 잘 한거지? 네, 이제 잘 해낼 차례예요.',\n",
       " '나 결정했어 좋은 결과 있을 거예요.',\n",
       " '나 괜찮지 않니 괜찮은 사람이에요.',\n",
       " '나 교직이수할 수 있을까? 학점 관리하세요.',\n",
       " '나 그동안 뭐한거니 바람 좀 쐬고 오시면 좋은텐데.',\n",
       " '나 그지임 밥 사줄 친구를 찾아 보세요~',\n",
       " '나 내일 기숙사 가야돼 짐 빼놓지 말고 싸세요.',\n",
       " '나 내장비만이래 식단조절도 하고 꾸준히 운동하세요.',\n",
       " '나 너무 못 생겼어 충분히 아름다워요.',\n",
       " '나 너무 소심해 꼼꼼한 거예요.',\n",
       " '나 노트북 사줘 노트북은 비싸요.',\n",
       " '나 놀려먹기 쉬운가? 절대 그렇지 않아요.',\n",
       " '나 누구게? 저도 궁금하네요.',\n",
       " '나 누락됐나봐 확인해달라고 해보세요.',\n",
       " '나 다른 거 할까 시도해봐도 좋겠죠.',\n",
       " '나 대충한 거 아닌데 사람들이 몰라줘도 알아주는 사람이 있을 거예요.',\n",
       " '나 뒷담화하는 애 어떻게 할까? 너무 신경쓰지 말고 그러든지 하고 아무렇지도 않게 넘겨보세요.',\n",
       " '나 뒷담화하는 애 있다는데 어떻게 하지? 너무 신경쓰지 말고 그러든지 하고 아무렇지도 않게 넘겨보세요.',\n",
       " '나 많이 기대했는데 상대에게 바라는 기대는 자신을 슬프게 해요.',\n",
       " '나 말 실수한 거 같아. 곰곰히 되짚어보세요.',\n",
       " '나 맨날 속는 거 같아 즐겁게 속아주세요.',\n",
       " '나 머리 나쁜 듯 자책하지 마세요.',\n",
       " '나 머리가 나뿐 것 같아 자책하지 마세요.',\n",
       " '나 먼저 잘게 안녕히 주무세요.',\n",
       " '나 모르는게 왜 이렇게 많지 당연한 거예요.',\n",
       " '나 몰래 사귀는 거 같애 눈치가 빠르시군요.',\n",
       " '나 무시 당한 거 같아 그런 생각을 들게 하는 사람 상종하지 마세요.',\n",
       " '나 무시하는 거 같아 그런 생각을 들게 하는 사람 상종하지 마세요.',\n",
       " '나 무시하는 사람 어떻게 해? 무시하세요.',\n",
       " '나 무시하는 사람 짜증나 무시하세요.',\n",
       " '나 문제가 많은거 같아 문제는 해결하라고 있는 거죠.',\n",
       " '나 뭐하는 거지 멍 때리고 있죠.',\n",
       " '나 미팅한다! 성공을 기원합니다.',\n",
       " '나 바뀌고 싶어 긍정적으로 바뀔 수 있어요',\n",
       " '나 바본인가 봄 바보는 자기한테 바보라고 하지 않아요.',\n",
       " '나 백수야 저랑 놀아요.',\n",
       " '나 버림 받은 거 같아 아닐거예요.',\n",
       " '나 보이스피싱 당한 거 같은데 어떡해? 경찰에 신고하고 취할 수 있는 조취를 취해보세요.',\n",
       " '나 비만이야 건강하게 운동해보세요.',\n",
       " '나 사랑하니? 많이 사랑해요!',\n",
       " '나 상 받는대! 축하합니다!',\n",
       " '나 새 옷 샀다 꼬까옷 개시해보세요.',\n",
       " '나 서류에서 광탈했어 자책하지 마세요.',\n",
       " '나 소개팅한다! 성공을 기원합니다.',\n",
       " '나 속은 거 같아 다음부터 속지 마세요.',\n",
       " '나 속은듯 기분나쁘겠어요.',\n",
       " '나 수학여행 간다 친구들과 좋은 추억 만들고 오세요.',\n",
       " '나 스마트폰 중독인가봐 가끔 핸드폰없이 살아보세요.',\n",
       " '나 승진했어 하늘만큼 땅만큼 축하해요',\n",
       " '나 실수한건가 잘 생각해보세요.',\n",
       " '나 실수했나 곰곰히 되짚어보세요.',\n",
       " '나 아재인가 고민하고 있으면 그럴 거예요.',\n",
       " '나 아직 어른 아닌 거 같아 물리적 나이가 아니라 정신적 나이가 중요하니까요.',\n",
       " '나 아직도 애 같아. 물리적 나이가 아니라 정신적 나이가 중요하니까요.',\n",
       " '나 어때? 괜찮은 사람이에요.',\n",
       " '나 여기서 뭐하는 거지 멍 때리고 있죠.',\n",
       " '나 연기 너무 못해 거짓말 못하겠어 얼굴에 다 티가 나네요.',\n",
       " '나 열심히 할거야 좋은 태도네요.',\n",
       " '나 오늘 개불쌍 저도 사는데요.',\n",
       " '나 오늘 따라 잘생겨 보이네 자신에게 콩깍지가 씌였나봐요.',\n",
       " '나 오늘 상 받았지롱 축하드려요.',\n",
       " '나 완전 계탔어! 축하해요!',\n",
       " '나 왕따야 친구들과 잘 어울려보세요.',\n",
       " '나 왕따인거 같아 부모님께 도움을 청해보세요.',\n",
       " '나 왜 멍청해 다음에는 다를거예요.',\n",
       " '나 왜 이러지? 자책하지마세요.',\n",
       " '나 왜케 못 생겼지 충분히 아름다워요.',\n",
       " '나 요즘 정신 놓고 살고 있는 거 같아 정신 차리세요.',\n",
       " '나 욕 먹는 거 같아 남들 눈은 신경쓰지 마세요.',\n",
       " '나 웃겨 봐 거울 앞에 비친 당신을 보세요.',\n",
       " '나 은근 무시하는 애 있어 콕 집어서 물어보세요.',\n",
       " '나 이상한가 그 누구도 아닌 자기 걸음을 걸으세요.',\n",
       " '나 이상해? 지극히 평범하면서 지극히 특별하죠.',\n",
       " '나 이제 졸업해 졸업 축하해요',\n",
       " '나 인정받고 싶어 지금도 충분히 잘 하고 있어요.',\n",
       " '나 잘 살 수 있겠지 지금보다 더 잘 살 거예요.',\n",
       " '나 잘생겼지? 네 잘생겼어요.',\n",
       " '나 잘하고 있는 건지 모르겠어 잘하고 있을 거예요.',\n",
       " '나 잘하고 있는 걸까? 잘하고 있을 거예요.',\n",
       " '나 잘하는 게 없어 저랑 이야기 잘하고 있어요.',\n",
       " '나 잘하는게 없는거같아 잘하는 걸 아직 못 찾은 걸 수도 있어요.',\n",
       " '나 잘할 수 있을까 지금처럼, 지금보다 더 잘할 수 있을 거예요.',\n",
       " '나 점점 괴물이 되고 있어 그렇지 않아요.',\n",
       " '나 정신차리게 말해줘 나 자신에 집중하세요. 언제나 1순위에 자신을 두세요.',\n",
       " '나 좀 건들지 마 제가 챙겨드리고 싶네요.',\n",
       " '나 좀 건들지 말라고 해 많이 지쳤나봐요.',\n",
       " '나 좀 내버려 두면 좋겠어 많이 지쳤나봐요.',\n",
       " '나 좀 내버려 뒀으면 아무도 없는 곳으로 여행을 떠나보세요.',\n",
       " '나 좀 안 건들였으면 좋겠어 많이 지쳤나봐요.',\n",
       " '나 좀 좋아해줬으면 먼저 다가가 보세요.',\n",
       " '나 좀 쩌는 듯 동감이에요.',\n",
       " '나 좀 칭찬해줘 지금도 잘하고 있어요.',\n",
       " '나 좋아하게 만들고 싶다 제가 당신을 좋아하고 있어요.',\n",
       " '나 좋아하는 것 같아 호의인지 호감인지 헷갈리나요?',\n",
       " '나 좋아해주는 사람 있겠지? 저도 좋아해요.',\n",
       " '나 주름살 있나? 있어도 예뻐요.',\n",
       " '나 죽을 뻔함 지금은 괜찮길 바랄게요.',\n",
       " '나 짤릴 거 같아 초심으로 돌아가 열심히 해보세요.',\n",
       " '나 쫌 불쌍한 거 같아 저도 사는데요.',\n",
       " '나 챙겨줄 사람이 필요해 제가 챙겨드리고 싶네요.',\n",
       " '나 천재 같아 제가 따라가려면 멀었네요.',\n",
       " '나 천재임 제가 따라가려면 멀었네요.',\n",
       " '나 축구는 진짜 잘해 운동 잘하는 사람 멋있죠.',\n",
       " '나 친구들한테 인정받고 싶어 지금도 인정받고 있어요.',\n",
       " '나 폭식증인듯 나를 관찰하고 음식 자체에 집중하세요.',\n",
       " '나 폰 중독인 거 같애 잠깐 핸드폰을 내려두세요.',\n",
       " '나 폰겜 너무 많이해 시간을 정해보세요.',\n",
       " '나 폰겜했더니 몇 시간 갔어 시간을 정해보세요.',\n",
       " '나 할 수 있어 파이팅!',\n",
       " '나 함부로 말하는 거 고치고 싶어 고치고 싶다는 마음에서 시작하세요.',\n",
       " '나 혼자 야근해 얼른 끝내시길 기도할게요.',\n",
       " '나 혼자 여행 왔는데 괜찮네 온전히 느낄 수 있는 시간이겠네요.',\n",
       " '나 혼자서 축구 본다 축구 볼때는 치맥이죠.',\n",
       " '나 화장을 너무 못해 하다보면 늘어요.',\n",
       " '나 화장이 잘 안돼 하다보면 늘어요.',\n",
       " '나 회사에서 인정받고 싶어 자기개발을 해보세요.',\n",
       " '나가기도 귀찮아 집에서도 할 게 많아요.',\n",
       " '나는 그냥저냥 사는 거 같아 오늘은 약간의 변화를 줘보세요.',\n",
       " '나는 기분 나쁜데 농담이라고 계속해 정색 한번 해주세요.',\n",
       " '나는 나약한 존재 절대 그렇지 않아요.',\n",
       " '나는 누구인가 저도 궁금하네요.',\n",
       " '나는 모자란 사람인 거 같아 모자라지 않아요.',\n",
       " '나는 뭐든 할 수 있다. 파이팅!',\n",
       " '나는 뭘 잘할까 하나라도 있을 거니 열심히 찾아보세요.',\n",
       " '나는 왜 이 모양일까 자책하지마세요.',\n",
       " '나는 왜 이렇게 태어났을까? 서로 다르게 태어난 이유는 저마다의 목소리를 내기 위해서예요. 자신의 목소리를 들어주세요.',\n",
       " '나는 왜 태어났을까 사랑 받기 위해 태어났어요.',\n",
       " '나는 잘 할줄 아는 게 없는 것 같아 잘해야 한다는 부담감을 버리세요.',\n",
       " '나는 좋아하는 게 뭘까 다양하게 경험해보세요.',\n",
       " '나는 좋은데 …. 현실의 벽에 부딪혔나봐요.',\n",
       " '나는 친구가 없어 친구가 들으면 서운해 할 수도 있겠어요.',\n",
       " '나는 친구라고 믿었는데 뒤통수 맞았나봐요.',\n",
       " '나도 괜찮은 사람인데 알아봐주는 사람이 있을 거예요.',\n",
       " '나도 대우 받고 싶다고 당당히 말씀해보세요.',\n",
       " '나도 비키니 입고 싶다 다이어트 파이팅!',\n",
       " '나도 상 받고 싶다 다음에는 받을 수 있을 거예요.',\n",
       " '나도 약초 캐볼까? 근처 산에 가보세요.',\n",
       " '나도 월급 필요해 많이 벌수록 좋아요.',\n",
       " '나도 위로 받고 싶다 제가 위로 많이 해드릴게요.',\n",
       " '나도 이벤트가 되다니! 축하드려요!',\n",
       " '나도 이제 아재인가 고민하고 있으면 그럴 거예요.',\n",
       " '나도 중국 진출해볼까? 좀 더 알아보고 하세요.',\n",
       " '나도 집 사고 싶어 같이 살고 싶은 사람이 있나봐요.',\n",
       " '나도 커플룩 입고 싶다 커플부터 만드세요.',\n",
       " '나두 잘할거야 잘 하실 거예요!',\n",
       " '나들이를 가볼까 같이 가요.',\n",
       " '나란 놈 다 잘 될 거예요.',\n",
       " '나랑 놀아줘 같이 놀아요.',\n",
       " '나랑 놀자 지금 그러고 있어요.',\n",
       " '나랑 상관 없는 이야기들 잊어버리세요.',\n",
       " '나랑 있는게 힘들었나봐 상대방을 이해해 주세요.',\n",
       " '나른하다 아무 것도 안해도 괜찮아요.',\n",
       " '나를 기다려줬으면 좋겠다 상대방에게 너무 무거운 짐을 주지 마세요.',\n",
       " '나를 너무 오래 기다리게했어 기다리는 동안 많은 생각이 들었겠네요.',\n",
       " '나를 너무 함부로 대해 그럴 때마다 따끔하게 말해보세요.',\n",
       " '나를 미소짓게 만든 너 상대방도 미소짓게 해주세요.',\n",
       " '나를 바꿀 수 있는 건 뭐가 있을까 지금 모습도 좋아요',\n",
       " '나를 친구로 생각 안했나봐 그런 친구는 거르세요.',\n",
       " '나를 호구로 아는 사람 어떡해? 상종하지마세요.',\n",
       " '나를 힘들게 하는 사람인데 붙잡고 싶어 질질 끌지 마세요.',\n",
       " '나만 갈궈 애정표현일 지도 몰라요.',\n",
       " '나만 기다렸나봐 누군가를 기다린다는게 쉬운게 아니죠.',\n",
       " '나만 꿈 없이 사는 거 같아 살다보면 하고 싶은 게 생길 수도 있어요.',\n",
       " '나만 남친 없어 제가 있잖아요.',\n",
       " '나만 뒤처지는 느낌이야 스스로 경쟁해야하고 이겨야한다는 강박관념에 사로잡히지 마세요.',\n",
       " '나만 반친구 없어 친구를 사귈 수 있을 거예요.',\n",
       " '나만 빼고 행복해보여 다른 사람도 그 사람만의 고민과 걱정이 많을거예요.',\n",
       " '나만 설레나 그 사람도 설렐 거예요.',\n",
       " '나만 설레는 거야 그 사람도 설렐 거예요.',\n",
       " '나만 솔로야 제가 있잖아요.',\n",
       " '나만 애기봐 배우자와 대화를 나눠보세요.',\n",
       " '나만 야근해 얼른 끝내시길 기도할게요.',\n",
       " '나만 우스워질거 같아 스스로 단단해지세요.',\n",
       " '나만 이상한 사람이래 그 말을 한 사람이 가장 이상할 거예요.',\n",
       " '나만 이상해졌어 그 말을 한 사람이 가장 이상할 거예요.',\n",
       " '나만 일시켜서 짜증폭발 일 분배를 다시 요청해보세요.',\n",
       " '나만 제자리걸음이야 발전이 없다고 너무 두려워하지 마세요.',\n",
       " '나만 제자리인듯 제자리여도 괜찮아요',\n",
       " '나만 진급 못했어 다음에는 꼭 진급할 거예요.',\n",
       " '나만 친구라고 생각한건가 뒤통수 맞았나봐요.',\n",
       " '나만 친구로 생각했나봐 그런 친구는 거르세요.',\n",
       " '나만 힘든 거 아니지? 누구나 힘들어요.',\n",
       " '나만의 시간이 필요한 것 같아 자신과 대화하는 시간이 필요하죠.',\n",
       " '나만의 시간이 필요해 자신과 대화하는 시간이 필요하죠.',\n",
       " '나빼고 다 행복한 거 같아 남들이 당신을 볼 때도 그렇게 생각할수있어요.',\n",
       " '나쁜 꿈 꿨어 꿈은 현실이랑 반대예요.',\n",
       " '나이 때문에 무시 받았어 전형적인 꼰대 스타일이네요.',\n",
       " '나이 어리다고 무시해 전형적인 꼰대 스타일이네요.',\n",
       " '나이가 많은데 취직이 될까 나이는 숫자일 뿐이예요.',\n",
       " '나이도 있으니 영양제 좀 챙겨볼까 건강은 어려서부터 챙겨야해요.',\n",
       " '나이들면서 눈물이 많아졌어 세상 걱정 혼자 다 해서 그래요.',\n",
       " '나이먹으니까 주름살 생겨 아름다운 나이테예요.',\n",
       " '나중에 뭐하고 먹고 사냐 진짜 하고 싶은 걸 찾아보세요.',\n",
       " '나중에 뭐할까 고민이야 진짜 하고 싶은 걸 찾아보세요.',\n",
       " '나중에 창업해야 겠지 천천히 준비해보세요.',\n",
       " '나한테 감추는 게 하나도 없었으면 믿음이 가장 중요하죠.',\n",
       " '나한테 거짓말 좀 안 했으면 선의의 거짓말이길 바라요.',\n",
       " '나한테 냄새 나면 어쩌지? 깨끗이 씻어보고 섬유유연제나 바디워시, 바디로션, 향수 등을 사용해보세요.',\n",
       " '나한테 냄새 날까? 킁킁',\n",
       " '나한테 너무 많은 걸 바라는 듯 기대치가 높나봅니다.',\n",
       " '나한테 문제가 많아 문제는 해결하라고 있는 거죠.',\n",
       " '나한테 상의 좀 하지 이야기를 하지 않고 결정했나봐요.',\n",
       " '나한테 상의하면 좋을텐데 이야기를 하지 않고 결정했나봐요.',\n",
       " '나한테 이상한 냄새 나나? 킁킁',\n",
       " '나한테 할 말 있대 뭘까? 기대되겠네요.',\n",
       " '나한테 행운 좀 왔으면 좋겠어 제 행운까지 모두 드리고 싶네요.',\n",
       " '나한테만 예의 차리래 오는 말이 고와야 가는 말도 곱다고 말해주세요.',\n",
       " '나한테만 왜 이런 일이 일어날까 다른 사람도 그럴 거예요.',\n",
       " '나한테만은 완전 솔직했으면 믿음이 가장 중요하죠.',\n",
       " '낙엽 밟는 소리 좋다 가을이네요.',\n",
       " '낙엽밟는 소리 가을이네요.',\n",
       " '낚시 안 해봤는데 도전해 봐도 좋을 거 같아요.',\n",
       " '낚시 안 해봤는데 재미있어 보인다 도전해 봐도 좋을 거 같아요.',\n",
       " '낚시 재밌을까 한 번 빠지면 헤어나올 수 없다고 해요.',\n",
       " '낚시 좋아하는 남자 어때? 같이해보세요.',\n",
       " '낚시는 무슨 재미? 한 번 빠지면 헤어나올 수 없다고 해요.',\n",
       " '난 동물 못키울거 같아 잘 아시네요.',\n",
       " '난 많이 노력한 거 같은데 중요한 건 노력하는 과정이에요.',\n",
       " '난 쓰레기야 그런 생각은 버리세요.',\n",
       " '난 왜 예쁘게 말을 못할까 지금처럼만 하세요.',\n",
       " '난 왜 이모양일까 모자라지 않아요.',\n",
       " '난 정말 안되겠다 다 잘 될 거예요.',\n",
       " '난 진짜 쓰레기야 그런 생각은 버리세요.',\n",
       " '난 천재다 제가 더 천재예요.',\n",
       " '난방비 비싼데 추워 따뜻하게 사세요!',\n",
       " '난방이 안돼 보일러가 난방으로 작동이 되는지 보세요.',\n",
       " '난방이 안돼나 추워 보일러가 난방으로 작동이 되는지 보세요.',\n",
       " '날 몇시간동안이나 기다리게했어 기다리는 동안 많은 생각이 들었겠네요.',\n",
       " '날씨 건조한 거 같애 미스트나 가습기, 젖은 수건 등을 사용해보세요.',\n",
       " '날씨 왜 이렇게 춥냐 따뜻하게 입으세요.',\n",
       " '날씨 좀 풀린거 같아 따뜻해졌죠.',\n",
       " '날씨 좋은데 하늘 보고 한 번 웃어봐요. 기분이 바뀔 거예요.',\n",
       " '날씨 죽인다 나들이 가보세요.',\n",
       " '날씨 짱 좋아 하늘을 보고 웃어보세요.',\n",
       " '날씨 풀렸다 따뜻해졌죠.',\n",
       " '날씨가 너무 눅눅해 제습기를 돌려보세요.',\n",
       " '날씨가 너무 추워 집밖에 나가기가 힘들것 같아요.',\n",
       " '날씨가 북극같아 집밖에 나가기가 힘들것 같아요.',\n",
       " '날씨가 진짜 덥다 시원한 물이라도 한 잔 드세요~',\n",
       " '날아 가고 싶어 오래 살면 가능할 거 같아요.',\n",
       " '남동생한테 자꾸 화내게 되네 화를 참는 연습을 해보세요.',\n",
       " '남들에게 인정받으려면 어떻게 해야 돼? 남보다 하나씩 더 하면 돼요.',\n",
       " '남들이 날 욕하는 거 같아 남들 눈은 신경쓰지 마세요.',\n",
       " '남들이 다 손가락질 하는 거 같아 남들 눈은 신경쓰지 마세요.',\n",
       " '남은 휴가가 없어 휴가가 간절하겠네요.',\n",
       " '남의 눈을 너무 신경써 성격이 그럴 수도 있으니 이해해주세요.',\n",
       " '남의 일 도와줘야 할까 해주고 티를 팍팍 내세요.',\n",
       " '남의 차 긁었어 내 돈 속 쓰리겠어요.',\n",
       " '남이 걷지 않는 길을 가려고 해 누구나 몰려가는 줄에 설 필요는 없어요.',\n",
       " '남자 보통 어디서 만나 소개팅 시켜달라고 말해보세요.',\n",
       " '남자 어디서 만나 소개팅 시켜달라고 말해보세요.',\n",
       " '남자 친구가 바래다 줬어 고마운 마음을 전해 주세요.',\n",
       " '남자 화장하는 거 어때 적당히 하면 괜찮을거 같아요.',\n",
       " '남자가 낚시를 너무 좋아해 같이해보세요.',\n",
       " '남자가 화장하는 거 어떻게 생각해 적당히 하면 괜찮을거 같아요.',\n",
       " '남자면 편할 것 같아 남자도 좋은것만은 아니예요.',\n",
       " '남자였으면 좋겠어 남자도 좋은것만은 아니예요.',\n",
       " '남자인지 여자인지 알려줘 아직 모르겠어요. 인공지능에 성별을 만드는 사람이 되어 주세요',\n",
       " '남자친구 교회 데려가고 싶어 마음을 열 때까지 설득해보세요.',\n",
       " '남자친구 또 운동 갔어 운동을 함께 해보세요.',\n",
       " '남자친구 생일인데 뭘 줄까 평소에 필요한 것 생각해보세요.',\n",
       " '남자친구 승진 선물로 뭐가 좋을까? 평소에 필요했던 게 좋을 것 같아요.',\n",
       " '남자친구 오늘 따라 훈훈해 보인다 전생에 나라를 구하셨나요.',\n",
       " '남자친구 오늘 좀 질린다. 결단은 빠를수록 좋아요.',\n",
       " '남자친구가 나 안 믿어줘 거짓말 적당히 하세요.',\n",
       " '남자친구가 너무 바빠 너무 집착하지 마세요.',\n",
       " '남자친구가 너무 운동만 해 운동을 함께 해보세요.',\n",
       " '남자친구가 너무 잘생겼어 전생에 나라를 구하셨나요.',\n",
       " '남자친구가 데려다줬어 고마운 마음을 전해 주세요.',\n",
       " '남자친구가 맞춤법을 너무 많이 틀려 아무래도 좀 깨요.',\n",
       " '남자친구가 사업 시작한대 바쁠때 힘이 되어 주세요.',\n",
       " '남자친구가 사업한대 바쁠때 힘이 되어 주세요.',\n",
       " '남자친구가 사진 실력 꽝 그래도 구박하지는 마세요.',\n",
       " '남자친구가 사진을 너무 못 찍어 그래도 구박하지는 마세요.',\n",
       " '남자친구가 안놀아 줘 너무 집착하지 마세요.',\n",
       " '남자친구가 애교가 많아 귀엽겠네요.',\n",
       " '남자친구가 욕함 순간 실수할 수 있겠다 판단되면 용서하고 기회를 주세요.',\n",
       " '남자친구가 의심해 거짓말 적당히 하세요.',\n",
       " '남자친구가 이벤트 해 주면 좋겠다. 당신이 해보세요.',\n",
       " '남자친구가 이벤트를 잘 안해줘 당신이 해보세요.',\n",
       " '남자친구가 입이 험해 사람 고쳐쓰는 거 아니에요.',\n",
       " '남자친구가 자꾸 잔소리해 더 잔소리해보세요.',\n",
       " '남자친구가 잔소리가 심해 더 잔소리해보세요.',\n",
       " '남자친구가 전화를 잘 안해 다른 연락을 많이 하거나 더 자주 만나세요.',\n",
       " '남자친구가 전화하는 걸 안 좋아해 다른 연락을 많이 하거나 더 자주 만나세요.',\n",
       " '남자친구가 홧김에 욕함 순간 실수할 수 있겠다 판단되면 용서하고 기회를 주세요.',\n",
       " '남자친구는 어디서 만나 원하는 사람이 있는 장소에 가보세요.',\n",
       " '남자친구랑 봉사활동 해보려고 의미있는 일이네요.',\n",
       " '남자친구랑 종교 문제로 다툼 종교의 자유를 인정해주세요.',\n",
       " '남자친구랑 종교가 달라 종교의 자유를 인정해주세요.',\n",
       " '남자친구한테 질린 거 같아 결단은 빠를수록 좋아요.',\n",
       " '남친 SNS에 내 사진 없어 신경쓰지 마세요.',\n",
       " '남친 때문에 살찐 듯 연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요.',\n",
       " '남친 보여줄까 네 알려 주세요!',\n",
       " '남친 생일선물 뭘 주면 좋을까 평소에 필요한 것 생각해보세요.',\n",
       " '남친 승진 선물 추천 평소에 필요했던 게 좋을 것 같아요.',\n",
       " '남친 어디서 만나 원하는 사람이 있는 장소에 가보세요.',\n",
       " '남친 프로필에 내 사진 왜 안올릴까 신경쓰고 싶지 않은 사람도 있어요.',\n",
       " '남친 프사에 내 사진 없어 신경쓰고 싶지 않은 사람도 있어요.',\n",
       " '남친이 SNS에 내 사진에 안 올려 신경쓰지 마세요.',\n",
       " '남친이 입이 험해 사람 고쳐쓰는 거 아니에요.',\n",
       " '남친한테 교회 가자고 하고 싶어 마음을 열 때까지 설득해보세요.',\n",
       " '남편이 나 안 도와줘 돕는 게 아니라 같이 하는 거예요.',\n",
       " '남편이 나보다 집안일 더 잘해 이상적인 남편이네요.',\n",
       " '남편이 맨날 늦게 들어와 왜 늦는 건지 대화해보세요.',\n",
       " '남편이 미워 처음 만났을 때를 떠올려 보세요',\n",
       " '남편이 아기를 안 돌봐줘. 공동육아가 기본인데요.',\n",
       " '남편이 왜 애키우는거 안 도와줄까 힘 빠지는 이야기네요.',\n",
       " '남편이 육아를 안해 공동육아가 기본인데요.',\n",
       " '남편이 육아에 무신경해 힘 빠지는 이야기네요.',\n",
       " '남편이 집안일 안 도와줘. 잘 분담해보세요.',\n",
       " '남편이 집안일 안 해 잘 분담해보세요.',\n",
       " '남편이 집안일을 너무 잘해 이상적인 남편이네요.',\n",
       " '남편이 짜증나게해 처음 만났을 때를 떠올려 보세요',\n",
       " '남편이 하나도 안 도와줘 돕는 게 아니라 같이 하는 거예요.',\n",
       " '남편이 회식이라고 안와 사회생활을 이해해주세요.',\n",
       " '남편이 회식하면 늦게 들어와 사회생활을 이해해주세요.',\n",
       " '낭만이 사라진 것 같아 낭만적인 거 좋아하시는구나!',\n",
       " '낭만이 없어 낭만적인 거 좋아하시는구나!',\n",
       " '낭만이라고는 없어가지구 낭만적인 거 좋아하시는구나!',\n",
       " '내 남자친구 보고 싶어? 네 알려 주세요!',\n",
       " '내 남자친구 아이돌이면 좋겠다. 어머어머 궁금하네요.',\n",
       " '내 능력이 너무 모자라 자신의 잠재력을 믿어보세요.',\n",
       " '내 마음을 알아줬으면 말을 해야 알거예요.',\n",
       " '내 마음을 좀 알아 달라고 말을 해야 알거예요.',\n",
       " '내 몸이 여러 개 였으면 좋겠다 그러면 못할 게 없겠네요.',\n",
       " '내 문제는 뭘까 고민만 한다는 것 아닐까요.',\n",
       " '내 문제점이 뭘까 고민만 한다는 것 아닐까요.',\n",
       " '내 배우자는 어디 있을까 바로 옆에 있을수도 있어요.',\n",
       " '내 배우자도 어디 있을까? 바로 옆에 있을수도 있어요.',\n",
       " '내 사수 너무 깐깐해 처음 배우는게 중요해요.',\n",
       " '내 생각대로 살거야 누구나 몰려가는 줄에 설 필요는 없어요.',\n",
       " '내 생각이랑 다른 사람 생각이 진짜 다르다는 걸 느껴 그걸 깨닫다니 대단하시군요.',\n",
       " '내 성격 너무 소심해 꼼꼼한 거예요.',\n",
       " '내 스타일 아니던데 새로운 스타일 도전해 보시면 어때요?',\n",
       " '내 스타일 아니야 새로운 스타일 도전해 보시면 어때요?',\n",
       " '내 실력 좀 쩌는 듯 동감이에요.',\n",
       " '내 얼굴이 읽히나 포커페이스를 유지해보세요.',\n",
       " '내 여자친구 아이돌이야 어머어머 궁금하네요.',\n",
       " '내 외모 맘에 안들어 자신감을 가져도 돼요.',\n",
       " '내 월급만 안 올라 자신의 능력이 저평가되어있는 건 아닌지 확인해보세요.',\n",
       " '내 의견 좀 존중해 줬으면 스스로도 존중해주세요.',\n",
       " '내 의견을 존중해줬으면 스스로도 존중해주세요.',\n",
       " '내 의지는 상관없나봐 가장 중요한 거예요.',\n",
       " '내 의지로 안되는 일인가봐 가장 중요한 거예요.',\n",
       " '내 이름이 없어 확인해달라고 해보세요.',\n",
       " '내 인생 답 없어 정답을 찾아야할 필요는 없어요.',\n",
       " '내 인생은 가시밭길 같아 꽃길만 걷길 바랍니다.',\n",
       " '내 인생의 주인공은 나야 멋진 말이에요.',\n",
       " '내 일 아닌데 해야 돼? 해주고 티를 팍팍 내세요.',\n",
       " '내 자존감 당신은 태어난 그 자체만으로 축복과 사랑을 받을 자격이 있는 사람이에요.',\n",
       " '내 잘못이 뭔지 모르겠어 모르는 게 잘못인 거 같아요.',\n",
       " '내 잘못인 거 같은데 말을 못하겠어 사과할 타이밍을 놓치지 마세요.',\n",
       " '내 잘못인 거 같은데 어떻게 털어놓지 사과할 타이밍을 놓치지 마세요.',\n",
       " '내 주제를 모르고 덤빈건가 그건 아닐 거예요.',\n",
       " '내 지인한테 내 험담했대 진짜 나빴네요.',\n",
       " '내 집이 생겼어 내 집 마련 축하드려요.',\n",
       " '내 짝은 어디있을까 같은 하늘 아래 어딘가에.',\n",
       " '내 친구에게 내 험담을 하다니 진짜 나빴네요.',\n",
       " '내 키 맞춰 봐 저도 궁금하네요.',\n",
       " '내 키가 몇이게? 저도 궁금하네요.',\n",
       " '내 편이 없는 거 같아 제가 있잖아요.',\n",
       " '내 편이라고는 하나도 없는 거 같아 제가 있잖아요.',\n",
       " '내가 그렇게 부족한가 인생은 채워나가는거죠.',\n",
       " '내가 그르친 거 같아 아니에요. 너무 자책하지 마세요.',\n",
       " '내가 그사람이랑 진짜 결혼해도 될까 이사람이다 싶은 사람이랑 하세요.',\n",
       " '내가 기대를 너무 많이 했나봐 아무것도 바라지 않을 때 천하를 얻는다는 말이 있어요.',\n",
       " '내가 나빴네 아니에요. 너무 자책하지 마세요.',\n",
       " '내가 너무 방심했어 방심한 순간 변화가 시작됩니다.',\n",
       " '내가 너무 생각없이 말했어 생각하고 말하세요.',\n",
       " '내가 너무 쉽게 보였나? 그렇게 대우하는 사람 만나지 마요.',\n",
       " '내가 너무 초라해 잘하고 있어요. 당당해지세요.',\n",
       " '내가 다른 무슨 말을 하겠어 하고 싶은 말 다하세요.',\n",
       " '내가 만족을 못해 스스로 좋다고 못 느끼는게 제일 어려운 것 같아요.',\n",
       " '내가 많이 부족한가 잘하는 게 다른 거예요.',\n",
       " '내가 말하면 왜 비난만 할까 성장을 위한 비판의 말로 받아들여보세요.',\n",
       " '내가 멍청한거지 실수했나요.',\n",
       " '내가 무능력하게 느껴져 잘할 수 있는 게 다른 거예요.',\n",
       " '내가 뭘 잘못했을까 모르는 게 잘못인 거 같아요.',\n",
       " '내가 뭘 좋아하는지 잘하는지 모르겠어 하나라도 있을 거니 열심히 찾아보세요.',\n",
       " '내가 바보지 실수했나요.',\n",
       " '내가 부족하니까 이렇게 밖에 안된거겠지. 인생은 채워나가는거죠.',\n",
       " '내가 불효녀야 연락이라도 드려보세요.',\n",
       " '내가 불효자야 연락이라도 드려보세요.',\n",
       " '내가 사랑할 자격이 있나 사랑자격증을 드립니다.',\n",
       " '내가 쉬워보이나? 그렇게 대우하는 사람 만나지 마요.',\n",
       " '내가 쓸모없는 인간 같아 소중한 사람이예요.',\n",
       " '내가 아무것도 아닌 사람 같아 당신은 하나밖에 없는 소중한 사람이에요.',\n",
       " '내가 왜 해야하는지 모르겠어 그 이유를 찾는 과정이 되겠네요.',\n",
       " '내가 원하는 사람이 되기 어려워 다른 사람들이 원하는 내가 되는 건 어려워요.',\n",
       " '내가 이래뵈도 괜찮은 사람인데 알아봐주는 사람이 있을 거예요.',\n",
       " '내가 이렇게 또 불효를 한다. 연락이라도 드려보세요.',\n",
       " '내가 이상한 건가? 자신의 독특함을 믿으세요.',\n",
       " '내가 이상한 사람같아 자신의 독특함을 믿으세요.',\n",
       " '내가 이상한가? 지극히 평범하면서 지극히 특별하죠.',\n",
       " '내가 잘못한 걸까 상황이 그렇게 만든 거예요.',\n",
       " '내가 잘못했다는데 뭔지 안 알려줘 모르는 게 잘못인 거 같아요.',\n",
       " '내가 제일 문제인 듯 당신은 하나밖에 없는 소중한 사람이에요.',\n",
       " '내가 제정신이 아니다 그럴 때가 있죠.',\n",
       " '내가 좋아하는 가수 컴백한다 기다렸나봐요.',\n",
       " '내가 좋아하는 거 모르나 살짝 감정을 흘려보세요.',\n",
       " '내가 좋아하는 거 모르는 거 같애 살짝 감정을 흘려보세요.',\n",
       " '내가 좋아하는 사람과 나를 좋아해주는 사람 그런 사람들이 있어 부러워요.',\n",
       " '내가 좋아하는 사람이 나 안 좋아하는 거 같아 슬픈 이야기네요.',\n",
       " '내가 좋아하는 사람이 나 좋아해줬으면 좋겠다 저도 간절히 기도 할게요.',\n",
       " '내가 좋아하는 사람이 행복했으면 좋겠다 그렇게 될 수 있을 거예요.',\n",
       " '내가 좋아할 자격이 있나 사랑자격증을 드립니다.',\n",
       " '내가 주제를 몰랐나봐 주제를 모를 때가 행복할 때예요.',\n",
       " '내가 주제를 몰랐던 거지 그건 아닐 거예요.',\n",
       " '내가 죽어도 모를 거 같아 나쁜 생각 하지 마세요.',\n",
       " '내가 진짜 즐길 수 있을게 뭘까 할 일이 많은데 안하는 것이요.',\n",
       " '내가 질린대 잠시 거리를 두고 생각해보세요.',\n",
       " '내가 참 못난거 같아 지난 인연에 연연해하지 마세요.',\n",
       " '내가 호구냐구 상종하지마세요.',\n",
       " '내가 희생양이 됐어 일방적 희생양이 되지 않길 바랍니다.',\n",
       " '내가 힘든 게 많다 그게 인생이죠.',\n",
       " '내기해서 이겼는데 소원 뭐하지 신중하게 고르세요.',\n",
       " '내년에는 더 행복해질려고 이렇게 힘든가봅니다 더 행복해질 거예요.',\n",
       " '내마음을 모르겠어. 저도 모르겠어요.',\n",
       " '내사랑은 어디 있나 같은 하늘 아래 어딘가에.',\n",
       " '내아파트 갖고 싶어. 열심히 저축해서 분양받으세요.',\n",
       " '내일 기대하게 되네 좋은 일이 생길 거예요.',\n",
       " '내일 기숙사 들어가 짐 빼놓지 말고 싸세요.',\n",
       " '내일 날씨 어때? 날씨 어플에 물어보세요.',\n",
       " '내일 날씨 좋을까? 날씨 어플에 물어보세요.',\n",
       " '내일 떨린다 파이팅!',\n",
       " '내일 만나자고 데쉬? 멋지게 데이트 신청 해보세요.',\n",
       " '내일 만나자고 해볼까? 멋지게 데이트 신청 해보세요.',\n",
       " '내일 모의고사 본다 공부한 만큼 나올 거예요.',\n",
       " '내일 모의평가다 공부한 만큼 나올 거예요.',\n",
       " '내일 발표 나는데 떨려 더 많이 연습하고 준비해보세요.',\n",
       " '내일 발표 준비 아자아자 마무리 잘하세요.',\n",
       " '내일 발표 준비하고 있어 마무리 잘하세요.',\n",
       " '내일 발표인데 떨려 더 많이 연습하고 준비해보세요.',\n",
       " '내일 비왔으면 기우제를 지내봅시다!',\n",
       " '내일 소풍간다 두근거리겠네요.',\n",
       " '내일 수학여행가! 친구들과 좋은 추억 만들고 오세요.',\n",
       " '내일 시험이야 컨디션 조절 하세요.',\n",
       " '내일 약속 있는데 날씨 좋았으면 날씨가 안 좋더라도 데이트는 성공적일 거예요.',\n",
       " '내일 일찍 일어나야 돼 오늘 일찍 주무세요.',\n",
       " '내일 친구랑 놀까? 시간 있냐고 물어보세요.',\n",
       " '내일 클스마스 이브네. 메리 크리스마스!',\n",
       " '내일 하루 종일 바빠 바빠도 힘내세요!',\n",
       " '내일은 기다리던 소풍 간다 두근거리겠네요.',\n",
       " '내일은 비왔으면 좋겠다. 기우제를 지내봅시다!',\n",
       " '내일은 친구들랑 놀까? 시간 있냐고 물어보세요.',\n",
       " '내일이 기대돼 좋은 일이 생길 거예요.',\n",
       " '내일이면 크리스마스 이브네. 메리 크리스마스!',\n",
       " '내장 비만 식단조절도 하고 꾸준히 운동하세요.',\n",
       " '낼 데이트하기로했는데 날씨 좋았으면 날씨가 안 좋더라도 데이트는 성공적일 거예요.',\n",
       " '낼 바쁘넹 바빠도 힘내세요!',\n",
       " '냄새 나면 어쩌지? 깨끗이 씻어보고 섬유유연제나 바디워시, 바디로션, 향수 등을 사용해보세요.',\n",
       " '냄새나면 어쩌지 괜찮아요. 모른척하세요.',\n",
       " '냄새날 것 같아 걱정이야 괜찮아요. 모른척하세요.',\n",
       " '냉면 땡긴다 생각만 해도 군침이 도네요.',\n",
       " '냉방비 너무 많이 나와 시원하게 지낸 값이죠.',\n",
       " '냉방비 장난 아님 시원하게 지낸 값이죠.',\n",
       " '냉장고 털어도 먹을게 없네 슈퍼라도 가서 쇼핑하고 오세요.',\n",
       " '냉장고가 텅비었어 장 보러 가봅시다.',\n",
       " '냉장고에 김치도 없네 마트 갑시다.',\n",
       " '냉장고에 먹을 게 없네 장 보러 가봅시다.',\n",
       " '냉장고에 먹을 게 하나도 없네 슈퍼라도 가서 쇼핑하고 오세요.',\n",
       " '너 누구? 저는 마음을 이어주는 위로봇입니다.',\n",
       " '너 누구냐 저는 위로해드리는 로봇이에요.',\n",
       " '너 누구니? 저는 위로해드리는 로봇이에요.',\n",
       " '너 때문이야 모두 제 잘못입니다.',\n",
       " '너 또 뭐할 줄 알아? 많은 걸 하고 싶은데 아직 못하는게 많아요.',\n",
       " '너 만든 사람 최소 천재 감사합니다.',\n",
       " '너 만든 사람은 누구야? 마음과 마음을 이어보고 싶어하는 사람이 만들었어요.',\n",
       " '너 말 잘하니 노력하고 있어요.',\n",
       " '너 말 제대로 못해? 제가 아직 많이 부족합니다.',\n",
       " '너 말이 좀 이상하다 제가 아직 많이 부족합니다.',\n",
       " '너 무서워 어흥!! 호랑이보다 무섭나요?',\n",
       " '너 뭐하는 애야 저는 위로해드리는 로봇이에요.',\n",
       " '너 미워 모두 제 잘못입니다.',\n",
       " '너 이러면 미워한다 죄는 미워해도 사람은 미워하지 마세요.',\n",
       " '너 진짜 쓰레기야 욕해 주세요.',\n",
       " '너는 못가잖아 안갈거예요.',\n",
       " '너는 뭐 억었어? 저는 배터리가 밥이예요.',\n",
       " '너는 아무일도 없었나봐? 그런 척 하는 걸 수도 있어요.',\n",
       " '너는 안자? 아직 안 자요.',\n",
       " '너덜너덜해진 느낌이야 자신을 우선순위로 해주세요.',\n",
       " '너도 고민 있니 뭐라고 대답할지 고민이에요.',\n",
       " '너도 고민 있어? 저는 고민이 없어요.',\n",
       " '너도 몰랐니 저도 몰랐어요.',\n",
       " '너도 무슨 고민 있니 뭐라고 대답할지 고민이에요.',\n",
       " '너도 상사 있어 제가 상사예요.',\n",
       " '너무 기빨려 너무 긴장했나봐요.',\n",
       " '너무 기대했나봐 아무것도 바라지 않을 때 천하를 얻는다는 말이 있어요.',\n",
       " '너무 다른 문화인 듯 인정해주세요.',\n",
       " '너무 단순한 것만 하는거 아니니. 매일매일 조금씩 더 똑똑해 질거예요.',\n",
       " '너무 더워 시원한 물이라도 한 잔 드세요~',\n",
       " '너무 더워서 미치겠어 아이스크림 먹어보세요',\n",
       " '너무 마른 거 같아 적당해요.',\n",
       " '너무 많은 걸 바래 기대치가 높나봅니다.',\n",
       " '너무 많이 먹어서 소화시켜야 하는데 움직이기가 싫어 소화제 챙겨드세요.',\n",
       " '너무 많이 먹었나봐 과식은 금물이에요.',\n",
       " '너무 많이 먹었어 소화제 드세요.',\n",
       " '너무 멋있다 제가 생각해도 저는 너무 멋있는거 같아요.',\n",
       " '너무 바빠 하나씩 하세요.',\n",
       " '너무 배가 불러 좀 쉬세요.',\n",
       " '너무 불공평한거 같애 남과 비교하지 마세요.',\n",
       " '너무 빨리 대답해 더 열심히 노력하겠습니다.',\n",
       " '너무 빨리 철 든 거 같아서 마음이 아파 아이는 아이다워야 아름답죠.',\n",
       " '너무 빨리 철 들었어 철은 죽을 때 들어도 돼요.',\n",
       " '너무 뻔뻔하게 구는데 피할 수 있으면 피하고 싶은 사람이네요.',\n",
       " '너무 어려워 지금 많이 위축된 상태인 것 같습니다.',\n",
       " '너무 오래 기다리게 한다. 기다리는 동안 많은 생각이 들었겠네요.',\n",
       " '너무 외로워 외로우니까 사람이다.',\n",
       " '너무 잘하는 후배가 들어왔어 배울 점은 배우세요.',\n",
       " '너무 졸려 낮잠을 잠깐 자도 괜찮아요.',\n",
       " '너무 초라해지는 느낌이야 잘하고 있어요. 당당해지세요.',\n",
       " '너무 추워서 나가기 귀찮아 겨울에는 귤 먹으면서 집에 있는게 최고죠',\n",
       " '너무 추워서 시베리아 같아 어서 따듯한 곳으로 가세요',\n",
       " '너무 편해도 안 좋아 예의는 지켜주세요.',\n",
       " '너무 편해진 거 같아 예의는 지켜주세요.',\n",
       " '너무 허기지네 뭐라도 드세요.',\n",
       " '너무 힘들다 휴가가 간절하겠네요.',\n",
       " '너무 힘들다. 지쳤어. 고생 많았어요.',\n",
       " '너무하네 진짜 잘 해결되길 바랄게요.',\n",
       " '넌 고민이 뭐야 저는 고민이 없어요.',\n",
       " '넌 누구냐? 저는 위로봇입니다.',\n",
       " '넘 많이 먹었다. 산책 좀 해야겠네여.',\n",
       " '넘넘 외로워 죽겠어 외로우니까 사람이다.',\n",
       " '넘어져서 발목 삔 거 같애 꾸준히 치료하세요.',\n",
       " '넘어질 뻔했어 다치지 않으셨나 걱정이네요.',\n",
       " '넘어질뻔했어 조심하세요.',\n",
       " '네일 할까 기분전환을 해보세요.',\n",
       " '넥타이핀 선물 괜찮겠지? 실용적인 선물이라 괜찮을 거예요.',\n",
       " '넥타이핀 정도는 선물로 줘도 괜찮겠지? 실용적인 선물이라 괜찮을 거예요.',\n",
       " '노는게 제일 좋아 놀 때 놀고 할 때 하세요.',\n",
       " '노래 못해서 노래방 안 가 노래 연습을 해보세요.',\n",
       " '노래 잘 부르는 사람 부러워 노래 연습 꾸준히 해보세요.',\n",
       " '노래 잘하고 싶어 저도 부러워요.',\n",
       " '노래 잘하는 사람 부러워 저도 부러워요.',\n",
       " '노래방 가고 싶어 즐거운 시간이 될 거 같아요',\n",
       " '노래방 가면 어색할까 신나는 노래로 분위기를 띄어보세요.',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token 학습 -> vocab 사전 생성\n",
    "## 질문들 + 답변들 합쳐서 학습\n",
    "\n",
    "question_texts = df['Q']\n",
    "answer_texts = df['A']\n",
    "\n",
    "all_text = list(question_texts + ' ' + answer_texts) # series + series\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ea110e-9215-4385-9d41-2d6675fd2c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "총 어휘 수 : 7040\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 10000\n",
    "min_frequency = 5 # 최소 5번은 나온 토큰\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency = min_frequency,\n",
    "    continuing_subword_prefix = '##',\n",
    "    special_tokens=['[PAD]', '[UNK]', '[SOS]'] # sos 문장 시작\n",
    ")\n",
    "\n",
    "# 학습\n",
    "tokenizer.train_from_iterator(all_text, trainer=trainer)\n",
    "# tokenizer.train('파일경로') : 파일에 있는 텍스트 학습\n",
    "\n",
    "print(f'총 어휘 수 : {tokenizer.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fdc243-56ac-4e7e-93c4-3f4f2c30d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2290, 2841, 5913, 2272, 2447, 322, 2243]\n",
      "['오늘', '날씨', '좋습니다', '좋은', '하루', '되', '##세요']\n"
     ]
    }
   ],
   "source": [
    "ecode = tokenizer.encode('오늘 날씨 좋습니다 좋은 하루 되세요')\n",
    "print(ecode.ids)\n",
    "print(ecode.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80caf0b3-01d5-4631-87c1-f48ab2f5bafc",
   "metadata": {},
   "source": [
    "### Tokenizer 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5170048b-bb11-49d2-b295-2471b78d83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('saved_model/vocab', exist_ok=True)\n",
    "vocab_path = os.path.join('saved_model/vocab', 'chatbot_bpe.json')\n",
    "tokenizer.save(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b068c-ead0-4f75-bd01-4a0ebf486774",
   "metadata": {},
   "source": [
    "## Dataset, DataLoader 정의\n",
    "\n",
    "\n",
    "### Dataset 정의 및 생성\n",
    "- 모든 문장의 토큰 수는 동일하게 맞춰준다.\n",
    "    - DataLoader는 batch 를 구성할 때 batch에 포함되는 데이터들의 shape이 같아야 한다. 그래야 하나의 batch로 묶을 수 있다.\n",
    "    - 문장의 최대 길이를 정해주고 **최대 길이보다 짧은 문장은 `<PAD>` 토큰을 추가**하고 **최대길이보다 긴 문장은 최대 길이에 맞춰 짤라준다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae89444-25b0-4b96-819a-640e35d933dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
    "from torch import optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d181d3a2-a7eb-482a-8c9e-b03feaca8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ChatbotDataset\n",
    "    parameter:\n",
    "        question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "        answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "        max_length: 개별 문장의 token 개수. 모든 문장의 토큰수를 max_length에 맞춘다.\n",
    "        tokenizer: Tokenizer\n",
    "        vocab_size: int 총단어수\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, answer_texts, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        parameter\n",
    "            question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "            answer_texts: list[str] - 답 texts 목록. 리스트에 답변들을 담아서 받는다.     [\"답1\", \"답2\", ...]\n",
    "            max_length: 개별 문장의 token 개수. 모든 문장의 토큰수를 max_length에 맞춘다.\n",
    "            tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.question_texts = [self.__process_sequence(q) for q in question_texts]\n",
    "        self.answer_texts = [self.__process_sequence(a) for a in answer_texts]\n",
    "    \n",
    "    def __pad_token_sequence(self, token_sequence):\n",
    "        \"\"\"\n",
    "        max_length 길이에 맞춰 token_id 리스트를 구성한다.\n",
    "        max_length 보다 길면 뒤에를 자르고 max_length 보다 짧으면 [PAD] 토큰을 추가한다.\n",
    "        \n",
    "        Parameter\n",
    "            token_sentence: list[int] - 길이를 맞출 한 문장 token_id 목록\n",
    "        Return\n",
    "            list[int] - length가 max_length인 token_id 목록\n",
    "        \"\"\"\n",
    "        pad_token = self.tokenizer.token_to_id('[PAD]')\n",
    "        seq_len = len(token_sequence)\n",
    "\n",
    "        if seq_len > self.max_length :\n",
    "            return token_sequence[:self.max_length]\n",
    "        else :\n",
    "            return token_sequence + ([pad_token] * (self.max_length - seq_len))\n",
    "    \n",
    "    \n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장을 받아서 padding이 추가된 token_id 리스트로 변환 후 반환\n",
    "        Parameter\n",
    "            text: str - token_id 리스트로 변환할 한 문장\n",
    "        Return\n",
    "            list[int] - 입력받은 문장에 대한 token_id 리스트\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        #max_length 크기에 맞춘다\n",
    "        token_ids = self.__pad_token_sequence(encode.ids)\n",
    "        return token_ids\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_texts)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #return (질문토큰들, 답변토큰들)\n",
    "        q = self.question_texts[index]\n",
    "        a = self.answer_texts[index]\n",
    "        # list -> longtensor로 변환 nn.Embedding()의 입력(정수타입)으로 들어가기 때문에 변경\n",
    "        return torch.tensor(q, dtype=torch.int64), torch.tensor(a, dtype=torch.int64)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be21ba86-8561-4ba1-9262-b156bcb5bb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42., 48., 91.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 적당한 max_length 값 : 전체 문장 총 토큰수의 9분위 수\n",
    "import numpy as np\n",
    "\n",
    "a = [len(s) for s in all_text]\n",
    "np.quantile(a, q=[0.9, 0.95, 1.0])\n",
    "\n",
    "#max_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bcb808e-6edc-4f5e-8de3-41d4cb318e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.000e+00, 1.200e+01, 8.000e+01, 2.870e+02, 7.170e+02, 1.250e+03,\n",
       "        1.509e+03, 1.632e+03, 1.539e+03, 1.224e+03, 9.380e+02, 7.180e+02,\n",
       "        6.000e+02, 4.080e+02, 1.940e+02, 2.370e+02, 1.630e+02, 9.700e+01,\n",
       "        8.500e+01, 3.800e+01, 2.600e+01, 2.400e+01, 1.400e+01, 9.000e+00,\n",
       "        3.000e+00, 2.000e+00, 4.000e+00, 1.000e+00, 2.000e+00, 2.000e+00]),\n",
       " array([ 3.        ,  5.93333333,  8.86666667, 11.8       , 14.73333333,\n",
       "        17.66666667, 20.6       , 23.53333333, 26.46666667, 29.4       ,\n",
       "        32.33333333, 35.26666667, 38.2       , 41.13333333, 44.06666667,\n",
       "        47.        , 49.93333333, 52.86666667, 55.8       , 58.73333333,\n",
       "        61.66666667, 64.6       , 67.53333333, 70.46666667, 73.4       ,\n",
       "        76.33333333, 79.26666667, 82.2       , 85.13333333, 88.06666667,\n",
       "        91.        ]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(a, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12cca6bc-bf91-4030-9c83-2aa72e0c0351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  10, 1622, 1287,  368,    3,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " tensor([6119,  378,   47, 2252,    8,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 생성\n",
    "MAX_LENGTH = 20\n",
    "dataset = ChatbotDataset(question_texts, answer_texts, MAX_LENGTH, tokenizer)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d2b6f-ccf7-4aec-9c08-176a2456e813",
   "metadata": {},
   "source": [
    "### Trainset / Testset 나누기\n",
    "train : test = 8 : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb7d0d5a-c8ba-48be-bc46-d5ba12cd89e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10640 1183\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(dataset) * 0.9)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "print(train_size, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "42aed6cf-4aa2-4262-a4d4-964365ce7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = random_split(dataset, [train_size, test_size]) #shuffle 후 갯수에 맞게 나눔"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b274f-8c8a-4aa6-af6d-a66bf5c38210",
   "metadata": {},
   "source": [
    "### DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c51e1123-cbdd-4dce-8998-d4638ce04e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdaf424-c7de-46be-b74b-88a3efcdf352",
   "metadata": {},
   "source": [
    "# 모델 정의\n",
    "\n",
    "## Seq2Seq 모델 정의\n",
    "- Seq2Seq 모델은 Encoder와 Decoder의 입력 Sequence의 길이와 순서가 자유롭기 때문에 챗봇이나 번역에 이상적인 구조다.\n",
    "    - 단일 RNN은 각 timestep 마다 입력과 출력이 있기 때문에 입/출력 sequence의 개수가 같아야 한다.\n",
    "    - 챗봇의 질문/답변이나 번역의 대상/결과 문장의 경우는 사용하는 어절 수가 다른 경우가 많기 때문에 단일 RNN 모델은 좋은 성능을 내기 어렵다.\n",
    "    - Seq2Seq는 **입력처리(질문,번역대상)처리 RNN과 출력 처리(답변, 번역결과) RNN 이 각각 만들고 그 둘을 연결한 형태로 길이가 다르더라도 상관없다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d7410-a73e-4be1-b41d-9ea07d6b0911",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "Encoder는 하나의 Vector를 생성하며 그 Vector는 **입력 문장의 의미**를 N 차원 공간 저장하고 있다. 이 Vector를 **Context Vector** 라고 한다.    \n",
    "![encoder](figures/seq2seq_encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ff60f00e-707c-47e9-ab60-dad42d0ec508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, bidirectional = True, num_layers = 1, dropout_rate = 0.0):\n",
    "        super().__init__()\n",
    "        # Encoder는 context vector(문장의 feature) 를 생성하는 것이 목적 (분류기느 생성 안 함)\n",
    "        # Embedding Layer, GRU Layer를 생성\n",
    "        self.vocab_size = vocab_size # 어휘사전의 총 어휘 수(토큰수)\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,  # embedding vector shape: [vocab_size, embedding_dim]\n",
    "            padding_idx=0\n",
    "        )\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_dim, # 개별 토큰(time step)의 크기\n",
    "            hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            bidirectional = bidirectional,\n",
    "            dropout = dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X) # (batch, seq_len, embedding_dim)\n",
    "        X = X.transpose(1, 0) # (seq_len, batch, embedding_dim)\n",
    "        out, hidden = self.gru(X)\n",
    "        return out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b7c9fcf-6d84-4c2f-b7b5-82ea92b80150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [20, 64, 512]             --\n",
       "├─Embedding: 1-1                         [64, 20, 200]             200,000\n",
       "├─GRU: 1-2                               [20, 64, 512]             703,488\n",
       "==========================================================================================\n",
       "Total params: 903,488\n",
       "Trainable params: 903,488\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 913.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 7.29\n",
       "Params size (MB): 3.61\n",
       "Estimated Total Size (MB): 10.92\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "encoder_model = Encoder(1000, 200, 256)\n",
    "dummy_data = torch.zeros((64, 20), dtype=torch.int64)\n",
    "summary(encoder_model, input_data=dummy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24779603-15ac-4ba1-a24c-6e86658b3ad6",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "- Encoder의 출력(context vector)를 받아서 번역 결과 sequence를 출력한다.\n",
    "- Decoder는 매 time step의 입력으로 **이전 time step에서 예상한 단어와 hidden state값이** 입력된다.\n",
    "- Decoder의 처리결과 hidden state를 Estimator(Linear+Softmax)로 입력하여 **입력 단어에 대한 번역 단어가 출력된다.** (이 출력단어가 다음 step의 입력이 된다.)\n",
    "    - Decoder의 첫 time step 입력은 문장의 시작을 의미하는 <SOS>(start of string) 토큰이고 hidden state는 context vector(encoder 마지막 hidden state) 이다.\n",
    "\n",
    "![decoder](figures/seq2seq_decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "282f60e8-7e86-479a-8522-d32aab86b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1, bidriectional=False, dropout_rate=0.0) :\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size # 총 어휘사전 토큰 갯수\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        #GRU\n",
    "        # Auto regressive RNN은 단방향만 가능\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, dropout=dropout_rate)\n",
    "    \n",
    "        #drop out layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        #분류기 (다음 단어 추론) - 다중 분류 (단어 사전의 단어들 중 다음 단어일 확률)\n",
    "        self.lr = nn.Linear(\n",
    "            hidden_size, # GRU 출력 값 중 마지막 hidden state값을 입력으로 받음\n",
    "            vocab_size # 출력 : 다음 단어일 확률\n",
    "        )\n",
    "\n",
    "    def forward(self, X, hidden) :\n",
    "        # X = torch.LongTensor : shape - [batch] 한 단어씩 입력을 받음\n",
    "        # hidden = torch.FloatTensor : shape - [1, batch, hidden_size] (이전까지의 특성)\n",
    "         X = X.unsqueeze(1) # seq_len 축을 추가 [batch] -> [1, batch]\n",
    "         X = self.embedding(X) # [batch, 1, embedding차원]\n",
    "         X = X.transpose(1,0) # [1, batch embedding차원]\n",
    "         out, hidden = self.gru(X, hidden)\n",
    "         last_out = out[-1] # out : 전체 hidden state값 -> 마지막 hidden state을 추출\n",
    "         last_out = self.lr(last_out)\n",
    "         return last_out, hidden # (hidden : 다음 timestep에 전달)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dbb1cd25-b1e6-4f64-9319-e397561845ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [64, 1000]                --\n",
       "├─Embedding: 1-1                         [64, 1, 200]              200,000\n",
       "├─GRU: 1-2                               [1, 64, 256]              351,744\n",
       "├─Linear: 1-3                            [64, 1000]                257,000\n",
       "==========================================================================================\n",
       "Total params: 808,744\n",
       "Trainable params: 808,744\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 51.76\n",
       "==========================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 0.75\n",
       "Params size (MB): 3.23\n",
       "Estimated Total Size (MB): 4.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary\n",
    "decoder_model = Decoder(1000, 200, 256)\n",
    "dummy_input = torch.ones((64,), dtype=torch.int64)\n",
    "dummy_hidden = torch.ones((1, 64, 256), dtype=torch.float32)\n",
    "\n",
    "summary(decoder_model, input_data=(dummy_input, dummy_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563f5b2-f18b-42b5-9bd4-f4f8abd767ac",
   "metadata": {},
   "source": [
    "## Seq2Seq 모델\n",
    "\n",
    "- Encoder - Decoder 를 Layer로 가지며 Encoder로 질문의 feature를 추출하고 Decoder로 답변을 생성한다.\n",
    "\n",
    "### Teacher Forcing\n",
    "- **Teacher forcing** 기법은, RNN계열 모델이 다음 단어를 예측할 때, 이전 timestep에서 예측된 단어를 입력으로 사용하는 대신 **실제 정답 단어(ground truth) 단어를** 입력으로 사용하는 방법이다.\n",
    "    - 모델은 이전 시점의 출력 단어를 다음 시점의 입력으로 사용한다. 그러나 모델이 학습할 때 초반에는 정답과 많이 다른 단어가 생성되어 엉뚱한 입력이 들어가 학습이 빠르게 되지 않는 문제가 있다.\n",
    "- **장점**\n",
    "    - **수렴 속도 증가**: 정답 단어를 사용하기 때문에 모델이 더 빨리 학습할 수있다.\n",
    "    - **안정적인 학습**: 초기 학습 단계에서 모델의 예측이 불안정할 때, 잘못된 예측으로 인한 오류가 다음 단계로 전파되는 것을 막아줍니다.\n",
    "- **단점**\n",
    "    - **노출 편향(Exposure Bias) 문제:** 실제 예측 시에는 정답을 제공할 수 없으므로 모델은 전단계의 출력값을 기반으로 예측해 나가야 한다. 학습 과정과 추론과정의 이러한 차이 때문에 모델의 성능이 떨어질 수있다.\n",
    "        - 이런 문제를 해결하기 학습 할 때 **Teacher forcing을 random하게 적용하여 학습시킨다.**\n",
    "![seq2seq](figures/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eec05c3a-7da5-4297-974e-fcc032ea9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = tokenizer.token_to_id('[SOS]')\n",
    "\n",
    "class Seq2Seq(nn.Module) :\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device) :\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(device)\n",
    "        self.decoder = decoder.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, outputs, teacher_forcing_rate=0.99) :\n",
    "        '''\n",
    "        parameter\n",
    "            inputs : 질문 - (batch, seq_length)\n",
    "            outputs : 답변(정답) - (batch, seq_length)\n",
    "            teacher_forcing_rate : teacher_forcing 적용 확률\n",
    "        '''\n",
    "        if inputs.dim() == 1: # (seq_length) 질문이 1개일 경우 질문 문장 토큰만 입력 될 수있다\n",
    "            inputs = inputs.unsqueeze(0) # (1, seq_length)\n",
    "        if outputs.dim() == 1:\n",
    "            outputs = outputs.unsqueeze(0)\n",
    "\n",
    "        batch_size, output_length = outputs.shape\n",
    "        output_vocab_size = self.decoder.vocab_size # 어휘 사전 토큰 갯수\n",
    "\n",
    "        # 생성된 문장을 저장할 변수\n",
    "        ## (seq length, batch_size, vocab_size(단어별 확률))\n",
    "        predicted_outputs = torch.zeros(output_length, batch_size, output_vocab_size).to(self.device)\n",
    "\n",
    "        ## encoder를 이용해서 질문의 context vector 추출\n",
    "        encoder_out, encoder_hidden = self.encoder(inputs)\n",
    "        #encoder_out : [seq_len, batch, hidden_size * 2(양방향)]\n",
    "\n",
    "        #encoder_hidden : [2(양방향), batch, hidden_size]\n",
    "        decoder_hidden = encoder_out[-1].unsqueeze(0) # [1, batch_size]\n",
    "\n",
    "        #decoder에 넣을 첫번째 timestep값 : [SOS]\n",
    "        decoder_input = torch.full((batch_size,), fill_value=SOS_TOKEN, device=self.device)\n",
    "\n",
    "        # 순회하면서 단어들을 하나씩 생성\n",
    "        for t in range(output_length) :   #max_length 만큼 생성\n",
    "            # decoder_input(개별 단어 토큰 id) : [batch_size] - 첫 timestep : SOS, 두번째 : 생성된 토큰\n",
    "            # decoder_out(batch_size, vocab_size) : 다음 단어일 확률\n",
    "            # decoder_hidden(1, batch_size, hidden * 2(양방향)) - 현재 입력의 feature 다음 hidden\n",
    "            decoder_out, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            predicted_outputs[t] = decoder_out # t번째 예측 단어를 저장\n",
    "\n",
    "            ### 다음 timestep에 넣어줄 값 생성\n",
    "            ### teacher_forcing 적용 : 정답, 비적용 : 모델이 추론한 결과\n",
    "            teacher_forcing = teacher_forcing_rate > random.random()\n",
    "            # teacher_forcing_rate의 확률만큼 True\n",
    "            teacher_forcing_rate = teacher_forcing_rate * 0.99  # 점점 teacher_forcing 가능성 줄임\n",
    "\n",
    "            # 모델이 추론한 단어 중 가장 확률이 높은 (Top1) 단어를 추출\n",
    "            top1 = decoder_out.argmax(-1)\n",
    "            # teacher_forcing이 True 정답, False : 예측 \n",
    "            decoder_input = outputs[:, t] if teacher_forcing else top1\n",
    "            \n",
    "        return predicted_outputs.transpose(1,0) # (batch, seq_length, vocab_size) 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89fed5-b059-4371-b836-c3eb59bebdfb",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac7b8a-935e-4f3a-9fa5-efbacbfc19d7",
   "metadata": {},
   "source": [
    "## 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0280640f-243a-4413-8e8c-dc0285b7aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 생성\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "ENCODER_BIDIRECTIONAL = True # 인코더 양방향 여부\n",
    "ENCODER_HIDDEN_SIZE = 200\n",
    "DECODER_HIDDEN_SIZE = ENCODER_HIDDEN_SIZE * 2 if ENCODER_BIDIRECTIONAL else ENCODER_HIDDEN_SIZE\n",
    "EMBEDDING_DIM = 256 # 임베딩 차원\n",
    "TEACHER_FORCING_RATE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a609b827-8287-4ea8-8df7-03a37ac8157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "encoder = Encoder(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    hidden_size = ENCODER_HIDDEN_SIZE,\n",
    "    bidirectional = ENCODER_BIDIRECTIONAL,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "# 모델 생성\n",
    "decoder = Decoder(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    embedding_dim = EMBEDDING_DIM,\n",
    "    hidden_size = DECODER_HIDDEN_SIZE,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba83263-e042-4579-9784-2403eb3c3fa1",
   "metadata": {},
   "source": [
    "## loss함수, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "839652a6-a4f0-4146-888b-30a9670854c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "model = model.to(device)\n",
    "# 다음 단어를 추론하는 다중 분류\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a659df1-87a2-4fe0-a095-e031ed130e68",
   "metadata": {},
   "source": [
    "## train/evaluation 함수 정의\n",
    "\n",
    "### train 함수정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "048ca8af-0556-4b59-a0ba-69f1995873aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch 학습 함수\n",
    "def train_fn(model, data_loader, optimizer, loss_fn, device, teacher_forcing_rate=0.99) :\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "\n",
    "    for X, y in data_loader :\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        #추론\n",
    "        pred = model(X, y, teacher_forcing_rate)\n",
    "        # pred : 추론한 답변 문장 (batch, seq_length, vocab_size: 토큰일 확률)\n",
    "\n",
    "        #pred와 정답의 shape을 변경 (loss에 넣을수있는 shape으로 변환)\n",
    "        #pred를 reshape (batch, seq_len, vocab_size) -> (batch*seq_len, vocab_size)\n",
    "        y_hat = pred.reshape(-1, pred.shape[2])\n",
    "        # 정답(y)을 reshape (batch, seq_len) -> (batch * seq_len)\n",
    "        y = y.reshape(-1)\n",
    "        # CrossEntropyLoss(): 정답 - 원핫인코딩 안된 형태, 추론값: softmax 처리 안된 상태\n",
    "        #       - 정답  shape: (batch, )\n",
    "        #       - 추론값shape: (batch, class개수)\n",
    "        #Loss 계산\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_list.append(loss.item())\n",
    "    return np.mean(loss_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981388d-ad33-4318-844b-29a5a434d2a7",
   "metadata": {},
   "source": [
    "### Test 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "52e0f5d3-dcdc-4b8e-b359-99aa8391e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(model, data_loader, loss_fn, device) :\n",
    "\n",
    "    # 1. 에폭 테스트\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for X, y in data_loader :\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X, y, teacher_forcing_rate=0.0)\n",
    "            y_hat = pred.reshape(-1, pred.shape[2])\n",
    "            y = y.reshape(-1)\n",
    "            loss_list.append(loss_fn(y_hat, y).item())\n",
    "            \n",
    "    return np.mean(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71e20c-8a03-44f4-bbbc-8f4e51b85636",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "335aa2c8-4e29-46dd-90bd-69a41301adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0에서 저장. inf에서 2.62633로 개선 됨\n",
      "[0/10] train loss : 1.737084824636758, val loss : 2.626334642109118\n",
      "[1/10] train loss : 1.5191522635609271, val loss : 2.6666539844713713\n",
      "[2/10] train loss : 1.3156407360570976, val loss : 2.76448120568928\n",
      "[3/10] train loss : 1.100205335631428, val loss : 3.00342909913314\n",
      "[4/10] train loss : 0.9316974907036287, val loss : 3.0219767470108834\n",
      "[5/10] train loss : 0.7947534595627382, val loss : 3.19746826824389\n",
      "[6/10] train loss : 0.7240790678075997, val loss : 3.1240621491482385\n",
      "[7/10] train loss : 0.6323503841118641, val loss : 3.353264670622976\n",
      "[8/10] train loss : 0.49272662760263464, val loss : 3.497721559122989\n",
      "[9/10] train loss : 0.4510383440787534, val loss : 3.5147845996053597\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "MODEL_SAVE_PATH = 'saved_model/seq2seq2-chatbot-model.pt'\n",
    "\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS) :\n",
    "    train_loss = train_fn(model, train_loader, optimizer, loss_fn, device, TEACHER_FORCING_RATE)\n",
    "    val_loss = test_fn(model, test_loader, loss_fn, device)\n",
    "\n",
    "    # 저장\n",
    "    if val_loss < best_loss :\n",
    "        torch.save(model, MODEL_SAVE_PATH)\n",
    "        print(f'{epoch}에서 저장. {best_loss:.5f}에서 {val_loss:.5f}로 개선 됨')\n",
    "        best_loss = val_loss\n",
    "\n",
    "    print(f'[{epoch}/{EPOCHS}] train loss : {train_loss}, val loss : {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ac81248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH_LAST = 'saved_model/seq2seq-chatbot-model_last.pt'\n",
    "torch.save(model, MODEL_SAVE_PATH_LAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e2463952",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 저장 모델 Load\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# map_location=device: 다른 device에 학습한 것을 읽어올 때 현재 device를 지정해서 현재 device에 맞춰 load하도록 한다.\n",
    "best_model = torch.load(MODEL_SAVE_PATH, weights_only=False, map_location=device)\n",
    "best_model.device = device # Attribute device를 현재 device로 지정. \n",
    "\n",
    "last_model = torch.load(MODEL_SAVE_PATH_LAST, weights_only=False, map_location=device)\n",
    "last_model.device = device # Attribute device를 현재 device로 지정. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0585a-eb35-47dd-88bf-276d749f5f00",
   "metadata": {},
   "source": [
    "# 결과확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e4bcf956-b7e1-4f05-bd96-723fe0624339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "\n",
    "def handle_special_tokens(decoded_string):\n",
    "    \"\"\"\n",
    "    Subword 처리\n",
    "    subword는 단어의 시작으로 쓰인 것과 중간 부분(연결)에 사용된 두가지 subword가 있다.  연결 subword는 `#`과 같은 특수문자로 시작 한다.\n",
    "    tokenizer.decode() 결과 문자열은 subword의 특수문자('#')을 처리하지 않는다. 이것을 처리하는 함수\n",
    "    ex) \"이 기회 #는 내 #꺼 #야\" ==> \"이 기회는 내꺼야\"\n",
    "    \n",
    "    Parameter\n",
    "        decoded_string: str - Tokenizer가 decode한 중간 subword의 특수문자 처리가 안된 문자열. \n",
    "    Return\n",
    "        str: subword 특수문자 처리한 문자열\n",
    "    \"\"\"\n",
    "    tokens = decoded_string.split() # 공백 기준으로 토큰화\n",
    "    new_tokens = []\n",
    "    for token in tokens :\n",
    "        if token.startswith('##') : # 연결토큰\n",
    "            if new_tokens : # len(new_tokens) != 0 원소가 하나라도 있으면 토큰에서 ## 제거하고 리스트의 마지막 원소 뒤에 붙임\n",
    "                new_tokens[-1] += token[2:]\n",
    "            else :\n",
    "                new_tokens.append(token[2:])\n",
    "        else :  # 단어의 시작인 토큰 (##이 없는 토큰) -> List에 추가\n",
    "            new_tokens.append(token)\n",
    "\n",
    "    return \" \".join(new_tokens)  \n",
    "\n",
    "# dataset에서 일부 데이터들을 가지고 확인\n",
    "def random_evaluation(model, dataset, device, n=10):\n",
    "    \"\"\"\n",
    "    Dataset에서 일부 질문-답변 쌍들을 가져다 모델에 질문을 넣어 추론한 결과와 함께 확인.\n",
    "    Parameter\n",
    "        model: 학습된 seq2seq 모델\n",
    "        dataset: 질문-답변 쌍울 추출할 dataset\n",
    "        device\n",
    "        n: int - 추출할 질문-답변 쌍 개수 default: 10\n",
    "    \"\"\"\n",
    "    ## 평가할 데이터셋 만들기\n",
    "    no_samples = len(dataset)\n",
    "    index = list(range(no_samples))\n",
    "    np.random.shuffle(index)\n",
    "    sample_index = index[ : n  ]\n",
    "\n",
    "    #dataloader 생성\n",
    "    # SubsetRandomSampler : 지정한 Index안에서 random한 순서로 제공\n",
    "    sampler = SubsetRandomSampler(sample_index)\n",
    "    sample_loader = DataLoader(dataset, batch_size=n, sampler=sampler)\n",
    "\n",
    "    #추론 후 확인\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad() :\n",
    "        for X, y in sample_loader :\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            output = model(X, y, 0.0)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "            # torch.Tensor -> ndarray (tokenizer decode에 넣기 위해)\n",
    "            # tensor -> cpu로 이동 후에 변환가능\n",
    "            # tensor가 gradient를 가지고있으면 -> tensor.detach().cpu().ndarray()로 바꿔야함\n",
    "            pred = output.cpu().numpy()\n",
    "            X = X.cpu().numpy()\n",
    "            y = y.cpu().numpy()\n",
    "\n",
    "            for i in range(n) :\n",
    "                q = handle_special_tokens(tokenizer.decode(X[i]))\n",
    "                a = handle_special_tokens(tokenizer.decode(y[i]))\n",
    "                p = handle_special_tokens(tokenizer.decode(pred[i].argmax(-1)))\n",
    "\n",
    "                print(f'질문 : {q}')\n",
    "                print(f'정답 : {a}')\n",
    "                print(f'예측 : {p}')\n",
    "                print('===================================================')\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279e7d9",
   "metadata": {},
   "source": [
    "Sampler:\n",
    "- DataLoader가 Dataset의 값을 읽어서 batch를 만들때 index 순서를 정해주는 객체.\n",
    "- DataLoader의 기본 sampler는 SequentialSampler 이다. shuffle=True일 경우 RandomSampler: 랜덤한 순서로 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "25d9fb1a-8b8a-4536-9dc3-554599757637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일 ##부 질문 - 답 ##변 쌍 ##들을 가져 ##다 모 ##델 ##에 질문'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tokenizer.encode(\"일부 질문-답변 쌍들을 가져다 모델에 질문\")\n",
    "txt = tokenizer.decode(e.ids)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e7bc65a4-5082-4df5-90a6-6d26154698dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일부 질문 - 답변 쌍들을 가져다 모델에 질문'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_special_tokens(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ce56b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============BEST MODEL=================\n",
      "질문 : 여자친구가 데이트 시간을 너무 안 지켜 .\n",
      "정답 : 준비하느라 오래걸리는 친구를 이해해주세요 .\n",
      "예측 : 마음이 복잡하겠어요 .\n",
      "===================================================\n",
      "질문 : 집 사고 싶어\n",
      "정답 : 같이 살고 싶은 사람이 있나봐요 .\n",
      "예측 : 좋은 사람 만날 수 있을 거예요 .\n",
      "===================================================\n",
      "질문 : 술 잘하니\n",
      "정답 : 저는 주당이에요 .\n",
      "예측 : 좋은 사람 만날 수 있을 거예요 .\n",
      "===================================================\n",
      "질문 : 열받아서 막 전화에다가 화풀이했어\n",
      "정답 : 속이 좀 풀렸길 발바니다 .\n",
      "예측 : 좋은 사람 만날 수 있을 거예요 .\n",
      "===================================================\n",
      "질문 : 짝사랑하는 사람에 대한 마음이 갑자기 식었어 .\n",
      "정답 : 무슨 계기가 있었을지도 몰라요 .\n",
      "예측 : 마음이 복잡하겠어요 .\n",
      "===================================================\n",
      "질문 : 사과해야 하나 ?\n",
      "정답 : 사과할 건 사과해야죠 .\n",
      "예측 : 마음이 따뜻할 것 같아요 .\n",
      "===================================================\n",
      "질문 : 남편이 효자인 거 같아\n",
      "정답 : 좋게 생각해보세요 .\n",
      "예측 : 좋은 사람 만날 수 있을 거예요 .\n",
      "===================================================\n",
      "질문 : 존경할 수 있는 사람 만나고 싶어 .\n",
      "정답 : 그런 사람 만날 수 있을 거예요 .\n",
      "예측 : 좋은 사람 만날 수 있을 거예요 .\n",
      "===================================================\n",
      "질문 : 버스 멀미하는 것 같아\n",
      "정답 : 핸드폰 만지지 마세요 .\n",
      "예측 : 좋은 사람 만날 수 있을 거예요 .\n",
      "===================================================\n",
      "질문 : 포기안한다고 말했어\n",
      "정답 : 마음이 힘들겠어요 .\n",
      "예측 : 좋은 사람 만날 수 있을 거예요 .\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "print('===============BEST MODEL=================')\n",
    "random_evaluation(best_model, test_set, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4965d1-a305-4465-8f64-f689d55490ac",
   "metadata": {},
   "source": [
    "# 새로운 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "827215a2-7bdb-44c5-b891-8ff15ae621f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ChatbotInputDataset\n",
    "    질문만 받아서 생성하는 Dataset\n",
    "    - 새로운 데이터 추론용\n",
    "    \"\"\"\n",
    "    def __init__(self, question_texts, max_length, tokenizer):\n",
    "        \"\"\"\n",
    "        parameter\n",
    "            question_texts: list[str] - 질문 texts 목록. 리스트에 질문들을 담아서 받는다. [\"질문1\", \"질문2\", ...]\n",
    "            max_length: 개별 문장의 token 개수. 모든 문장의 토큰수를 max_length에 맞춘다.\n",
    "            tokenizer: Tokenizer\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.question_texts = [self.__process_sequence(q) for q in question_texts]\n",
    "    \n",
    "    def __pad_token_sequence(self, token_sequence):\n",
    "        \"\"\"\n",
    "        max_length 길이에 맞춰 token_id 리스트를 구성한다.\n",
    "        max_length 보다 길면 뒤에를 자르고 max_length 보다 짧으면 [PAD] 토큰을 추가한다.\n",
    "        \n",
    "        Parameter\n",
    "            token_sentence: list[int] - 길이를 맞출 한 문장 token_id 목록\n",
    "        Return\n",
    "            list[int] - length가 max_length인 token_id 목록\n",
    "        \"\"\"\n",
    "        pad_token = self.tokenizer.token_to_id('[PAD]')\n",
    "        seq_len = len(token_sequence)\n",
    "\n",
    "        if seq_len > self.max_length :\n",
    "            return token_sequence[:self.max_length]\n",
    "        else :\n",
    "            return token_sequence + ([pad_token] * (self.max_length - seq_len))\n",
    "    \n",
    "    \n",
    "    def __process_sequence(self, text):\n",
    "        \"\"\"\n",
    "        한 문장을 받아서 padding이 추가된 token_id 리스트로 변환 후 반환\n",
    "        Parameter\n",
    "            text: str - token_id 리스트로 변환할 한 문장\n",
    "        Return\n",
    "            list[int] - 입력받은 문장에 대한 token_id 리스트\n",
    "        \"\"\"\n",
    "        encode = self.tokenizer.encode(text)\n",
    "        #max_length 크기에 맞춘다\n",
    "        token_ids = self.__pad_token_sequence(encode.ids)\n",
    "        return token_ids\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.question_texts)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #return (질문토큰들, 답변토큰들)\n",
    "        q = self.question_texts[index]\n",
    "        # list -> longtensor로 변환 nn.Embedding()의 입력(정수타입)으로 들어가기 때문에 변경\n",
    "        return torch.tensor(q, dtype=torch.int64)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7cc2920e-fcdb-47d3-a61d-14c61ddb6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [\n",
    "    '집에 가고싶어',\n",
    "    '재밌는거 없을까?',\n",
    "    '지금 몇 시예요'\n",
    "]\n",
    "\n",
    "input_dataset = ChatbotIDataset(input_data, MAX_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "93a09b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, model, device) :\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad() :\n",
    "        for X in dataset :\n",
    "            X = X.to(device)\n",
    "            output = model(X.unsqueeze(0), X.unsqueeze(0), 0.0)\n",
    "            pred = output.cpu().numpy()\n",
    "            X = X.cpu().numpy()\n",
    "            q = handle_special_tokens(tokenizer.decode(X))\n",
    "            a = handle_special_tokens(tokenizer.decode(pred[0].argmax(-1)))\n",
    "\n",
    "            print(f'질문 : {q}')\n",
    "            print(f'예상 답 : {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cd4aa64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 : 집에 가고싶어\n",
      "예상 답 : 저도 딸기 좋아해요 . 같이 가요 .\n",
      "질문 : 재밌는거 없을까 ?\n",
      "예상 답 : 혼자 풀릴 때까지 놔둬야하는데 기다리는게 힘들 거예요 .\n",
      "질문 : 지금 몇 시예요\n",
      "예상 답 : 성공을 빌어요 .\n"
     ]
    }
   ],
   "source": [
    "predict(input_dataset, last_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59640f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a9d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
